{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generation of Questions with a seq2seq network \n",
    "*************************************************************\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "::\n",
    "\n",
    "    [KEY: > input, = target, < output]\n",
    "\n",
    "   \n",
    "\n",
    "This is made possible by the simple but powerful idea of the `sequence\n",
    "to sequence network <http://arxiv.org/abs/1409.3215>`__, in which two\n",
    "recurrent neural networks work together to transform one sequence to\n",
    "another. An encoder network condenses an input sequence into a vector,\n",
    "and a decoder network unfolds that vector into a new sequence.\n",
    "\n",
    ".. figure:: /_static/img/seq-seq-images/seq2seq.png\n",
    "   :alt:\n",
    "\n",
    "To improve upon this model we'll use an `attention\n",
    "mechanism <https://arxiv.org/abs/1409.0473>`__, which lets the decoder\n",
    "learn to focus over a specific range of the input sequence.\n",
    "\n",
    "**Requirements**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data files\n",
    "===================\n",
    "\n",
    "The data for this project is a set of many thousands of sentence pairs, broken down into two main groups i.e agreement and no agreement and are present in the dataset folder of the project\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the character encoding used in the character-level RNN\n",
    "tutorials, we will be representing each word in a language as a one-hot\n",
    "vector, or giant vector of zeros except for a single one (at the index\n",
    "of the word). Compared to the dozens of characters that might exist in a\n",
    "language, there are many many more words, so the encoding vector is much\n",
    "larger. We will however cheat a bit and trim the data to only use a few\n",
    "thousand words per language.\n",
    "\n",
    ".. figure:: /_static/img/seq-seq-images/word-encoding.png\n",
    "   :alt:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need a unique index per word to use as the inputs and targets of\n",
    "the networks later. To keep track of all this we will use a helper class\n",
    "called ``Lang`` which has word → index (``word2index``) and index → word\n",
    "(``index2word``) dictionaries, as well as a count of each word\n",
    "``word2count`` to use to later replace rare words.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will make evertything lower case adn trim most punctuation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read the data file we will split the file into lines, and then split\n",
    "lines into pairs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('./../data/no_agreement_data/train_data.txt' ,).\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum length of each sentence has been set to a 100 words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 100\n",
    "\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full process for preparing the data is:\n",
    "\n",
    "-  Read text file and split into lines, split lines into pairs\n",
    "-  Normalize text, filter by length and content\n",
    "-  Make word lists from sentences in pairs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 196980 sentence pairs\n",
      "Trimmed to 196980 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "inp 54\n",
      "out 52\n",
      "['the yaks that will entertain the elephants will giggle quest', 'will the yaks that will entertain the elephants giggle']\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('out', 'inp', True)\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Seq2Seq Model\n",
    "=================\n",
    "\n",
    "A Recurrent Neural Network, or RNN, is a network that operates on a\n",
    "sequence and uses its own output as input for subsequent steps.\n",
    "\n",
    "A `Sequence to Sequence network <http://arxiv.org/abs/1409.3215>`__, or\n",
    "seq2seq network, or `Encoder Decoder\n",
    "network <https://arxiv.org/pdf/1406.1078v3.pdf>`__, is a model\n",
    "consisting of two RNNs called the encoder and decoder. The encoder reads\n",
    "an input sequence and outputs a single vector, and the decoder reads\n",
    "that vector to produce an output sequence.\n",
    "\n",
    ".. figure:: /_static/img/seq-seq-images/seq2seq.png\n",
    "   :alt:\n",
    "\n",
    "Unlike sequence prediction with a single RNN, where every input\n",
    "corresponds to an output, the seq2seq model frees us from sequence\n",
    "length and order, which makes it ideal for translation between two\n",
    "languages.\n",
    "\n",
    "Consider the sentence \"Je ne suis pas le chat noir\" → \"I am not the\n",
    "black cat\". Most of the words in the input sentence have a direct\n",
    "translation in the output sentence, but are in slightly different\n",
    "orders, e.g. \"chat noir\" and \"black cat\". Because of the \"ne/pas\"\n",
    "construction there is also one more word in the input sentence. It would\n",
    "be difficult to produce a correct translation directly from the sequence\n",
    "of input words.\n",
    "\n",
    "With a seq2seq model the encoder creates a single vector which, in the\n",
    "ideal case, encodes the \"meaning\" of the input sequence into a single\n",
    "vector — a single point in some N dimensional space of sentences.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Encoder\n",
    "-----------\n",
    "\n",
    "The encoder of a seq2seq network is a RNN that outputs some value for\n",
    "every word from the input sentence. For every input word the encoder\n",
    "outputs a vector and a hidden state, and uses the hidden state for the\n",
    "next input word.\n",
    "\n",
    ".. figure:: /_static/img/seq-seq-images/encoder-network.png\n",
    "   :alt:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Decoder\n",
    "-----------\n",
    "\n",
    "The decoder is another RNN that takes the encoder output vector(s) and\n",
    "outputs a sequence of words to create the translation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Decoder\n",
    "^^^^^^^^^^^^^^\n",
    "\n",
    "In the simplest seq2seq decoder we use only last output of the encoder.\n",
    "This last output is sometimes called the *context vector* as it encodes\n",
    "context from the entire sequence. This context vector is used as the\n",
    "initial hidden state of the decoder.\n",
    "\n",
    "At every step of decoding, the decoder is given an input token and\n",
    "hidden state. The initial input token is the start-of-string ``<SOS>``\n",
    "token, and the first hidden state is the context vector (the encoder's\n",
    "last hidden state).\n",
    "\n",
    ".. figure:: /_static/img/seq-seq-images/decoder-network.png\n",
    "   :alt:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I encourage you to train and observe the results of this model, but to\n",
    "save space we'll be going straight for the gold and introducing the\n",
    "Attention Mechanism.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention Decoder\n",
    "^^^^^^^^^^^^^^^^^\n",
    "\n",
    "If only the context vector is passed betweeen the encoder and decoder,\n",
    "that single vector carries the burden of encoding the entire sentence.\n",
    "\n",
    "Attention allows the decoder network to \"focus\" on a different part of\n",
    "the encoder's outputs for every step of the decoder's own outputs. First\n",
    "we calculate a set of *attention weights*. These will be multiplied by\n",
    "the encoder output vectors to create a weighted combination. The result\n",
    "(called ``attn_applied`` in the code) should contain information about\n",
    "that specific part of the input sequence, and thus help the decoder\n",
    "choose the right output words.\n",
    "\n",
    ".. figure:: https://i.imgur.com/1152PYf.png\n",
    "   :alt:\n",
    "\n",
    "Calculating the attention weights is done with another feed-forward\n",
    "layer ``attn``, using the decoder's input and hidden state as inputs.\n",
    "Because there are sentences of all sizes in the training data, to\n",
    "actually create and train this layer we have to choose a maximum\n",
    "sentence length (input length, for encoder outputs) that it can apply\n",
    "to. Sentences of the maximum length will use all the attention weights,\n",
    "while shorter sentences will only use the first few.\n",
    "\n",
    ".. figure:: /_static/img/seq-seq-images/attention-decoder-network.png\n",
    "   :alt:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>Note</h4><p>There are other forms of attention that work around the length\n",
    "  limitation by using a relative position approach. Read about \"local\n",
    "  attention\" in `Effective Approaches to Attention-based Neural Machine\n",
    "  Translation <https://arxiv.org/abs/1508.04025>`__.</p></div>\n",
    "\n",
    "Training\n",
    "========\n",
    "\n",
    "Preparing Training Data\n",
    "-----------------------\n",
    "\n",
    "To train, for each pair we will need an input tensor (indexes of the\n",
    "words in the input sentence) and target tensor (indexes of the words in\n",
    "the target sentence). While creating these vectors we will append the\n",
    "EOS token to both sequences.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Model\n",
    "------------------\n",
    "\n",
    "To train we run the input sentence through the encoder, and keep track\n",
    "of every output and the latest hidden state. Then the decoder is given\n",
    "the ``<SOS>`` token as its first input, and the last hidden state of the\n",
    "encoder as its first hidden state.\n",
    "\n",
    "\"Teacher forcing\" is the concept of using the real target outputs as\n",
    "each next input, instead of using the decoder's guess as the next input.\n",
    "Using teacher forcing causes it to converge faster but `when the trained\n",
    "network is exploited, it may exhibit\n",
    "instability <http://minds.jacobs-university.de/sites/default/files/uploads/papers/ESNTutorialRev.pdf>`__.\n",
    "\n",
    "You can observe outputs of teacher-forced networks that read with\n",
    "coherent grammar but wander far from the correct translation -\n",
    "intuitively it has learned to represent the output grammar and can \"pick\n",
    "up\" the meaning once the teacher tells it the first few words, but it\n",
    "has not properly learned how to create the sentence from the translation\n",
    "in the first place.\n",
    "\n",
    "Because of the freedom PyTorch's autograd gives us, we can randomly\n",
    "choose to use teacher forcing or not with a simple if statement. Turn\n",
    "``teacher_forcing_ratio`` up to use more of it.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a helper function to print time elapsed and estimated time\n",
    "remaining given the current time and progress %.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole training process looks like this:\n",
    "\n",
    "-  Start a timer\n",
    "-  Initialize optimizers and criterion\n",
    "-  Create set of training pairs\n",
    "-  Start empty losses array for plotting\n",
    "\n",
    "Then we call ``train`` many times and occasionally print the progress (%\n",
    "of examples, time so far, estimated time) and average loss.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting results\n",
    "----------------\n",
    "\n",
    "Plotting is done with matplotlib, using the array of loss values\n",
    "``plot_losses`` saved while training.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation\n",
    "==========\n",
    "\n",
    "Evaluation is mostly the same as training, but there are no targets so\n",
    "we simply feed the decoder's predictions back to itself for each step.\n",
    "Every time it predicts a word we add it to the output string, and if it\n",
    "predicts the EOS token we stop there. We also store the decoder's\n",
    "attention outputs for display later.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate random sentences from the training set and print out the\n",
    "input, target, and output to make some subjective quality judgements:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and Evaluating\n",
    "=======================\n",
    "\n",
    "With all these helper functions in place (it looks like extra work, but\n",
    "it makes it easier to run multiple experiments) we can actually\n",
    "initialize a network and start training.\n",
    "\n",
    "Remember that the input sentences were heavily filtered. For this small\n",
    "dataset we can use relatively small networks of 256 hidden nodes and a\n",
    "single GRU layer. After about 40 minutes on a MacBook CPU we'll get some\n",
    "reasonable results.\n",
    "\n",
    ".. Note::\n",
    "   If you run this notebook you can train, interrupt the kernel,\n",
    "   evaluate, and continue training later. Comment out the lines where the\n",
    "   encoder and decoder are initialized and run ``trainIters`` again.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 2s (- 71m 3s) (50 0%) 2.9451\n",
      "0m 3s (- 49m 29s) (100 0%) 2.6636\n",
      "0m 5s (- 42m 34s) (150 0%) 3.0684\n",
      "0m 6s (- 39m 22s) (200 0%) 2.8410\n",
      "0m 7s (- 37m 16s) (250 0%) 2.9088\n",
      "0m 8s (- 36m 4s) (300 0%) 2.7883\n",
      "0m 9s (- 35m 2s) (350 0%) 2.5754\n",
      "0m 10s (- 34m 11s) (400 0%) 2.6211\n",
      "0m 12s (- 33m 29s) (450 0%) 2.3578\n",
      "0m 13s (- 33m 3s) (500 0%) 2.5245\n",
      "0m 14s (- 32m 46s) (550 0%) 2.4896\n",
      "0m 15s (- 32m 45s) (600 0%) 2.6542\n",
      "0m 17s (- 32m 29s) (650 0%) 2.6394\n",
      "0m 18s (- 32m 22s) (700 0%) 2.4809\n",
      "0m 19s (- 32m 18s) (750 1%) 2.7272\n",
      "0m 20s (- 32m 8s) (800 1%) 2.6463\n",
      "0m 22s (- 32m 0s) (850 1%) 2.5437\n",
      "0m 23s (- 31m 52s) (900 1%) 2.6954\n",
      "0m 24s (- 31m 46s) (950 1%) 2.5506\n",
      "0m 25s (- 31m 40s) (1000 1%) 2.5308\n",
      "0m 27s (- 31m 44s) (1050 1%) 2.5245\n",
      "0m 28s (- 31m 58s) (1100 1%) 2.6929\n",
      "0m 29s (- 31m 51s) (1150 1%) 2.6096\n",
      "0m 30s (- 31m 46s) (1200 1%) 2.6287\n",
      "0m 32s (- 31m 47s) (1250 1%) 2.6386\n",
      "0m 34s (- 32m 10s) (1300 1%) 2.5934\n",
      "0m 36s (- 32m 49s) (1350 1%) 2.4100\n",
      "0m 37s (- 32m 55s) (1400 1%) 2.4479\n",
      "0m 38s (- 32m 57s) (1450 1%) 2.4810\n",
      "0m 40s (- 33m 3s) (1500 2%) 2.5809\n",
      "0m 41s (- 32m 59s) (1550 2%) 2.3811\n",
      "0m 43s (- 32m 59s) (1600 2%) 2.6872\n",
      "0m 44s (- 33m 4s) (1650 2%) 2.4630\n",
      "0m 46s (- 33m 11s) (1700 2%) 2.4606\n",
      "0m 47s (- 33m 21s) (1750 2%) 2.5391\n",
      "0m 49s (- 33m 20s) (1800 2%) 2.2837\n",
      "0m 50s (- 33m 14s) (1850 2%) 2.4397\n",
      "0m 51s (- 33m 11s) (1900 2%) 2.4749\n",
      "0m 53s (- 33m 19s) (1950 2%) 2.3110\n",
      "0m 55s (- 33m 57s) (2000 2%) 2.3717\n",
      "0m 59s (- 35m 18s) (2050 2%) 2.2918\n",
      "1m 1s (- 35m 52s) (2100 2%) 2.2526\n",
      "1m 3s (- 35m 48s) (2150 2%) 2.2619\n",
      "1m 4s (- 35m 47s) (2200 2%) 2.0034\n",
      "1m 6s (- 35m 54s) (2250 3%) 2.0520\n",
      "1m 8s (- 35m 55s) (2300 3%) 2.1903\n",
      "1m 9s (- 35m 55s) (2350 3%) 2.0349\n",
      "1m 11s (- 35m 51s) (2400 3%) 2.1829\n",
      "1m 12s (- 35m 55s) (2450 3%) 2.1794\n",
      "1m 14s (- 35m 54s) (2500 3%) 1.9496\n",
      "1m 15s (- 35m 51s) (2550 3%) 2.0858\n",
      "1m 17s (- 35m 50s) (2600 3%) 2.0368\n",
      "1m 18s (- 35m 48s) (2650 3%) 2.0333\n",
      "1m 20s (- 35m 48s) (2700 3%) 2.0187\n",
      "1m 21s (- 35m 46s) (2750 3%) 1.9149\n",
      "1m 23s (- 35m 45s) (2800 3%) 1.9328\n",
      "1m 24s (- 35m 42s) (2850 3%) 1.9417\n",
      "1m 26s (- 35m 42s) (2900 3%) 1.9978\n",
      "1m 27s (- 35m 40s) (2950 3%) 1.8302\n",
      "1m 29s (- 35m 40s) (3000 4%) 1.8360\n",
      "1m 30s (- 35m 37s) (3050 4%) 1.7094\n",
      "1m 31s (- 35m 33s) (3100 4%) 1.7442\n",
      "1m 33s (- 35m 32s) (3150 4%) 2.0043\n",
      "1m 34s (- 35m 30s) (3200 4%) 1.7780\n",
      "1m 36s (- 35m 32s) (3250 4%) 1.8159\n",
      "1m 38s (- 35m 34s) (3300 4%) 1.7589\n",
      "1m 39s (- 35m 36s) (3350 4%) 1.6970\n",
      "1m 41s (- 35m 34s) (3400 4%) 1.7660\n",
      "1m 42s (- 35m 31s) (3450 4%) 1.6606\n",
      "1m 44s (- 35m 34s) (3500 4%) 1.5873\n",
      "1m 46s (- 35m 34s) (3550 4%) 1.7481\n",
      "1m 47s (- 35m 32s) (3600 4%) 1.6031\n",
      "1m 49s (- 35m 38s) (3650 4%) 1.6498\n",
      "1m 51s (- 35m 39s) (3700 4%) 1.5865\n",
      "1m 52s (- 35m 46s) (3750 5%) 1.5732\n",
      "1m 54s (- 35m 53s) (3800 5%) 1.5233\n",
      "1m 56s (- 35m 58s) (3850 5%) 1.3779\n",
      "1m 58s (- 35m 55s) (3900 5%) 1.4603\n",
      "1m 59s (- 35m 53s) (3950 5%) 1.5449\n",
      "2m 1s (- 35m 52s) (4000 5%) 1.2196\n",
      "2m 2s (- 35m 50s) (4050 5%) 1.4376\n",
      "2m 4s (- 35m 47s) (4100 5%) 1.3100\n",
      "2m 5s (- 35m 47s) (4150 5%) 1.4463\n",
      "2m 7s (- 35m 44s) (4200 5%) 1.3646\n",
      "2m 8s (- 35m 44s) (4250 5%) 1.2518\n",
      "2m 10s (- 35m 49s) (4300 5%) 1.1953\n",
      "2m 12s (- 35m 49s) (4350 5%) 1.3374\n",
      "2m 14s (- 35m 56s) (4400 5%) 1.4146\n",
      "2m 16s (- 35m 58s) (4450 5%) 1.2588\n",
      "2m 17s (- 35m 59s) (4500 6%) 1.2184\n",
      "2m 19s (- 35m 59s) (4550 6%) 1.2716\n",
      "2m 21s (- 35m 59s) (4600 6%) 1.1960\n",
      "2m 22s (- 36m 2s) (4650 6%) 1.1977\n",
      "2m 24s (- 35m 58s) (4700 6%) 1.3074\n",
      "2m 25s (- 35m 54s) (4750 6%) 1.1567\n",
      "2m 27s (- 35m 51s) (4800 6%) 1.0489\n",
      "2m 28s (- 35m 53s) (4850 6%) 1.1938\n",
      "2m 30s (- 35m 54s) (4900 6%) 1.1158\n",
      "2m 32s (- 35m 54s) (4950 6%) 1.0136\n",
      "2m 33s (- 35m 54s) (5000 6%) 1.0416\n",
      "2m 35s (- 35m 56s) (5050 6%) 1.0655\n",
      "2m 37s (- 36m 1s) (5100 6%) 1.0629\n",
      "2m 39s (- 36m 4s) (5150 6%) 0.9634\n",
      "2m 41s (- 36m 4s) (5200 6%) 1.0085\n",
      "2m 43s (- 36m 9s) (5250 7%) 1.1212\n",
      "2m 45s (- 36m 12s) (5300 7%) 0.9015\n",
      "2m 47s (- 36m 16s) (5350 7%) 0.9709\n",
      "2m 48s (- 36m 17s) (5400 7%) 1.0142\n",
      "2m 50s (- 36m 14s) (5450 7%) 1.2563\n",
      "2m 52s (- 36m 14s) (5500 7%) 1.1080\n",
      "2m 53s (- 36m 11s) (5550 7%) 1.0693\n",
      "2m 55s (- 36m 8s) (5600 7%) 1.0549\n",
      "2m 56s (- 36m 7s) (5650 7%) 0.8432\n",
      "2m 58s (- 36m 5s) (5700 7%) 1.0163\n",
      "3m 0s (- 36m 8s) (5750 7%) 0.9653\n",
      "3m 1s (- 36m 8s) (5800 7%) 0.8277\n",
      "3m 3s (- 36m 5s) (5850 7%) 0.8982\n",
      "3m 4s (- 36m 3s) (5900 7%) 1.0143\n",
      "3m 6s (- 36m 0s) (5950 7%) 0.9983\n",
      "3m 8s (- 36m 3s) (6000 8%) 0.9747\n",
      "3m 9s (- 36m 2s) (6050 8%) 0.9442\n",
      "3m 11s (- 36m 0s) (6100 8%) 0.8692\n",
      "3m 12s (- 35m 58s) (6150 8%) 0.8365\n",
      "3m 14s (- 35m 59s) (6200 8%) 0.8814\n",
      "3m 16s (- 36m 3s) (6250 8%) 0.8345\n",
      "3m 18s (- 36m 3s) (6300 8%) 0.7202\n",
      "3m 19s (- 36m 2s) (6350 8%) 0.8213\n",
      "3m 21s (- 36m 0s) (6400 8%) 0.8814\n",
      "3m 22s (- 35m 57s) (6450 8%) 0.8085\n",
      "3m 24s (- 35m 55s) (6500 8%) 0.7748\n",
      "3m 26s (- 35m 54s) (6550 8%) 0.7421\n",
      "3m 27s (- 35m 52s) (6600 8%) 0.7250\n",
      "3m 29s (- 35m 53s) (6650 8%) 0.7027\n",
      "3m 31s (- 35m 59s) (6700 8%) 0.7237\n",
      "3m 33s (- 36m 3s) (6750 9%) 0.6876\n",
      "3m 35s (- 36m 4s) (6800 9%) 0.8416\n",
      "3m 37s (- 36m 2s) (6850 9%) 0.8753\n",
      "3m 38s (- 35m 59s) (6900 9%) 0.7538\n",
      "3m 40s (- 35m 58s) (6950 9%) 0.6903\n",
      "3m 41s (- 35m 56s) (7000 9%) 0.7840\n",
      "3m 43s (- 35m 54s) (7050 9%) 0.5941\n",
      "3m 44s (- 35m 51s) (7100 9%) 0.6364\n",
      "3m 46s (- 35m 49s) (7150 9%) 0.7756\n",
      "3m 47s (- 35m 46s) (7200 9%) 0.6775\n",
      "3m 49s (- 35m 43s) (7250 9%) 0.6433\n",
      "3m 50s (- 35m 41s) (7300 9%) 0.5877\n",
      "3m 52s (- 35m 41s) (7350 9%) 0.7281\n",
      "3m 54s (- 35m 39s) (7400 9%) 0.6435\n",
      "3m 55s (- 35m 36s) (7450 9%) 0.6999\n",
      "3m 57s (- 35m 33s) (7500 10%) 0.8527\n",
      "3m 58s (- 35m 31s) (7550 10%) 0.7178\n",
      "4m 0s (- 35m 28s) (7600 10%) 0.6315\n",
      "4m 1s (- 35m 25s) (7650 10%) 0.6251\n",
      "4m 2s (- 35m 22s) (7700 10%) 0.6331\n",
      "4m 4s (- 35m 19s) (7750 10%) 0.5206\n",
      "4m 5s (- 35m 17s) (7800 10%) 0.6330\n",
      "4m 7s (- 35m 15s) (7850 10%) 0.6337\n",
      "4m 8s (- 35m 13s) (7900 10%) 0.5321\n",
      "4m 10s (- 35m 11s) (7950 10%) 0.5807\n",
      "4m 11s (- 35m 8s) (8000 10%) 0.6229\n",
      "4m 13s (- 35m 5s) (8050 10%) 0.4747\n",
      "4m 14s (- 35m 3s) (8100 10%) 0.5455\n",
      "4m 16s (- 35m 0s) (8150 10%) 0.5997\n",
      "4m 17s (- 34m 57s) (8200 10%) 0.4973\n",
      "4m 18s (- 34m 54s) (8250 11%) 0.5234\n",
      "4m 20s (- 34m 52s) (8300 11%) 0.5986\n",
      "4m 21s (- 34m 50s) (8350 11%) 0.5503\n",
      "4m 23s (- 34m 47s) (8400 11%) 0.5910\n",
      "4m 24s (- 34m 45s) (8450 11%) 0.6683\n",
      "4m 26s (- 34m 43s) (8500 11%) 0.4928\n",
      "4m 27s (- 34m 41s) (8550 11%) 0.5400\n",
      "4m 29s (- 34m 38s) (8600 11%) 0.4562\n",
      "4m 30s (- 34m 36s) (8650 11%) 0.4605\n",
      "4m 32s (- 34m 35s) (8700 11%) 0.4221\n",
      "4m 33s (- 34m 33s) (8750 11%) 0.4696\n",
      "4m 35s (- 34m 30s) (8800 11%) 0.4519\n",
      "4m 36s (- 34m 27s) (8850 11%) 0.4582\n",
      "4m 38s (- 34m 26s) (8900 11%) 0.5128\n",
      "4m 39s (- 34m 24s) (8950 11%) 0.4656\n",
      "4m 41s (- 34m 21s) (9000 12%) 0.4967\n",
      "4m 42s (- 34m 20s) (9050 12%) 0.4095\n",
      "4m 44s (- 34m 18s) (9100 12%) 0.4926\n",
      "4m 45s (- 34m 16s) (9150 12%) 0.4178\n",
      "4m 47s (- 34m 14s) (9200 12%) 0.4436\n",
      "4m 48s (- 34m 12s) (9250 12%) 0.5020\n",
      "4m 50s (- 34m 10s) (9300 12%) 0.4188\n",
      "4m 51s (- 34m 7s) (9350 12%) 0.4701\n",
      "4m 53s (- 34m 5s) (9400 12%) 0.4451\n",
      "4m 54s (- 34m 3s) (9450 12%) 0.3838\n",
      "4m 56s (- 34m 1s) (9500 12%) 0.4748\n",
      "4m 57s (- 34m 0s) (9550 12%) 0.4432\n",
      "4m 59s (- 33m 58s) (9600 12%) 0.3893\n",
      "5m 0s (- 33m 56s) (9650 12%) 0.4964\n",
      "5m 2s (- 33m 55s) (9700 12%) 0.4929\n",
      "5m 3s (- 33m 53s) (9750 13%) 0.3863\n",
      "5m 5s (- 33m 51s) (9800 13%) 0.4211\n",
      "5m 6s (- 33m 48s) (9850 13%) 0.3801\n",
      "5m 8s (- 33m 46s) (9900 13%) 0.3923\n",
      "5m 9s (- 33m 45s) (9950 13%) 0.4031\n",
      "5m 11s (- 33m 42s) (10000 13%) 0.3266\n",
      "5m 12s (- 33m 40s) (10050 13%) 0.3664\n",
      "5m 14s (- 33m 39s) (10100 13%) 0.3918\n",
      "5m 15s (- 33m 37s) (10150 13%) 0.4194\n",
      "5m 17s (- 33m 35s) (10200 13%) 0.3751\n",
      "5m 18s (- 33m 33s) (10250 13%) 0.3202\n",
      "5m 20s (- 33m 30s) (10300 13%) 0.3386\n",
      "5m 21s (- 33m 28s) (10350 13%) 0.3595\n",
      "5m 23s (- 33m 27s) (10400 13%) 0.3500\n",
      "5m 24s (- 33m 25s) (10450 13%) 0.3410\n",
      "5m 26s (- 33m 23s) (10500 14%) 0.4208\n",
      "5m 27s (- 33m 21s) (10550 14%) 0.3523\n",
      "5m 29s (- 33m 20s) (10600 14%) 0.3272\n",
      "5m 30s (- 33m 19s) (10650 14%) 0.3536\n",
      "5m 32s (- 33m 18s) (10700 14%) 0.4195\n",
      "5m 34s (- 33m 16s) (10750 14%) 0.3742\n",
      "5m 35s (- 33m 15s) (10800 14%) 0.3394\n",
      "5m 37s (- 33m 13s) (10850 14%) 0.4072\n",
      "5m 38s (- 33m 12s) (10900 14%) 0.2831\n",
      "5m 40s (- 33m 10s) (10950 14%) 0.4040\n",
      "5m 41s (- 33m 9s) (11000 14%) 0.3447\n",
      "5m 43s (- 33m 7s) (11050 14%) 0.3086\n",
      "5m 44s (- 33m 5s) (11100 14%) 0.2799\n",
      "5m 46s (- 33m 4s) (11150 14%) 0.3296\n",
      "5m 48s (- 33m 2s) (11200 14%) 0.3051\n",
      "5m 49s (- 33m 0s) (11250 15%) 0.3314\n",
      "5m 51s (- 32m 59s) (11300 15%) 0.3692\n",
      "5m 52s (- 32m 57s) (11350 15%) 0.3023\n",
      "5m 54s (- 32m 56s) (11400 15%) 0.3185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5m 56s (- 32m 56s) (11450 15%) 0.2991\n",
      "5m 57s (- 32m 54s) (11500 15%) 0.3199\n",
      "5m 59s (- 32m 52s) (11550 15%) 0.2965\n",
      "6m 0s (- 32m 51s) (11600 15%) 0.3098\n",
      "6m 2s (- 32m 50s) (11650 15%) 0.3157\n",
      "6m 4s (- 32m 49s) (11700 15%) 0.3073\n",
      "6m 5s (- 32m 47s) (11750 15%) 0.2911\n",
      "6m 7s (- 32m 46s) (11800 15%) 0.3037\n",
      "6m 8s (- 32m 44s) (11850 15%) 0.2312\n",
      "6m 10s (- 32m 43s) (11900 15%) 0.2807\n",
      "6m 11s (- 32m 42s) (11950 15%) 0.2551\n",
      "6m 13s (- 32m 40s) (12000 16%) 0.2504\n",
      "6m 15s (- 32m 39s) (12050 16%) 0.2473\n",
      "6m 16s (- 32m 37s) (12100 16%) 0.2821\n",
      "6m 18s (- 32m 36s) (12150 16%) 0.3002\n",
      "6m 19s (- 32m 34s) (12200 16%) 0.3085\n",
      "6m 21s (- 32m 33s) (12250 16%) 0.2922\n",
      "6m 22s (- 32m 32s) (12300 16%) 0.2606\n",
      "6m 24s (- 32m 30s) (12350 16%) 0.3845\n",
      "6m 26s (- 32m 28s) (12400 16%) 0.2678\n",
      "6m 27s (- 32m 27s) (12450 16%) 0.3001\n",
      "6m 29s (- 32m 25s) (12500 16%) 0.2560\n",
      "6m 30s (- 32m 24s) (12550 16%) 0.2808\n",
      "6m 32s (- 32m 22s) (12600 16%) 0.2492\n",
      "6m 33s (- 32m 20s) (12650 16%) 0.2723\n",
      "6m 35s (- 32m 19s) (12700 16%) 0.2881\n",
      "6m 36s (- 32m 18s) (12750 17%) 0.3450\n",
      "6m 38s (- 32m 17s) (12800 17%) 0.3224\n",
      "6m 40s (- 32m 16s) (12850 17%) 0.2586\n",
      "6m 41s (- 32m 15s) (12900 17%) 0.2725\n",
      "6m 43s (- 32m 13s) (12950 17%) 0.2732\n",
      "6m 45s (- 32m 12s) (13000 17%) 0.4111\n",
      "6m 46s (- 32m 11s) (13050 17%) 0.2954\n",
      "6m 48s (- 32m 9s) (13100 17%) 0.2698\n",
      "6m 49s (- 32m 8s) (13150 17%) 0.2450\n",
      "6m 51s (- 32m 6s) (13200 17%) 0.2789\n",
      "6m 53s (- 32m 5s) (13250 17%) 0.3867\n",
      "6m 54s (- 32m 4s) (13300 17%) 0.2277\n",
      "6m 56s (- 32m 2s) (13350 17%) 0.2614\n",
      "6m 58s (- 32m 1s) (13400 17%) 0.2131\n",
      "6m 59s (- 32m 0s) (13450 17%) 0.2418\n",
      "7m 1s (- 31m 59s) (13500 18%) 0.2498\n",
      "7m 2s (- 31m 57s) (13550 18%) 0.3078\n",
      "7m 4s (- 31m 55s) (13600 18%) 0.2735\n",
      "7m 5s (- 31m 53s) (13650 18%) 0.2296\n",
      "7m 7s (- 31m 52s) (13700 18%) 0.2791\n",
      "7m 8s (- 31m 50s) (13750 18%) 0.2390\n",
      "7m 10s (- 31m 49s) (13800 18%) 0.3652\n",
      "7m 12s (- 31m 48s) (13850 18%) 0.2449\n",
      "7m 13s (- 31m 47s) (13900 18%) 0.2263\n",
      "7m 15s (- 31m 45s) (13950 18%) 0.2333\n",
      "7m 17s (- 31m 44s) (14000 18%) 0.2169\n",
      "7m 18s (- 31m 42s) (14050 18%) 0.1932\n",
      "7m 20s (- 31m 41s) (14100 18%) 0.2512\n",
      "7m 21s (- 31m 40s) (14150 18%) 0.2330\n",
      "7m 23s (- 31m 39s) (14200 18%) 0.2538\n",
      "7m 24s (- 31m 37s) (14250 19%) 0.2566\n",
      "7m 26s (- 31m 35s) (14300 19%) 0.2462\n",
      "7m 28s (- 31m 33s) (14350 19%) 0.3029\n",
      "7m 29s (- 31m 32s) (14400 19%) 0.2326\n",
      "7m 31s (- 31m 31s) (14450 19%) 0.2761\n",
      "7m 33s (- 31m 30s) (14500 19%) 0.2568\n",
      "7m 34s (- 31m 29s) (14550 19%) 0.3326\n",
      "7m 36s (- 31m 28s) (14600 19%) 0.2269\n",
      "7m 38s (- 31m 26s) (14650 19%) 0.2620\n",
      "7m 39s (- 31m 25s) (14700 19%) 0.2042\n",
      "7m 41s (- 31m 24s) (14750 19%) 0.3049\n",
      "7m 42s (- 31m 23s) (14800 19%) 0.2382\n",
      "7m 44s (- 31m 22s) (14850 19%) 0.2263\n",
      "7m 46s (- 31m 20s) (14900 19%) 0.2467\n",
      "7m 47s (- 31m 19s) (14950 19%) 0.2223\n",
      "7m 49s (- 31m 18s) (15000 20%) 0.1986\n",
      "7m 51s (- 31m 16s) (15050 20%) 0.2712\n",
      "7m 52s (- 31m 14s) (15100 20%) 0.2261\n",
      "7m 53s (- 31m 12s) (15150 20%) 0.2138\n",
      "7m 55s (- 31m 9s) (15200 20%) 0.2318\n",
      "7m 56s (- 31m 8s) (15250 20%) 0.1674\n",
      "7m 58s (- 31m 6s) (15300 20%) 0.2030\n",
      "7m 59s (- 31m 4s) (15350 20%) 0.2487\n",
      "8m 1s (- 31m 2s) (15400 20%) 0.2268\n",
      "8m 3s (- 31m 1s) (15450 20%) 0.2303\n",
      "8m 4s (- 31m 0s) (15500 20%) 0.1947\n",
      "8m 6s (- 30m 58s) (15550 20%) 0.2297\n",
      "8m 7s (- 30m 57s) (15600 20%) 0.1910\n",
      "8m 9s (- 30m 56s) (15650 20%) 0.2472\n",
      "8m 11s (- 30m 54s) (15700 20%) 0.1880\n",
      "8m 12s (- 30m 53s) (15750 21%) 0.1958\n",
      "8m 14s (- 30m 51s) (15800 21%) 0.1693\n",
      "8m 15s (- 30m 50s) (15850 21%) 0.2047\n",
      "8m 17s (- 30m 48s) (15900 21%) 0.2149\n",
      "8m 18s (- 30m 47s) (15950 21%) 0.1858\n",
      "8m 20s (- 30m 45s) (16000 21%) 0.2000\n",
      "8m 22s (- 30m 44s) (16050 21%) 0.1600\n",
      "8m 23s (- 30m 42s) (16100 21%) 0.1618\n",
      "8m 25s (- 30m 41s) (16150 21%) 0.1892\n",
      "8m 26s (- 30m 39s) (16200 21%) 0.2037\n",
      "8m 28s (- 30m 38s) (16250 21%) 0.2200\n",
      "8m 30s (- 30m 36s) (16300 21%) 0.1922\n",
      "8m 31s (- 30m 35s) (16350 21%) 0.2051\n",
      "8m 33s (- 30m 33s) (16400 21%) 0.2788\n",
      "8m 34s (- 30m 32s) (16450 21%) 0.1892\n",
      "8m 36s (- 30m 30s) (16500 22%) 0.1698\n",
      "8m 38s (- 30m 29s) (16550 22%) 0.1976\n",
      "8m 39s (- 30m 28s) (16600 22%) 0.1448\n",
      "8m 41s (- 30m 26s) (16650 22%) 0.1531\n",
      "8m 42s (- 30m 25s) (16700 22%) 0.2613\n",
      "8m 44s (- 30m 24s) (16750 22%) 0.2004\n",
      "8m 45s (- 30m 22s) (16800 22%) 0.1899\n",
      "8m 47s (- 30m 20s) (16850 22%) 0.2575\n",
      "8m 49s (- 30m 19s) (16900 22%) 0.2276\n",
      "8m 50s (- 30m 18s) (16950 22%) 0.2017\n",
      "8m 52s (- 30m 16s) (17000 22%) 0.2104\n",
      "8m 54s (- 30m 15s) (17050 22%) 0.2282\n",
      "8m 55s (- 30m 14s) (17100 22%) 0.2406\n",
      "8m 57s (- 30m 13s) (17150 22%) 0.2294\n",
      "8m 59s (- 30m 11s) (17200 22%) 0.1906\n",
      "9m 0s (- 30m 10s) (17250 23%) 0.1832\n",
      "9m 2s (- 30m 8s) (17300 23%) 0.1712\n",
      "9m 3s (- 30m 6s) (17350 23%) 0.2255\n",
      "9m 5s (- 30m 5s) (17400 23%) 0.1655\n",
      "9m 6s (- 30m 3s) (17450 23%) 0.1610\n",
      "9m 8s (- 30m 2s) (17500 23%) 0.2181\n",
      "9m 10s (- 30m 1s) (17550 23%) 0.1729\n",
      "9m 11s (- 29m 59s) (17600 23%) 0.1705\n",
      "9m 13s (- 29m 58s) (17650 23%) 0.1564\n",
      "9m 14s (- 29m 56s) (17700 23%) 0.1772\n",
      "9m 16s (- 29m 54s) (17750 23%) 0.1626\n",
      "9m 17s (- 29m 52s) (17800 23%) 0.1539\n",
      "9m 19s (- 29m 51s) (17850 23%) 0.2307\n",
      "9m 21s (- 29m 50s) (17900 23%) 0.1774\n",
      "9m 22s (- 29m 48s) (17950 23%) 0.1642\n",
      "9m 24s (- 29m 47s) (18000 24%) 0.1530\n",
      "9m 25s (- 29m 45s) (18050 24%) 0.1765\n",
      "9m 27s (- 29m 44s) (18100 24%) 0.2029\n",
      "9m 29s (- 29m 42s) (18150 24%) 0.1813\n",
      "9m 30s (- 29m 41s) (18200 24%) 0.1599\n",
      "9m 32s (- 29m 39s) (18250 24%) 0.1720\n",
      "9m 33s (- 29m 37s) (18300 24%) 0.2015\n",
      "9m 35s (- 29m 36s) (18350 24%) 0.1775\n",
      "9m 37s (- 29m 35s) (18400 24%) 0.1583\n",
      "9m 38s (- 29m 33s) (18450 24%) 0.1505\n",
      "9m 40s (- 29m 31s) (18500 24%) 0.1531\n",
      "9m 41s (- 29m 29s) (18550 24%) 0.1778\n",
      "9m 43s (- 29m 28s) (18600 24%) 0.1636\n",
      "9m 44s (- 29m 26s) (18650 24%) 0.1483\n",
      "9m 46s (- 29m 25s) (18700 24%) 0.1458\n",
      "9m 47s (- 29m 23s) (18750 25%) 0.1457\n",
      "9m 49s (- 29m 22s) (18800 25%) 0.1713\n",
      "9m 51s (- 29m 20s) (18850 25%) 0.1914\n",
      "9m 52s (- 29m 19s) (18900 25%) 0.1376\n",
      "9m 54s (- 29m 17s) (18950 25%) 0.1565\n",
      "9m 55s (- 29m 16s) (19000 25%) 0.1553\n",
      "9m 57s (- 29m 15s) (19050 25%) 0.2419\n",
      "9m 59s (- 29m 13s) (19100 25%) 0.1898\n",
      "10m 0s (- 29m 12s) (19150 25%) 0.1333\n",
      "10m 2s (- 29m 10s) (19200 25%) 0.1439\n",
      "10m 4s (- 29m 9s) (19250 25%) 0.1611\n",
      "10m 5s (- 29m 8s) (19300 25%) 0.2092\n",
      "10m 7s (- 29m 6s) (19350 25%) 0.1080\n",
      "10m 8s (- 29m 4s) (19400 25%) 0.2113\n",
      "10m 10s (- 29m 2s) (19450 25%) 0.1517\n",
      "10m 11s (- 29m 1s) (19500 26%) 0.1521\n",
      "10m 13s (- 29m 0s) (19550 26%) 0.1479\n",
      "10m 15s (- 28m 58s) (19600 26%) 0.1811\n",
      "10m 16s (- 28m 57s) (19650 26%) 0.1547\n",
      "10m 18s (- 28m 55s) (19700 26%) 0.1836\n",
      "10m 20s (- 28m 54s) (19750 26%) 0.1674\n",
      "10m 21s (- 28m 53s) (19800 26%) 0.2698\n",
      "10m 23s (- 28m 51s) (19850 26%) 0.1344\n",
      "10m 24s (- 28m 50s) (19900 26%) 0.1564\n",
      "10m 26s (- 28m 48s) (19950 26%) 0.1464\n",
      "10m 27s (- 28m 46s) (20000 26%) 0.1516\n",
      "10m 29s (- 28m 44s) (20050 26%) 0.1418\n",
      "10m 30s (- 28m 43s) (20100 26%) 0.1574\n",
      "10m 32s (- 28m 41s) (20150 26%) 0.1922\n",
      "10m 34s (- 28m 40s) (20200 26%) 0.1633\n",
      "10m 35s (- 28m 38s) (20250 27%) 0.1510\n",
      "10m 37s (- 28m 37s) (20300 27%) 0.1272\n",
      "10m 39s (- 28m 36s) (20350 27%) 0.1503\n",
      "10m 40s (- 28m 34s) (20400 27%) 0.1370\n",
      "10m 42s (- 28m 32s) (20450 27%) 0.1416\n",
      "10m 43s (- 28m 31s) (20500 27%) 0.1818\n",
      "10m 45s (- 28m 29s) (20550 27%) 0.1124\n",
      "10m 46s (- 28m 28s) (20600 27%) 0.1978\n",
      "10m 48s (- 28m 27s) (20650 27%) 0.1501\n",
      "10m 50s (- 28m 25s) (20700 27%) 0.1077\n",
      "10m 51s (- 28m 23s) (20750 27%) 0.1729\n",
      "10m 53s (- 28m 22s) (20800 27%) 0.1173\n",
      "10m 55s (- 28m 21s) (20850 27%) 0.1766\n",
      "10m 56s (- 28m 19s) (20900 27%) 0.1367\n",
      "10m 58s (- 28m 17s) (20950 27%) 0.1381\n",
      "10m 59s (- 28m 16s) (21000 28%) 0.1657\n",
      "11m 1s (- 28m 14s) (21050 28%) 0.1559\n",
      "11m 2s (- 28m 13s) (21100 28%) 0.1620\n",
      "11m 4s (- 28m 11s) (21150 28%) 0.1367\n",
      "11m 6s (- 28m 10s) (21200 28%) 0.1364\n",
      "11m 7s (- 28m 8s) (21250 28%) 0.1099\n",
      "11m 9s (- 28m 7s) (21300 28%) 0.1505\n",
      "11m 10s (- 28m 5s) (21350 28%) 0.1296\n",
      "11m 12s (- 28m 4s) (21400 28%) 0.1671\n",
      "11m 14s (- 28m 2s) (21450 28%) 0.1325\n",
      "11m 15s (- 28m 1s) (21500 28%) 0.1419\n",
      "11m 17s (- 27m 59s) (21550 28%) 0.1280\n",
      "11m 18s (- 27m 58s) (21600 28%) 0.1566\n",
      "11m 20s (- 27m 56s) (21650 28%) 0.1179\n",
      "11m 22s (- 27m 55s) (21700 28%) 0.1270\n",
      "11m 23s (- 27m 54s) (21750 28%) 0.1699\n",
      "11m 25s (- 27m 52s) (21800 29%) 0.1027\n",
      "11m 26s (- 27m 50s) (21850 29%) 0.1674\n",
      "11m 28s (- 27m 49s) (21900 29%) 0.1318\n",
      "11m 30s (- 27m 48s) (21950 29%) 0.1459\n",
      "11m 31s (- 27m 46s) (22000 29%) 0.1099\n",
      "11m 33s (- 27m 44s) (22050 29%) 0.1012\n",
      "11m 34s (- 27m 43s) (22100 29%) 0.1469\n",
      "11m 36s (- 27m 41s) (22150 29%) 0.1293\n",
      "11m 38s (- 27m 40s) (22200 29%) 0.1288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11m 39s (- 27m 38s) (22250 29%) 0.1619\n",
      "11m 41s (- 27m 36s) (22300 29%) 0.1168\n",
      "11m 42s (- 27m 35s) (22350 29%) 0.1230\n",
      "11m 44s (- 27m 33s) (22400 29%) 0.1105\n",
      "11m 45s (- 27m 31s) (22450 29%) 0.1380\n",
      "11m 47s (- 27m 30s) (22500 30%) 0.1291\n",
      "11m 48s (- 27m 28s) (22550 30%) 0.1151\n",
      "11m 50s (- 27m 27s) (22600 30%) 0.1458\n",
      "11m 52s (- 27m 26s) (22650 30%) 0.1498\n",
      "11m 53s (- 27m 24s) (22700 30%) 0.1187\n",
      "11m 55s (- 27m 22s) (22750 30%) 0.1107\n",
      "11m 56s (- 27m 21s) (22800 30%) 0.1212\n",
      "11m 58s (- 27m 19s) (22850 30%) 0.1527\n",
      "12m 0s (- 27m 18s) (22900 30%) 0.1167\n",
      "12m 1s (- 27m 16s) (22950 30%) 0.1018\n",
      "12m 3s (- 27m 15s) (23000 30%) 0.1227\n",
      "12m 4s (- 27m 13s) (23050 30%) 0.1391\n",
      "12m 6s (- 27m 12s) (23100 30%) 0.1124\n",
      "12m 8s (- 27m 10s) (23150 30%) 0.1142\n",
      "12m 9s (- 27m 9s) (23200 30%) 0.1094\n",
      "12m 11s (- 27m 7s) (23250 31%) 0.1158\n",
      "12m 12s (- 27m 6s) (23300 31%) 0.1207\n",
      "12m 14s (- 27m 4s) (23350 31%) 0.1118\n",
      "12m 16s (- 27m 3s) (23400 31%) 0.1257\n",
      "12m 17s (- 27m 1s) (23450 31%) 0.0738\n",
      "12m 19s (- 26m 59s) (23500 31%) 0.1256\n",
      "12m 20s (- 26m 58s) (23550 31%) 0.1583\n",
      "12m 22s (- 26m 56s) (23600 31%) 0.1171\n",
      "12m 23s (- 26m 55s) (23650 31%) 0.1065\n",
      "12m 25s (- 26m 53s) (23700 31%) 0.1408\n",
      "12m 27s (- 26m 52s) (23750 31%) 0.1278\n",
      "12m 28s (- 26m 50s) (23800 31%) 0.1220\n",
      "12m 30s (- 26m 48s) (23850 31%) 0.1111\n",
      "12m 31s (- 26m 47s) (23900 31%) 0.1366\n",
      "12m 33s (- 26m 45s) (23950 31%) 0.1137\n",
      "12m 34s (- 26m 43s) (24000 32%) 0.0945\n",
      "12m 36s (- 26m 41s) (24050 32%) 0.1077\n",
      "12m 37s (- 26m 39s) (24100 32%) 0.0920\n",
      "12m 39s (- 26m 38s) (24150 32%) 0.1004\n",
      "12m 40s (- 26m 36s) (24200 32%) 0.1016\n",
      "12m 42s (- 26m 34s) (24250 32%) 0.1212\n",
      "12m 43s (- 26m 33s) (24300 32%) 0.0965\n",
      "12m 45s (- 26m 32s) (24350 32%) 0.1039\n",
      "12m 47s (- 26m 30s) (24400 32%) 0.1223\n",
      "12m 48s (- 26m 29s) (24450 32%) 0.1160\n",
      "12m 50s (- 26m 27s) (24500 32%) 0.0967\n",
      "12m 52s (- 26m 26s) (24550 32%) 0.1147\n",
      "12m 53s (- 26m 25s) (24600 32%) 0.1107\n",
      "12m 55s (- 26m 23s) (24650 32%) 0.1321\n",
      "12m 57s (- 26m 22s) (24700 32%) 0.0931\n",
      "12m 58s (- 26m 20s) (24750 33%) 0.0951\n",
      "13m 0s (- 26m 19s) (24800 33%) 0.0953\n",
      "13m 1s (- 26m 17s) (24850 33%) 0.0872\n",
      "13m 3s (- 26m 16s) (24900 33%) 0.1067\n",
      "13m 5s (- 26m 15s) (24950 33%) 0.1377\n",
      "13m 6s (- 26m 13s) (25000 33%) 0.1058\n",
      "13m 8s (- 26m 12s) (25050 33%) 0.0844\n",
      "13m 9s (- 26m 10s) (25100 33%) 0.0921\n",
      "13m 11s (- 26m 9s) (25150 33%) 0.1005\n",
      "13m 13s (- 26m 7s) (25200 33%) 0.1097\n",
      "13m 14s (- 26m 6s) (25250 33%) 0.0792\n",
      "13m 16s (- 26m 4s) (25300 33%) 0.1077\n",
      "13m 17s (- 26m 2s) (25350 33%) 0.1225\n",
      "13m 19s (- 26m 1s) (25400 33%) 0.0769\n",
      "13m 21s (- 25m 59s) (25450 33%) 0.1090\n",
      "13m 22s (- 25m 58s) (25500 34%) 0.1057\n",
      "13m 24s (- 25m 57s) (25550 34%) 0.0964\n",
      "13m 26s (- 25m 55s) (25600 34%) 0.1156\n",
      "13m 27s (- 25m 53s) (25650 34%) 0.1099\n",
      "13m 29s (- 25m 52s) (25700 34%) 0.0675\n",
      "13m 30s (- 25m 50s) (25750 34%) 0.0785\n",
      "13m 32s (- 25m 49s) (25800 34%) 0.1605\n",
      "13m 33s (- 25m 47s) (25850 34%) 0.0826\n",
      "13m 35s (- 25m 45s) (25900 34%) 0.1144\n",
      "13m 37s (- 25m 44s) (25950 34%) 0.1081\n",
      "13m 38s (- 25m 42s) (26000 34%) 0.1148\n",
      "13m 40s (- 25m 41s) (26050 34%) 0.1005\n",
      "13m 41s (- 25m 39s) (26100 34%) 0.1015\n",
      "13m 43s (- 25m 38s) (26150 34%) 0.1803\n",
      "13m 45s (- 25m 36s) (26200 34%) 0.1272\n",
      "13m 46s (- 25m 35s) (26250 35%) 0.1113\n",
      "13m 48s (- 25m 33s) (26300 35%) 0.1045\n",
      "13m 49s (- 25m 32s) (26350 35%) 0.0975\n",
      "13m 51s (- 25m 30s) (26400 35%) 0.0964\n",
      "13m 53s (- 25m 29s) (26450 35%) 0.0990\n",
      "13m 54s (- 25m 27s) (26500 35%) 0.1050\n",
      "13m 56s (- 25m 26s) (26550 35%) 0.0939\n",
      "13m 57s (- 25m 24s) (26600 35%) 0.1134\n",
      "13m 59s (- 25m 23s) (26650 35%) 0.0927\n",
      "14m 1s (- 25m 21s) (26700 35%) 0.0962\n",
      "14m 2s (- 25m 20s) (26750 35%) 0.0996\n",
      "14m 4s (- 25m 18s) (26800 35%) 0.1012\n",
      "14m 6s (- 25m 17s) (26850 35%) 0.0864\n",
      "14m 7s (- 25m 15s) (26900 35%) 0.0824\n",
      "14m 9s (- 25m 14s) (26950 35%) 0.0910\n",
      "14m 10s (- 25m 12s) (27000 36%) 0.0806\n",
      "14m 12s (- 25m 10s) (27050 36%) 0.0932\n",
      "14m 13s (- 25m 9s) (27100 36%) 0.1013\n",
      "14m 15s (- 25m 7s) (27150 36%) 0.0901\n",
      "14m 17s (- 25m 6s) (27200 36%) 0.1054\n",
      "14m 18s (- 25m 4s) (27250 36%) 0.0975\n",
      "14m 20s (- 25m 3s) (27300 36%) 0.0869\n",
      "14m 21s (- 25m 1s) (27350 36%) 0.0859\n",
      "14m 23s (- 25m 0s) (27400 36%) 0.0975\n",
      "14m 25s (- 24m 58s) (27450 36%) 0.1014\n",
      "14m 26s (- 24m 57s) (27500 36%) 0.0854\n",
      "14m 28s (- 24m 55s) (27550 36%) 0.0723\n",
      "14m 30s (- 24m 54s) (27600 36%) 0.0779\n",
      "14m 32s (- 24m 53s) (27650 36%) 0.0870\n",
      "14m 40s (- 25m 3s) (27700 36%) 0.0723\n",
      "14m 42s (- 25m 1s) (27750 37%) 0.0896\n",
      "14m 43s (- 25m 0s) (27800 37%) 0.1052\n",
      "14m 44s (- 24m 58s) (27850 37%) 0.1072\n",
      "14m 46s (- 24m 56s) (27900 37%) 0.0822\n",
      "14m 47s (- 24m 54s) (27950 37%) 0.0989\n",
      "14m 49s (- 24m 52s) (28000 37%) 0.0748\n",
      "14m 51s (- 24m 51s) (28050 37%) 0.0887\n",
      "14m 52s (- 24m 50s) (28100 37%) 0.0941\n",
      "14m 54s (- 24m 49s) (28150 37%) 0.1247\n",
      "14m 56s (- 24m 47s) (28200 37%) 0.1797\n",
      "14m 57s (- 24m 45s) (28250 37%) 0.0976\n",
      "14m 58s (- 24m 43s) (28300 37%) 0.0855\n",
      "15m 0s (- 24m 41s) (28350 37%) 0.0985\n",
      "15m 1s (- 24m 39s) (28400 37%) 0.1019\n",
      "15m 2s (- 24m 37s) (28450 37%) 0.0706\n",
      "15m 4s (- 24m 35s) (28500 38%) 0.0826\n",
      "15m 5s (- 24m 33s) (28550 38%) 0.0954\n",
      "15m 7s (- 24m 31s) (28600 38%) 0.1090\n",
      "15m 8s (- 24m 29s) (28650 38%) 0.0700\n",
      "15m 9s (- 24m 27s) (28700 38%) 0.0716\n",
      "15m 11s (- 24m 25s) (28750 38%) 0.0765\n",
      "15m 12s (- 24m 23s) (28800 38%) 0.0706\n",
      "15m 13s (- 24m 22s) (28850 38%) 0.0793\n",
      "15m 15s (- 24m 20s) (28900 38%) 0.0773\n",
      "15m 16s (- 24m 18s) (28950 38%) 0.0575\n",
      "15m 18s (- 24m 16s) (29000 38%) 0.0875\n",
      "15m 19s (- 24m 14s) (29050 38%) 0.0741\n",
      "15m 20s (- 24m 12s) (29100 38%) 0.0819\n",
      "15m 22s (- 24m 10s) (29150 38%) 0.0853\n",
      "15m 23s (- 24m 8s) (29200 38%) 0.0949\n",
      "15m 24s (- 24m 6s) (29250 39%) 0.0993\n",
      "15m 26s (- 24m 4s) (29300 39%) 0.0683\n",
      "15m 27s (- 24m 2s) (29350 39%) 0.0931\n",
      "15m 28s (- 24m 0s) (29400 39%) 0.0750\n",
      "15m 30s (- 23m 58s) (29450 39%) 0.0427\n",
      "15m 31s (- 23m 56s) (29500 39%) 0.1228\n",
      "15m 33s (- 23m 55s) (29550 39%) 0.0982\n",
      "15m 34s (- 23m 53s) (29600 39%) 0.0580\n",
      "15m 35s (- 23m 51s) (29650 39%) 0.0927\n",
      "15m 37s (- 23m 49s) (29700 39%) 0.0675\n",
      "15m 38s (- 23m 47s) (29750 39%) 0.0671\n",
      "15m 40s (- 23m 45s) (29800 39%) 0.0678\n",
      "15m 41s (- 23m 43s) (29850 39%) 0.0898\n",
      "15m 42s (- 23m 41s) (29900 39%) 0.0561\n",
      "15m 44s (- 23m 40s) (29950 39%) 0.0628\n",
      "15m 45s (- 23m 38s) (30000 40%) 0.0736\n",
      "15m 46s (- 23m 36s) (30050 40%) 0.0830\n",
      "15m 48s (- 23m 34s) (30100 40%) 0.0923\n",
      "15m 49s (- 23m 32s) (30150 40%) 0.0666\n",
      "15m 51s (- 23m 30s) (30200 40%) 0.0660\n",
      "15m 52s (- 23m 28s) (30250 40%) 0.0706\n",
      "15m 53s (- 23m 27s) (30300 40%) 0.0724\n",
      "15m 55s (- 23m 25s) (30350 40%) 0.0722\n",
      "15m 56s (- 23m 23s) (30400 40%) 0.1213\n",
      "15m 58s (- 23m 21s) (30450 40%) 0.0574\n",
      "15m 59s (- 23m 20s) (30500 40%) 0.0620\n",
      "16m 1s (- 23m 18s) (30550 40%) 0.0854\n",
      "16m 2s (- 23m 16s) (30600 40%) 0.0703\n",
      "16m 4s (- 23m 14s) (30650 40%) 0.0561\n",
      "16m 5s (- 23m 13s) (30700 40%) 0.0811\n",
      "16m 6s (- 23m 11s) (30750 41%) 0.0969\n",
      "16m 8s (- 23m 9s) (30800 41%) 0.0607\n",
      "16m 9s (- 23m 7s) (30850 41%) 0.0786\n",
      "16m 11s (- 23m 5s) (30900 41%) 0.0812\n",
      "16m 12s (- 23m 3s) (30950 41%) 0.0747\n",
      "16m 13s (- 23m 2s) (31000 41%) 0.0541\n",
      "16m 15s (- 23m 0s) (31050 41%) 0.0592\n",
      "16m 16s (- 22m 58s) (31100 41%) 0.0424\n",
      "16m 17s (- 22m 56s) (31150 41%) 0.0729\n",
      "16m 19s (- 22m 55s) (31200 41%) 0.0665\n",
      "16m 20s (- 22m 53s) (31250 41%) 0.0732\n",
      "16m 22s (- 22m 51s) (31300 41%) 0.0576\n",
      "16m 23s (- 22m 49s) (31350 41%) 0.0837\n",
      "16m 25s (- 22m 47s) (31400 41%) 0.0838\n",
      "16m 26s (- 22m 46s) (31450 41%) 0.0772\n",
      "16m 27s (- 22m 44s) (31500 42%) 0.0780\n",
      "16m 29s (- 22m 42s) (31550 42%) 0.0849\n",
      "16m 30s (- 22m 40s) (31600 42%) 0.0751\n",
      "16m 32s (- 22m 38s) (31650 42%) 0.1612\n",
      "16m 33s (- 22m 36s) (31700 42%) 0.1540\n",
      "16m 34s (- 22m 35s) (31750 42%) 0.0942\n",
      "16m 36s (- 22m 33s) (31800 42%) 0.0718\n",
      "16m 37s (- 22m 31s) (31850 42%) 0.0678\n",
      "16m 39s (- 22m 29s) (31900 42%) 0.0718\n",
      "16m 40s (- 22m 27s) (31950 42%) 0.0630\n",
      "16m 41s (- 22m 26s) (32000 42%) 0.0457\n",
      "16m 43s (- 22m 24s) (32050 42%) 0.0697\n",
      "16m 44s (- 22m 22s) (32100 42%) 0.1097\n",
      "16m 45s (- 22m 20s) (32150 42%) 0.0695\n",
      "16m 47s (- 22m 18s) (32200 42%) 0.0764\n",
      "16m 48s (- 22m 17s) (32250 43%) 0.0574\n",
      "16m 50s (- 22m 15s) (32300 43%) 0.0784\n",
      "16m 51s (- 22m 13s) (32350 43%) 0.0800\n",
      "16m 53s (- 22m 11s) (32400 43%) 0.0707\n",
      "16m 54s (- 22m 10s) (32450 43%) 0.0771\n",
      "16m 55s (- 22m 8s) (32500 43%) 0.0637\n",
      "16m 57s (- 22m 6s) (32550 43%) 0.0805\n",
      "16m 58s (- 22m 4s) (32600 43%) 0.0851\n",
      "17m 0s (- 22m 3s) (32650 43%) 0.0614\n",
      "17m 1s (- 22m 1s) (32700 43%) 0.0674\n",
      "17m 2s (- 21m 59s) (32750 43%) 0.0526\n",
      "17m 4s (- 21m 57s) (32800 43%) 0.0740\n",
      "17m 5s (- 21m 55s) (32850 43%) 0.0714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17m 6s (- 21m 53s) (32900 43%) 0.0344\n",
      "17m 8s (- 21m 52s) (32950 43%) 0.0557\n",
      "17m 9s (- 21m 50s) (33000 44%) 0.0783\n",
      "17m 10s (- 21m 48s) (33050 44%) 0.0846\n",
      "17m 12s (- 21m 46s) (33100 44%) 0.0612\n",
      "17m 13s (- 21m 45s) (33150 44%) 0.0683\n",
      "17m 15s (- 21m 43s) (33200 44%) 0.0598\n",
      "17m 16s (- 21m 41s) (33250 44%) 0.0711\n",
      "17m 17s (- 21m 39s) (33300 44%) 0.0709\n",
      "17m 19s (- 21m 38s) (33350 44%) 0.1000\n",
      "17m 20s (- 21m 36s) (33400 44%) 0.0660\n",
      "17m 22s (- 21m 34s) (33450 44%) 0.0805\n",
      "17m 23s (- 21m 32s) (33500 44%) 0.0538\n",
      "17m 24s (- 21m 30s) (33550 44%) 0.0660\n",
      "17m 26s (- 21m 28s) (33600 44%) 0.0696\n",
      "17m 27s (- 21m 27s) (33650 44%) 0.0521\n",
      "17m 28s (- 21m 25s) (33700 44%) 0.0687\n",
      "17m 30s (- 21m 23s) (33750 45%) 0.0767\n",
      "17m 31s (- 21m 21s) (33800 45%) 0.0841\n",
      "17m 33s (- 21m 20s) (33850 45%) 0.0600\n",
      "17m 34s (- 21m 18s) (33900 45%) 0.0537\n",
      "17m 36s (- 21m 16s) (33950 45%) 0.0472\n",
      "17m 37s (- 21m 15s) (34000 45%) 0.0529\n",
      "17m 38s (- 21m 13s) (34050 45%) 0.0627\n",
      "17m 40s (- 21m 11s) (34100 45%) 0.0499\n",
      "17m 41s (- 21m 9s) (34150 45%) 0.0503\n",
      "17m 42s (- 21m 8s) (34200 45%) 0.0515\n",
      "17m 44s (- 21m 6s) (34250 45%) 0.0398\n",
      "17m 45s (- 21m 4s) (34300 45%) 0.0848\n",
      "17m 47s (- 21m 2s) (34350 45%) 0.0481\n",
      "17m 48s (- 21m 1s) (34400 45%) 0.0482\n",
      "17m 50s (- 20m 59s) (34450 45%) 0.3194\n",
      "17m 51s (- 20m 57s) (34500 46%) 0.0819\n",
      "17m 52s (- 20m 56s) (34550 46%) 0.0699\n",
      "17m 54s (- 20m 54s) (34600 46%) 0.0728\n",
      "17m 55s (- 20m 52s) (34650 46%) 0.0461\n",
      "17m 57s (- 20m 50s) (34700 46%) 0.0713\n",
      "17m 58s (- 20m 49s) (34750 46%) 0.0834\n",
      "17m 59s (- 20m 47s) (34800 46%) 0.0598\n",
      "18m 1s (- 20m 45s) (34850 46%) 0.0474\n",
      "18m 2s (- 20m 43s) (34900 46%) 0.0746\n",
      "18m 4s (- 20m 42s) (34950 46%) 0.0782\n",
      "18m 5s (- 20m 41s) (35000 46%) 0.0739\n",
      "18m 7s (- 20m 40s) (35050 46%) 0.0482\n",
      "18m 9s (- 20m 38s) (35100 46%) 0.0428\n",
      "18m 10s (- 20m 36s) (35150 46%) 0.0915\n",
      "18m 11s (- 20m 34s) (35200 46%) 0.0661\n",
      "18m 13s (- 20m 33s) (35250 47%) 0.0627\n",
      "18m 15s (- 20m 32s) (35300 47%) 0.0561\n",
      "18m 17s (- 20m 30s) (35350 47%) 0.0540\n",
      "18m 18s (- 20m 28s) (35400 47%) 0.0583\n",
      "18m 19s (- 20m 27s) (35450 47%) 0.0661\n",
      "18m 21s (- 20m 25s) (35500 47%) 0.0523\n",
      "18m 23s (- 20m 24s) (35550 47%) 0.0730\n",
      "18m 25s (- 20m 23s) (35600 47%) 0.0506\n",
      "18m 26s (- 20m 21s) (35650 47%) 0.0625\n",
      "18m 27s (- 20m 19s) (35700 47%) 0.0523\n",
      "18m 29s (- 20m 17s) (35750 47%) 0.0512\n",
      "18m 31s (- 20m 16s) (35800 47%) 0.0639\n",
      "18m 33s (- 20m 15s) (35850 47%) 0.0318\n",
      "18m 34s (- 20m 13s) (35900 47%) 0.0330\n",
      "18m 35s (- 20m 11s) (35950 47%) 0.0500\n",
      "18m 37s (- 20m 10s) (36000 48%) 0.0634\n",
      "18m 39s (- 20m 9s) (36050 48%) 0.0670\n",
      "18m 41s (- 20m 7s) (36100 48%) 0.0457\n",
      "18m 42s (- 20m 6s) (36150 48%) 0.0523\n",
      "18m 43s (- 20m 4s) (36200 48%) 0.0539\n",
      "18m 45s (- 20m 2s) (36250 48%) 0.0512\n",
      "18m 46s (- 20m 1s) (36300 48%) 0.0728\n",
      "18m 48s (- 20m 0s) (36350 48%) 0.0468\n",
      "18m 50s (- 19m 58s) (36400 48%) 0.0713\n",
      "18m 51s (- 19m 56s) (36450 48%) 0.0559\n",
      "18m 52s (- 19m 54s) (36500 48%) 0.0595\n",
      "18m 54s (- 19m 53s) (36550 48%) 0.0704\n",
      "18m 56s (- 19m 52s) (36600 48%) 0.0664\n",
      "18m 58s (- 19m 50s) (36650 48%) 0.0390\n",
      "18m 59s (- 19m 49s) (36700 48%) 0.0717\n",
      "19m 0s (- 19m 47s) (36750 49%) 0.0345\n",
      "19m 2s (- 19m 46s) (36800 49%) 0.0684\n",
      "19m 4s (- 19m 45s) (36850 49%) 0.0478\n",
      "19m 6s (- 19m 43s) (36900 49%) 0.0389\n",
      "19m 7s (- 19m 41s) (36950 49%) 0.0518\n",
      "19m 8s (- 19m 39s) (37000 49%) 0.0289\n",
      "19m 10s (- 19m 38s) (37050 49%) 0.0556\n",
      "19m 12s (- 19m 37s) (37100 49%) 0.0278\n",
      "19m 13s (- 19m 35s) (37150 49%) 0.0485\n",
      "19m 15s (- 19m 33s) (37200 49%) 0.0945\n",
      "19m 16s (- 19m 31s) (37250 49%) 0.1092\n",
      "19m 18s (- 19m 30s) (37300 49%) 0.0402\n",
      "19m 20s (- 19m 29s) (37350 49%) 0.0542\n",
      "19m 21s (- 19m 27s) (37400 49%) 0.0638\n",
      "19m 22s (- 19m 26s) (37450 49%) 0.0519\n",
      "19m 24s (- 19m 24s) (37500 50%) 0.0552\n",
      "19m 26s (- 19m 23s) (37550 50%) 0.0460\n",
      "19m 28s (- 19m 21s) (37600 50%) 0.0513\n",
      "19m 29s (- 19m 20s) (37650 50%) 0.0551\n",
      "19m 30s (- 19m 18s) (37700 50%) 0.0575\n",
      "19m 32s (- 19m 16s) (37750 50%) 0.0680\n",
      "19m 34s (- 19m 15s) (37800 50%) 0.0583\n",
      "19m 36s (- 19m 14s) (37850 50%) 0.0392\n",
      "19m 37s (- 19m 12s) (37900 50%) 0.0498\n",
      "19m 39s (- 19m 11s) (37950 50%) 0.0460\n",
      "19m 40s (- 19m 9s) (38000 50%) 0.0442\n",
      "19m 42s (- 19m 8s) (38050 50%) 0.0497\n",
      "19m 44s (- 19m 7s) (38100 50%) 0.0511\n",
      "19m 45s (- 19m 5s) (38150 50%) 0.0329\n",
      "19m 47s (- 19m 3s) (38200 50%) 0.0672\n",
      "19m 48s (- 19m 1s) (38250 51%) 0.0411\n",
      "19m 50s (- 19m 0s) (38300 51%) 0.0394\n",
      "19m 52s (- 18m 59s) (38350 51%) 0.0316\n",
      "19m 53s (- 18m 57s) (38400 51%) 0.0335\n",
      "19m 55s (- 18m 56s) (38450 51%) 0.0312\n",
      "19m 56s (- 18m 54s) (38500 51%) 0.0379\n",
      "19m 58s (- 18m 52s) (38550 51%) 0.0636\n",
      "20m 0s (- 18m 51s) (38600 51%) 0.0467\n",
      "20m 1s (- 18m 50s) (38650 51%) 0.0316\n",
      "20m 3s (- 18m 48s) (38700 51%) 0.0593\n",
      "20m 4s (- 18m 46s) (38750 51%) 0.0626\n",
      "20m 5s (- 18m 44s) (38800 51%) 0.0378\n",
      "20m 7s (- 18m 43s) (38850 51%) 0.0422\n",
      "20m 8s (- 18m 41s) (38900 51%) 0.0615\n",
      "20m 10s (- 18m 40s) (38950 51%) 0.0336\n",
      "20m 11s (- 18m 38s) (39000 52%) 0.0604\n",
      "20m 13s (- 18m 36s) (39050 52%) 0.0616\n",
      "20m 14s (- 18m 35s) (39100 52%) 0.0354\n",
      "20m 16s (- 18m 33s) (39150 52%) 0.0463\n",
      "20m 17s (- 18m 31s) (39200 52%) 0.0594\n",
      "20m 18s (- 18m 30s) (39250 52%) 0.0339\n",
      "20m 20s (- 18m 28s) (39300 52%) 0.0800\n",
      "20m 21s (- 18m 26s) (39350 52%) 0.0928\n",
      "20m 23s (- 18m 25s) (39400 52%) 0.0837\n",
      "20m 24s (- 18m 23s) (39450 52%) 0.0371\n",
      "20m 26s (- 18m 22s) (39500 52%) 0.0418\n",
      "20m 27s (- 18m 20s) (39550 52%) 0.0614\n",
      "20m 29s (- 18m 18s) (39600 52%) 0.0419\n",
      "20m 30s (- 18m 17s) (39650 52%) 0.0454\n",
      "20m 32s (- 18m 15s) (39700 52%) 0.0282\n",
      "20m 33s (- 18m 13s) (39750 53%) 0.0316\n",
      "20m 35s (- 18m 12s) (39800 53%) 0.0337\n",
      "20m 36s (- 18m 10s) (39850 53%) 0.0687\n",
      "20m 38s (- 18m 9s) (39900 53%) 0.0548\n",
      "20m 39s (- 18m 7s) (39950 53%) 0.0272\n",
      "20m 41s (- 18m 5s) (40000 53%) 0.0334\n",
      "20m 42s (- 18m 4s) (40050 53%) 0.0302\n",
      "20m 44s (- 18m 2s) (40100 53%) 0.0301\n",
      "20m 45s (- 18m 1s) (40150 53%) 0.0439\n",
      "20m 47s (- 17m 59s) (40200 53%) 0.0403\n",
      "20m 48s (- 17m 58s) (40250 53%) 0.0407\n",
      "20m 50s (- 17m 56s) (40300 53%) 0.0470\n",
      "20m 52s (- 17m 55s) (40350 53%) 0.0536\n",
      "20m 53s (- 17m 53s) (40400 53%) 0.0398\n",
      "20m 55s (- 17m 52s) (40450 53%) 0.0429\n",
      "20m 56s (- 17m 50s) (40500 54%) 0.0678\n",
      "20m 58s (- 17m 49s) (40550 54%) 0.0413\n",
      "21m 0s (- 17m 47s) (40600 54%) 0.0548\n",
      "21m 1s (- 17m 46s) (40650 54%) 0.0297\n",
      "21m 3s (- 17m 44s) (40700 54%) 0.0298\n",
      "21m 4s (- 17m 42s) (40750 54%) 0.0387\n",
      "21m 5s (- 17m 41s) (40800 54%) 0.0497\n",
      "21m 7s (- 17m 39s) (40850 54%) 0.0393\n",
      "21m 8s (- 17m 37s) (40900 54%) 0.0348\n",
      "21m 10s (- 17m 36s) (40950 54%) 0.0373\n",
      "21m 11s (- 17m 34s) (41000 54%) 0.0353\n",
      "21m 13s (- 17m 33s) (41050 54%) 0.0531\n",
      "21m 14s (- 17m 31s) (41100 54%) 0.0467\n",
      "21m 16s (- 17m 29s) (41150 54%) 0.0523\n",
      "21m 17s (- 17m 28s) (41200 54%) 0.0356\n",
      "21m 19s (- 17m 26s) (41250 55%) 0.0725\n",
      "21m 20s (- 17m 25s) (41300 55%) 0.0244\n",
      "21m 22s (- 17m 23s) (41350 55%) 0.0485\n",
      "21m 23s (- 17m 22s) (41400 55%) 0.0457\n",
      "21m 25s (- 17m 20s) (41450 55%) 0.0297\n",
      "21m 27s (- 17m 18s) (41500 55%) 0.0618\n",
      "21m 28s (- 17m 17s) (41550 55%) 0.0493\n",
      "21m 30s (- 17m 15s) (41600 55%) 0.0541\n",
      "21m 31s (- 17m 14s) (41650 55%) 0.0301\n",
      "21m 33s (- 17m 12s) (41700 55%) 0.0334\n",
      "21m 34s (- 17m 11s) (41750 55%) 0.0303\n",
      "21m 36s (- 17m 9s) (41800 55%) 0.0346\n",
      "21m 37s (- 17m 7s) (41850 55%) 0.0441\n",
      "21m 38s (- 17m 6s) (41900 55%) 0.0336\n",
      "21m 40s (- 17m 4s) (41950 55%) 0.0499\n",
      "21m 41s (- 17m 2s) (42000 56%) 0.0463\n",
      "21m 43s (- 17m 1s) (42050 56%) 0.0310\n",
      "21m 44s (- 16m 59s) (42100 56%) 0.0440\n",
      "21m 46s (- 16m 58s) (42150 56%) 0.0349\n",
      "21m 47s (- 16m 56s) (42200 56%) 0.0223\n",
      "21m 49s (- 16m 55s) (42250 56%) 0.0628\n",
      "21m 50s (- 16m 53s) (42300 56%) 0.0343\n",
      "21m 52s (- 16m 51s) (42350 56%) 0.0333\n",
      "21m 53s (- 16m 50s) (42400 56%) 0.0340\n",
      "21m 55s (- 16m 48s) (42450 56%) 0.0354\n",
      "21m 57s (- 16m 47s) (42500 56%) 0.0577\n",
      "21m 58s (- 16m 45s) (42550 56%) 0.0636\n",
      "22m 0s (- 16m 44s) (42600 56%) 0.0452\n",
      "22m 1s (- 16m 42s) (42650 56%) 0.0403\n",
      "22m 3s (- 16m 40s) (42700 56%) 0.0464\n",
      "22m 4s (- 16m 39s) (42750 56%) 0.0347\n",
      "22m 6s (- 16m 37s) (42800 57%) 0.0452\n",
      "22m 7s (- 16m 36s) (42850 57%) 0.0377\n",
      "22m 9s (- 16m 34s) (42900 57%) 0.0251\n",
      "22m 11s (- 16m 33s) (42950 57%) 0.0434\n",
      "22m 12s (- 16m 31s) (43000 57%) 0.0424\n",
      "22m 14s (- 16m 30s) (43050 57%) 0.0694\n",
      "22m 15s (- 16m 28s) (43100 57%) 0.0513\n",
      "22m 17s (- 16m 27s) (43150 57%) 0.0350\n",
      "22m 18s (- 16m 25s) (43200 57%) 0.0418\n",
      "22m 20s (- 16m 24s) (43250 57%) 0.0245\n",
      "22m 22s (- 16m 22s) (43300 57%) 0.0578\n",
      "22m 23s (- 16m 21s) (43350 57%) 0.0305\n",
      "22m 25s (- 16m 19s) (43400 57%) 0.0336\n",
      "22m 26s (- 16m 17s) (43450 57%) 0.0234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22m 28s (- 16m 16s) (43500 57%) 0.0458\n",
      "22m 29s (- 16m 14s) (43550 58%) 0.0492\n",
      "22m 31s (- 16m 13s) (43600 58%) 0.0341\n",
      "22m 33s (- 16m 11s) (43650 58%) 0.0514\n",
      "22m 34s (- 16m 10s) (43700 58%) 0.0334\n",
      "22m 36s (- 16m 8s) (43750 58%) 0.0308\n",
      "22m 37s (- 16m 7s) (43800 58%) 0.0289\n",
      "22m 39s (- 16m 5s) (43850 58%) 0.0311\n",
      "22m 41s (- 16m 4s) (43900 58%) 0.0432\n",
      "22m 42s (- 16m 2s) (43950 58%) 0.0375\n",
      "22m 44s (- 16m 1s) (44000 58%) 0.0531\n",
      "22m 45s (- 15m 59s) (44050 58%) 0.0565\n",
      "22m 47s (- 15m 58s) (44100 58%) 0.0339\n",
      "22m 49s (- 15m 56s) (44150 58%) 0.0437\n",
      "22m 50s (- 15m 55s) (44200 58%) 0.0696\n",
      "22m 52s (- 15m 53s) (44250 59%) 0.0412\n",
      "22m 54s (- 15m 52s) (44300 59%) 0.0464\n",
      "22m 55s (- 15m 50s) (44350 59%) 0.0484\n",
      "22m 57s (- 15m 49s) (44400 59%) 0.0246\n",
      "22m 58s (- 15m 47s) (44450 59%) 0.0352\n",
      "23m 0s (- 15m 46s) (44500 59%) 0.0282\n",
      "23m 2s (- 15m 44s) (44550 59%) 0.0306\n",
      "23m 3s (- 15m 43s) (44600 59%) 0.0196\n",
      "23m 5s (- 15m 41s) (44650 59%) 0.0313\n",
      "23m 6s (- 15m 40s) (44700 59%) 0.0242\n",
      "23m 8s (- 15m 38s) (44750 59%) 0.0362\n",
      "23m 10s (- 15m 37s) (44800 59%) 0.0223\n",
      "23m 11s (- 15m 35s) (44850 59%) 0.0301\n",
      "23m 13s (- 15m 34s) (44900 59%) 0.0325\n",
      "23m 15s (- 15m 32s) (44950 59%) 0.0362\n",
      "23m 16s (- 15m 31s) (45000 60%) 0.0235\n",
      "23m 18s (- 15m 29s) (45050 60%) 0.0366\n",
      "23m 19s (- 15m 28s) (45100 60%) 0.0465\n",
      "23m 21s (- 15m 26s) (45150 60%) 0.0399\n",
      "23m 23s (- 15m 25s) (45200 60%) 0.0221\n",
      "23m 24s (- 15m 23s) (45250 60%) 0.0237\n",
      "23m 26s (- 15m 21s) (45300 60%) 0.0271\n",
      "23m 27s (- 15m 20s) (45350 60%) 0.0162\n",
      "23m 29s (- 15m 18s) (45400 60%) 0.0272\n",
      "23m 30s (- 15m 17s) (45450 60%) 0.0320\n",
      "23m 32s (- 15m 15s) (45500 60%) 0.0275\n",
      "23m 34s (- 15m 14s) (45550 60%) 0.0333\n",
      "23m 36s (- 15m 13s) (45600 60%) 0.0204\n",
      "23m 37s (- 15m 11s) (45650 60%) 0.0510\n",
      "23m 39s (- 15m 9s) (45700 60%) 0.0337\n",
      "23m 40s (- 15m 8s) (45750 61%) 0.0461\n",
      "23m 42s (- 15m 6s) (45800 61%) 0.0309\n",
      "23m 43s (- 15m 5s) (45850 61%) 0.0209\n",
      "23m 45s (- 15m 3s) (45900 61%) 0.0232\n",
      "23m 47s (- 15m 2s) (45950 61%) 0.0235\n",
      "23m 48s (- 15m 0s) (46000 61%) 0.0251\n",
      "23m 50s (- 14m 59s) (46050 61%) 0.0133\n",
      "23m 52s (- 14m 57s) (46100 61%) 0.0237\n",
      "23m 53s (- 14m 56s) (46150 61%) 0.0201\n",
      "23m 55s (- 14m 54s) (46200 61%) 0.0288\n",
      "23m 56s (- 14m 53s) (46250 61%) 0.0142\n",
      "23m 58s (- 14m 51s) (46300 61%) 0.0329\n",
      "24m 0s (- 14m 50s) (46350 61%) 0.0375\n",
      "24m 1s (- 14m 48s) (46400 61%) 0.0288\n",
      "24m 3s (- 14m 47s) (46450 61%) 0.0499\n",
      "24m 4s (- 14m 45s) (46500 62%) 0.0346\n",
      "24m 6s (- 14m 43s) (46550 62%) 0.0373\n",
      "24m 8s (- 14m 42s) (46600 62%) 0.0272\n",
      "24m 9s (- 14m 40s) (46650 62%) 0.0165\n",
      "24m 11s (- 14m 39s) (46700 62%) 0.0230\n",
      "24m 12s (- 14m 37s) (46750 62%) 0.0199\n",
      "24m 14s (- 14m 36s) (46800 62%) 0.0191\n",
      "24m 16s (- 14m 34s) (46850 62%) 0.0438\n",
      "24m 17s (- 14m 33s) (46900 62%) 0.0187\n",
      "24m 19s (- 14m 31s) (46950 62%) 0.0466\n",
      "24m 20s (- 14m 30s) (47000 62%) 0.0394\n",
      "24m 22s (- 14m 28s) (47050 62%) 0.0185\n",
      "24m 23s (- 14m 27s) (47100 62%) 0.0436\n",
      "24m 25s (- 14m 25s) (47150 62%) 0.0272\n",
      "24m 27s (- 14m 24s) (47200 62%) 0.0369\n",
      "24m 28s (- 14m 22s) (47250 63%) 0.0313\n",
      "24m 30s (- 14m 20s) (47300 63%) 0.0335\n",
      "24m 31s (- 14m 19s) (47350 63%) 0.0358\n",
      "24m 33s (- 14m 17s) (47400 63%) 0.0225\n",
      "24m 34s (- 14m 16s) (47450 63%) 0.0190\n",
      "24m 36s (- 14m 14s) (47500 63%) 0.0348\n",
      "24m 38s (- 14m 13s) (47550 63%) 0.0258\n",
      "24m 39s (- 14m 11s) (47600 63%) 0.0179\n",
      "24m 41s (- 14m 10s) (47650 63%) 0.0323\n",
      "24m 42s (- 14m 8s) (47700 63%) 0.0428\n",
      "24m 44s (- 14m 7s) (47750 63%) 0.0339\n",
      "24m 46s (- 14m 5s) (47800 63%) 0.0258\n",
      "24m 47s (- 14m 4s) (47850 63%) 0.0175\n",
      "24m 49s (- 14m 2s) (47900 63%) 0.0185\n",
      "24m 51s (- 14m 1s) (47950 63%) 0.0319\n",
      "24m 53s (- 13m 59s) (48000 64%) 0.0348\n",
      "24m 54s (- 13m 58s) (48050 64%) 0.0341\n",
      "24m 56s (- 13m 56s) (48100 64%) 0.0268\n",
      "24m 58s (- 13m 55s) (48150 64%) 0.0298\n",
      "24m 59s (- 13m 53s) (48200 64%) 0.0320\n",
      "25m 1s (- 13m 52s) (48250 64%) 0.0226\n",
      "25m 3s (- 13m 50s) (48300 64%) 0.0291\n",
      "25m 4s (- 13m 49s) (48350 64%) 0.0224\n",
      "25m 6s (- 13m 47s) (48400 64%) 0.0319\n",
      "25m 8s (- 13m 46s) (48450 64%) 0.0275\n",
      "25m 10s (- 13m 45s) (48500 64%) 0.0259\n",
      "25m 11s (- 13m 43s) (48550 64%) 0.0400\n",
      "25m 13s (- 13m 42s) (48600 64%) 0.0296\n",
      "25m 15s (- 13m 40s) (48650 64%) 0.0223\n",
      "25m 16s (- 13m 39s) (48700 64%) 0.0314\n",
      "25m 18s (- 13m 37s) (48750 65%) 0.0185\n",
      "25m 20s (- 13m 36s) (48800 65%) 0.0318\n",
      "25m 21s (- 13m 34s) (48850 65%) 0.0333\n",
      "25m 23s (- 13m 33s) (48900 65%) 0.0294\n",
      "25m 25s (- 13m 31s) (48950 65%) 0.0298\n",
      "25m 26s (- 13m 30s) (49000 65%) 0.0225\n",
      "25m 28s (- 13m 28s) (49050 65%) 0.0300\n",
      "25m 30s (- 13m 27s) (49100 65%) 0.0308\n",
      "25m 31s (- 13m 25s) (49150 65%) 0.0223\n",
      "25m 33s (- 13m 24s) (49200 65%) 0.0249\n",
      "25m 35s (- 13m 22s) (49250 65%) 0.0146\n",
      "25m 36s (- 13m 21s) (49300 65%) 0.0286\n",
      "25m 38s (- 13m 19s) (49350 65%) 0.0266\n",
      "25m 40s (- 13m 18s) (49400 65%) 0.0263\n",
      "25m 41s (- 13m 16s) (49450 65%) 0.0210\n",
      "25m 43s (- 13m 14s) (49500 66%) 0.0269\n",
      "25m 44s (- 13m 13s) (49550 66%) 0.0172\n",
      "25m 46s (- 13m 11s) (49600 66%) 0.0205\n",
      "25m 47s (- 13m 10s) (49650 66%) 0.0266\n",
      "25m 49s (- 13m 8s) (49700 66%) 0.0279\n",
      "25m 51s (- 13m 7s) (49750 66%) 0.0399\n",
      "25m 52s (- 13m 5s) (49800 66%) 0.0159\n",
      "25m 54s (- 13m 4s) (49850 66%) 0.0253\n",
      "25m 56s (- 13m 2s) (49900 66%) 0.0353\n",
      "25m 57s (- 13m 1s) (49950 66%) 0.0197\n",
      "25m 59s (- 12m 59s) (50000 66%) 0.0246\n",
      "26m 1s (- 12m 58s) (50050 66%) 0.0353\n",
      "26m 2s (- 12m 56s) (50100 66%) 0.0164\n",
      "26m 4s (- 12m 55s) (50150 66%) 0.0336\n",
      "26m 6s (- 12m 53s) (50200 66%) 0.0242\n",
      "26m 7s (- 12m 52s) (50250 67%) 0.0264\n",
      "26m 9s (- 12m 50s) (50300 67%) 0.0192\n",
      "26m 11s (- 12m 49s) (50350 67%) 0.0222\n",
      "26m 13s (- 12m 47s) (50400 67%) 0.0254\n",
      "26m 14s (- 12m 46s) (50450 67%) 0.0296\n",
      "26m 16s (- 12m 44s) (50500 67%) 0.0197\n",
      "26m 18s (- 12m 43s) (50550 67%) 0.0191\n",
      "26m 19s (- 12m 41s) (50600 67%) 0.0230\n",
      "26m 21s (- 12m 40s) (50650 67%) 0.0185\n",
      "26m 22s (- 12m 38s) (50700 67%) 0.0292\n",
      "26m 24s (- 12m 37s) (50750 67%) 0.0160\n",
      "26m 26s (- 12m 35s) (50800 67%) 0.0158\n",
      "26m 27s (- 12m 34s) (50850 67%) 0.0211\n",
      "26m 29s (- 12m 32s) (50900 67%) 0.0207\n",
      "26m 31s (- 12m 31s) (50950 67%) 0.0548\n",
      "26m 32s (- 12m 29s) (51000 68%) 0.0298\n",
      "26m 34s (- 12m 27s) (51050 68%) 0.0273\n",
      "26m 35s (- 12m 26s) (51100 68%) 0.0273\n",
      "26m 37s (- 12m 24s) (51150 68%) 0.0195\n",
      "26m 38s (- 12m 23s) (51200 68%) 0.0153\n",
      "26m 40s (- 12m 21s) (51250 68%) 0.0258\n",
      "26m 42s (- 12m 20s) (51300 68%) 0.0285\n",
      "26m 43s (- 12m 18s) (51350 68%) 0.0155\n",
      "26m 45s (- 12m 17s) (51400 68%) 0.0268\n",
      "26m 46s (- 12m 15s) (51450 68%) 0.0347\n",
      "26m 48s (- 12m 13s) (51500 68%) 0.0271\n",
      "26m 50s (- 12m 12s) (51550 68%) 0.0249\n",
      "26m 51s (- 12m 10s) (51600 68%) 0.0418\n",
      "26m 53s (- 12m 9s) (51650 68%) 0.0339\n",
      "26m 54s (- 12m 7s) (51700 68%) 0.0258\n",
      "26m 56s (- 12m 6s) (51750 69%) 0.0210\n",
      "26m 58s (- 12m 4s) (51800 69%) 0.0363\n",
      "26m 59s (- 12m 3s) (51850 69%) 0.0329\n",
      "27m 1s (- 12m 1s) (51900 69%) 0.0286\n",
      "27m 2s (- 12m 0s) (51950 69%) 0.0397\n",
      "27m 4s (- 11m 58s) (52000 69%) 0.0228\n",
      "27m 6s (- 11m 56s) (52050 69%) 0.0324\n",
      "27m 7s (- 11m 55s) (52100 69%) 0.0366\n",
      "27m 9s (- 11m 53s) (52150 69%) 0.0264\n",
      "27m 10s (- 11m 52s) (52200 69%) 0.0171\n",
      "27m 12s (- 11m 50s) (52250 69%) 0.0304\n",
      "27m 14s (- 11m 49s) (52300 69%) 0.0409\n",
      "27m 15s (- 11m 47s) (52350 69%) 0.0305\n",
      "27m 17s (- 11m 46s) (52400 69%) 0.0317\n",
      "27m 18s (- 11m 44s) (52450 69%) 0.0244\n",
      "27m 20s (- 11m 43s) (52500 70%) 0.0414\n",
      "27m 22s (- 11m 41s) (52550 70%) 0.0329\n",
      "27m 23s (- 11m 39s) (52600 70%) 0.0296\n",
      "27m 25s (- 11m 38s) (52650 70%) 0.0248\n",
      "27m 26s (- 11m 36s) (52700 70%) 0.0352\n",
      "27m 28s (- 11m 35s) (52750 70%) 0.0257\n",
      "27m 30s (- 11m 33s) (52800 70%) 0.0316\n",
      "27m 31s (- 11m 32s) (52850 70%) 0.0184\n",
      "27m 33s (- 11m 30s) (52900 70%) 0.0279\n",
      "27m 34s (- 11m 29s) (52950 70%) 0.0193\n",
      "27m 36s (- 11m 27s) (53000 70%) 0.0273\n",
      "27m 38s (- 11m 26s) (53050 70%) 0.0130\n",
      "27m 39s (- 11m 24s) (53100 70%) 0.0136\n",
      "27m 41s (- 11m 22s) (53150 70%) 0.0321\n",
      "27m 42s (- 11m 21s) (53200 70%) 0.0262\n",
      "27m 44s (- 11m 19s) (53250 71%) 0.0306\n",
      "27m 45s (- 11m 18s) (53300 71%) 0.0368\n",
      "27m 47s (- 11m 16s) (53350 71%) 0.0377\n",
      "27m 48s (- 11m 14s) (53400 71%) 0.0325\n",
      "27m 50s (- 11m 13s) (53450 71%) 0.0733\n",
      "27m 51s (- 11m 11s) (53500 71%) 0.0258\n",
      "27m 53s (- 11m 10s) (53550 71%) 0.0371\n",
      "27m 55s (- 11m 8s) (53600 71%) 0.0567\n",
      "27m 56s (- 11m 7s) (53650 71%) 0.0274\n",
      "27m 58s (- 11m 5s) (53700 71%) 0.0251\n",
      "27m 59s (- 11m 4s) (53750 71%) 0.0239\n",
      "28m 1s (- 11m 2s) (53800 71%) 0.0232\n",
      "28m 2s (- 11m 0s) (53850 71%) 0.0320\n",
      "28m 4s (- 10m 59s) (53900 71%) 0.0199\n",
      "28m 5s (- 10m 57s) (53950 71%) 0.0496\n",
      "28m 7s (- 10m 56s) (54000 72%) 0.0301\n",
      "28m 8s (- 10m 54s) (54050 72%) 0.0591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28m 10s (- 10m 53s) (54100 72%) 0.0233\n",
      "28m 12s (- 10m 51s) (54150 72%) 0.0233\n",
      "28m 13s (- 10m 50s) (54200 72%) 0.0262\n",
      "28m 15s (- 10m 48s) (54250 72%) 0.0191\n",
      "28m 16s (- 10m 46s) (54300 72%) 0.0234\n",
      "28m 18s (- 10m 45s) (54350 72%) 0.0259\n",
      "28m 20s (- 10m 43s) (54400 72%) 0.0306\n",
      "28m 21s (- 10m 42s) (54450 72%) 0.0204\n",
      "28m 23s (- 10m 40s) (54500 72%) 0.0327\n",
      "28m 24s (- 10m 39s) (54550 72%) 0.0252\n",
      "28m 26s (- 10m 37s) (54600 72%) 0.0433\n",
      "28m 28s (- 10m 36s) (54650 72%) 0.0286\n",
      "28m 29s (- 10m 34s) (54700 72%) 0.0112\n",
      "28m 31s (- 10m 32s) (54750 73%) 0.0118\n",
      "28m 32s (- 10m 31s) (54800 73%) 0.0149\n",
      "28m 34s (- 10m 29s) (54850 73%) 0.0204\n",
      "28m 36s (- 10m 28s) (54900 73%) 0.0196\n",
      "28m 37s (- 10m 26s) (54950 73%) 0.0223\n",
      "28m 39s (- 10m 25s) (55000 73%) 0.0186\n",
      "28m 41s (- 10m 23s) (55050 73%) 0.0256\n",
      "28m 42s (- 10m 22s) (55100 73%) 0.0180\n",
      "28m 44s (- 10m 20s) (55150 73%) 0.0165\n",
      "28m 45s (- 10m 19s) (55200 73%) 0.0208\n",
      "28m 47s (- 10m 17s) (55250 73%) 0.0147\n",
      "28m 49s (- 10m 15s) (55300 73%) 0.0194\n",
      "28m 50s (- 10m 14s) (55350 73%) 0.0217\n",
      "28m 52s (- 10m 12s) (55400 73%) 0.0243\n",
      "28m 53s (- 10m 11s) (55450 73%) 0.0219\n",
      "28m 55s (- 10m 9s) (55500 74%) 0.0283\n",
      "28m 56s (- 10m 8s) (55550 74%) 0.0166\n",
      "28m 58s (- 10m 6s) (55600 74%) 0.0178\n",
      "29m 0s (- 10m 5s) (55650 74%) 0.0168\n",
      "29m 1s (- 10m 3s) (55700 74%) 0.0199\n",
      "29m 3s (- 10m 1s) (55750 74%) 0.0211\n",
      "29m 5s (- 10m 0s) (55800 74%) 0.0260\n",
      "29m 6s (- 9m 58s) (55850 74%) 0.0255\n",
      "29m 8s (- 9m 57s) (55900 74%) 0.0213\n",
      "29m 9s (- 9m 55s) (55950 74%) 0.0186\n",
      "29m 11s (- 9m 54s) (56000 74%) 0.0223\n",
      "29m 13s (- 9m 52s) (56050 74%) 0.0253\n",
      "29m 14s (- 9m 51s) (56100 74%) 0.0321\n",
      "29m 16s (- 9m 49s) (56150 74%) 0.0190\n",
      "29m 17s (- 9m 48s) (56200 74%) 0.0327\n",
      "29m 19s (- 9m 46s) (56250 75%) 0.0202\n",
      "29m 21s (- 9m 44s) (56300 75%) 0.0288\n",
      "29m 22s (- 9m 43s) (56350 75%) 0.0289\n",
      "29m 24s (- 9m 41s) (56400 75%) 0.1293\n",
      "29m 25s (- 9m 40s) (56450 75%) 0.0149\n",
      "29m 27s (- 9m 38s) (56500 75%) 0.0250\n",
      "29m 29s (- 9m 37s) (56550 75%) 0.0281\n",
      "29m 30s (- 9m 35s) (56600 75%) 0.0409\n",
      "29m 32s (- 9m 34s) (56650 75%) 0.0214\n",
      "29m 33s (- 9m 32s) (56700 75%) 0.0150\n",
      "29m 35s (- 9m 30s) (56750 75%) 0.0110\n",
      "29m 37s (- 9m 29s) (56800 75%) 0.0176\n",
      "29m 38s (- 9m 27s) (56850 75%) 0.0165\n",
      "29m 40s (- 9m 26s) (56900 75%) 0.0333\n",
      "29m 41s (- 9m 24s) (56950 75%) 0.0239\n",
      "29m 43s (- 9m 23s) (57000 76%) 0.0167\n",
      "29m 45s (- 9m 21s) (57050 76%) 0.0140\n",
      "29m 46s (- 9m 20s) (57100 76%) 0.0297\n",
      "29m 48s (- 9m 18s) (57150 76%) 0.0077\n",
      "29m 49s (- 9m 16s) (57200 76%) 0.0301\n",
      "29m 51s (- 9m 15s) (57250 76%) 0.0138\n",
      "29m 52s (- 9m 13s) (57300 76%) 0.0169\n",
      "29m 54s (- 9m 12s) (57350 76%) 0.0118\n",
      "29m 56s (- 9m 10s) (57400 76%) 0.0261\n",
      "29m 57s (- 9m 9s) (57450 76%) 0.0183\n",
      "29m 59s (- 9m 7s) (57500 76%) 0.0115\n",
      "30m 1s (- 9m 6s) (57550 76%) 0.0170\n",
      "30m 2s (- 9m 4s) (57600 76%) 0.0174\n",
      "30m 4s (- 9m 2s) (57650 76%) 0.0144\n",
      "30m 5s (- 9m 1s) (57700 76%) 0.0057\n",
      "30m 7s (- 8m 59s) (57750 77%) 0.0211\n",
      "30m 8s (- 8m 58s) (57800 77%) 0.0212\n",
      "30m 10s (- 8m 56s) (57850 77%) 0.0215\n",
      "30m 12s (- 8m 55s) (57900 77%) 0.0138\n",
      "30m 13s (- 8m 53s) (57950 77%) 0.0079\n",
      "30m 15s (- 8m 52s) (58000 77%) 0.0219\n",
      "30m 16s (- 8m 50s) (58050 77%) 0.0189\n",
      "30m 18s (- 8m 48s) (58100 77%) 0.0177\n",
      "30m 19s (- 8m 47s) (58150 77%) 0.0247\n",
      "30m 21s (- 8m 45s) (58200 77%) 0.0134\n",
      "30m 23s (- 8m 44s) (58250 77%) 0.0133\n",
      "30m 24s (- 8m 42s) (58300 77%) 0.0306\n",
      "30m 26s (- 8m 41s) (58350 77%) 0.0166\n",
      "30m 28s (- 8m 39s) (58400 77%) 0.0263\n",
      "30m 29s (- 8m 38s) (58450 77%) 0.0262\n",
      "30m 31s (- 8m 36s) (58500 78%) 0.0116\n",
      "30m 32s (- 8m 34s) (58550 78%) 0.0201\n",
      "30m 34s (- 8m 33s) (58600 78%) 0.0103\n",
      "30m 36s (- 8m 31s) (58650 78%) 0.0195\n",
      "30m 37s (- 8m 30s) (58700 78%) 0.0195\n",
      "30m 39s (- 8m 28s) (58750 78%) 0.0313\n",
      "30m 40s (- 8m 27s) (58800 78%) 0.0099\n",
      "30m 42s (- 8m 25s) (58850 78%) 0.0120\n",
      "30m 44s (- 8m 24s) (58900 78%) 0.0155\n",
      "30m 45s (- 8m 22s) (58950 78%) 0.0248\n",
      "30m 47s (- 8m 20s) (59000 78%) 0.0194\n",
      "30m 48s (- 8m 19s) (59050 78%) 0.0803\n",
      "30m 50s (- 8m 17s) (59100 78%) 0.0343\n",
      "30m 51s (- 8m 16s) (59150 78%) 0.0197\n",
      "30m 53s (- 8m 14s) (59200 78%) 0.0711\n",
      "30m 55s (- 8m 13s) (59250 79%) 0.0117\n",
      "30m 56s (- 8m 11s) (59300 79%) 0.0209\n",
      "30m 58s (- 8m 10s) (59350 79%) 0.0150\n",
      "30m 59s (- 8m 8s) (59400 79%) 0.0283\n",
      "31m 1s (- 8m 6s) (59450 79%) 0.0127\n",
      "31m 3s (- 8m 5s) (59500 79%) 0.0124\n",
      "31m 4s (- 8m 3s) (59550 79%) 0.0150\n",
      "31m 6s (- 8m 2s) (59600 79%) 0.0075\n",
      "31m 7s (- 8m 0s) (59650 79%) 0.0160\n",
      "31m 9s (- 7m 59s) (59700 79%) 0.0129\n",
      "31m 11s (- 7m 57s) (59750 79%) 0.0556\n",
      "31m 12s (- 7m 55s) (59800 79%) 0.0174\n",
      "31m 14s (- 7m 54s) (59850 79%) 0.0218\n",
      "31m 15s (- 7m 52s) (59900 79%) 0.0182\n",
      "31m 17s (- 7m 51s) (59950 79%) 0.0214\n",
      "31m 19s (- 7m 49s) (60000 80%) 0.0189\n",
      "31m 20s (- 7m 48s) (60050 80%) 0.0215\n",
      "31m 22s (- 7m 46s) (60100 80%) 0.0147\n",
      "31m 23s (- 7m 45s) (60150 80%) 0.0159\n",
      "31m 25s (- 7m 43s) (60200 80%) 0.0201\n",
      "31m 26s (- 7m 41s) (60250 80%) 0.0193\n",
      "31m 28s (- 7m 40s) (60300 80%) 0.0232\n",
      "31m 30s (- 7m 38s) (60350 80%) 0.0182\n",
      "31m 31s (- 7m 37s) (60400 80%) 0.0139\n",
      "31m 33s (- 7m 35s) (60450 80%) 0.0229\n",
      "31m 34s (- 7m 34s) (60500 80%) 0.0173\n",
      "31m 36s (- 7m 32s) (60550 80%) 0.0096\n",
      "31m 37s (- 7m 31s) (60600 80%) 0.0100\n",
      "31m 39s (- 7m 29s) (60650 80%) 0.0140\n",
      "31m 41s (- 7m 27s) (60700 80%) 0.0161\n",
      "31m 42s (- 7m 26s) (60750 81%) 0.0087\n",
      "31m 44s (- 7m 24s) (60800 81%) 0.0238\n",
      "31m 45s (- 7m 23s) (60850 81%) 0.0176\n",
      "31m 47s (- 7m 21s) (60900 81%) 0.0070\n",
      "31m 49s (- 7m 20s) (60950 81%) 0.0121\n",
      "31m 50s (- 7m 18s) (61000 81%) 0.0147\n",
      "31m 52s (- 7m 16s) (61050 81%) 0.0099\n",
      "31m 54s (- 7m 15s) (61100 81%) 0.0167\n",
      "31m 55s (- 7m 13s) (61150 81%) 0.0210\n",
      "31m 57s (- 7m 12s) (61200 81%) 0.0210\n",
      "31m 58s (- 7m 10s) (61250 81%) 0.0077\n",
      "32m 0s (- 7m 9s) (61300 81%) 0.0155\n",
      "32m 1s (- 7m 7s) (61350 81%) 0.0183\n",
      "32m 3s (- 7m 6s) (61400 81%) 0.0200\n",
      "32m 5s (- 7m 4s) (61450 81%) 0.0211\n",
      "32m 6s (- 7m 2s) (61500 82%) 0.0167\n",
      "32m 8s (- 7m 1s) (61550 82%) 0.0152\n",
      "32m 9s (- 6m 59s) (61600 82%) 0.0138\n",
      "32m 11s (- 6m 58s) (61650 82%) 0.0140\n",
      "32m 13s (- 6m 56s) (61700 82%) 0.0131\n",
      "32m 14s (- 6m 55s) (61750 82%) 0.0143\n",
      "32m 16s (- 6m 53s) (61800 82%) 0.0075\n",
      "32m 17s (- 6m 52s) (61850 82%) 0.0076\n",
      "32m 19s (- 6m 50s) (61900 82%) 0.0205\n",
      "32m 21s (- 6m 48s) (61950 82%) 0.0146\n",
      "32m 22s (- 6m 47s) (62000 82%) 0.0126\n",
      "32m 24s (- 6m 45s) (62050 82%) 0.0171\n",
      "32m 25s (- 6m 44s) (62100 82%) 0.0144\n",
      "32m 27s (- 6m 42s) (62150 82%) 0.0097\n",
      "32m 28s (- 6m 41s) (62200 82%) 0.0088\n",
      "32m 30s (- 6m 39s) (62250 83%) 0.0121\n",
      "32m 32s (- 6m 37s) (62300 83%) 0.0174\n",
      "32m 33s (- 6m 36s) (62350 83%) 0.0140\n",
      "32m 35s (- 6m 34s) (62400 83%) 0.0128\n",
      "32m 37s (- 6m 33s) (62450 83%) 0.0148\n",
      "32m 38s (- 6m 31s) (62500 83%) 0.0410\n",
      "32m 40s (- 6m 30s) (62550 83%) 0.0118\n",
      "32m 41s (- 6m 28s) (62600 83%) 0.0117\n",
      "32m 43s (- 6m 27s) (62650 83%) 0.0156\n",
      "32m 45s (- 6m 25s) (62700 83%) 0.0114\n",
      "32m 46s (- 6m 23s) (62750 83%) 0.0111\n",
      "32m 48s (- 6m 22s) (62800 83%) 0.0121\n",
      "32m 50s (- 6m 20s) (62850 83%) 0.0160\n",
      "32m 51s (- 6m 19s) (62900 83%) 0.0140\n",
      "32m 53s (- 6m 17s) (62950 83%) 0.0224\n",
      "32m 54s (- 6m 16s) (63000 84%) 0.0103\n",
      "32m 56s (- 6m 14s) (63050 84%) 0.0129\n",
      "32m 57s (- 6m 13s) (63100 84%) 0.0095\n",
      "32m 59s (- 6m 11s) (63150 84%) 0.0042\n",
      "33m 1s (- 6m 9s) (63200 84%) 0.0138\n",
      "33m 2s (- 6m 8s) (63250 84%) 0.0123\n",
      "33m 4s (- 6m 6s) (63300 84%) 0.0101\n",
      "33m 5s (- 6m 5s) (63350 84%) 0.0099\n",
      "33m 7s (- 6m 3s) (63400 84%) 0.0155\n",
      "33m 9s (- 6m 2s) (63450 84%) 0.0094\n",
      "33m 10s (- 6m 0s) (63500 84%) 0.0107\n",
      "33m 12s (- 5m 58s) (63550 84%) 0.0086\n",
      "33m 13s (- 5m 57s) (63600 84%) 0.0100\n",
      "33m 15s (- 5m 55s) (63650 84%) 0.0298\n",
      "33m 16s (- 5m 54s) (63700 84%) 0.0256\n",
      "33m 18s (- 5m 52s) (63750 85%) 0.0094\n",
      "33m 20s (- 5m 51s) (63800 85%) 0.0090\n",
      "33m 21s (- 5m 49s) (63850 85%) 0.0173\n",
      "33m 23s (- 5m 47s) (63900 85%) 0.0066\n",
      "33m 24s (- 5m 46s) (63950 85%) 0.0143\n",
      "33m 26s (- 5m 44s) (64000 85%) 0.0089\n",
      "33m 27s (- 5m 43s) (64050 85%) 0.0107\n",
      "33m 29s (- 5m 41s) (64100 85%) 0.0408\n",
      "33m 31s (- 5m 40s) (64150 85%) 0.0243\n",
      "33m 32s (- 5m 38s) (64200 85%) 0.0176\n",
      "33m 34s (- 5m 37s) (64250 85%) 0.0201\n",
      "33m 36s (- 5m 35s) (64300 85%) 0.0170\n",
      "33m 37s (- 5m 33s) (64350 85%) 0.0178\n",
      "33m 39s (- 5m 32s) (64400 85%) 0.0150\n",
      "33m 41s (- 5m 30s) (64450 85%) 0.0121\n",
      "33m 42s (- 5m 29s) (64500 86%) 0.0111\n",
      "33m 44s (- 5m 27s) (64550 86%) 0.0203\n",
      "33m 45s (- 5m 26s) (64600 86%) 0.0329\n",
      "33m 47s (- 5m 24s) (64650 86%) 0.0124\n",
      "33m 49s (- 5m 23s) (64700 86%) 0.0200\n",
      "33m 50s (- 5m 21s) (64750 86%) 0.0117\n",
      "33m 52s (- 5m 19s) (64800 86%) 0.0100\n",
      "33m 53s (- 5m 18s) (64850 86%) 0.0198\n",
      "33m 55s (- 5m 16s) (64900 86%) 0.0182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33m 57s (- 5m 15s) (64950 86%) 0.0127\n",
      "33m 58s (- 5m 13s) (65000 86%) 0.0227\n",
      "34m 0s (- 5m 12s) (65050 86%) 0.0162\n",
      "34m 2s (- 5m 10s) (65100 86%) 0.0138\n",
      "34m 3s (- 5m 8s) (65150 86%) 0.0138\n",
      "34m 5s (- 5m 7s) (65200 86%) 0.0110\n",
      "34m 6s (- 5m 5s) (65250 87%) 0.0086\n",
      "34m 8s (- 5m 4s) (65300 87%) 0.0151\n",
      "34m 10s (- 5m 2s) (65350 87%) 0.0151\n",
      "34m 11s (- 5m 1s) (65400 87%) 0.0094\n",
      "34m 13s (- 4m 59s) (65450 87%) 0.0115\n",
      "34m 14s (- 4m 58s) (65500 87%) 0.0137\n",
      "34m 16s (- 4m 56s) (65550 87%) 0.0120\n",
      "34m 17s (- 4m 54s) (65600 87%) 0.0149\n",
      "34m 19s (- 4m 53s) (65650 87%) 0.0196\n",
      "34m 21s (- 4m 51s) (65700 87%) 0.0110\n",
      "34m 22s (- 4m 50s) (65750 87%) 0.0150\n",
      "34m 24s (- 4m 48s) (65800 87%) 0.0166\n",
      "34m 25s (- 4m 47s) (65850 87%) 0.0136\n",
      "34m 27s (- 4m 45s) (65900 87%) 0.0265\n",
      "34m 29s (- 4m 43s) (65950 87%) 0.0153\n",
      "34m 30s (- 4m 42s) (66000 88%) 0.0189\n",
      "34m 32s (- 4m 40s) (66050 88%) 0.0324\n",
      "34m 34s (- 4m 39s) (66100 88%) 0.0181\n",
      "34m 35s (- 4m 37s) (66150 88%) 0.0114\n",
      "34m 37s (- 4m 36s) (66200 88%) 0.0168\n",
      "34m 39s (- 4m 34s) (66250 88%) 0.0216\n",
      "34m 40s (- 4m 33s) (66300 88%) 0.0184\n",
      "34m 42s (- 4m 31s) (66350 88%) 0.0103\n",
      "34m 43s (- 4m 29s) (66400 88%) 0.0117\n",
      "34m 45s (- 4m 28s) (66450 88%) 0.0199\n",
      "34m 47s (- 4m 26s) (66500 88%) 0.0165\n",
      "34m 48s (- 4m 25s) (66550 88%) 0.0139\n",
      "34m 50s (- 4m 23s) (66600 88%) 0.0173\n",
      "34m 51s (- 4m 22s) (66650 88%) 0.0137\n",
      "34m 53s (- 4m 20s) (66700 88%) 0.0176\n",
      "34m 54s (- 4m 18s) (66750 89%) 0.0156\n",
      "34m 56s (- 4m 17s) (66800 89%) 0.0219\n",
      "34m 58s (- 4m 15s) (66850 89%) 0.0224\n",
      "34m 59s (- 4m 14s) (66900 89%) 0.0181\n",
      "35m 1s (- 4m 12s) (66950 89%) 0.0149\n",
      "35m 2s (- 4m 11s) (67000 89%) 0.0164\n",
      "35m 4s (- 4m 9s) (67050 89%) 0.0147\n",
      "35m 5s (- 4m 7s) (67100 89%) 0.0106\n",
      "35m 7s (- 4m 6s) (67150 89%) 0.0107\n",
      "35m 9s (- 4m 4s) (67200 89%) 0.0125\n",
      "35m 10s (- 4m 3s) (67250 89%) 0.0082\n",
      "35m 12s (- 4m 1s) (67300 89%) 0.0183\n",
      "35m 13s (- 4m 0s) (67350 89%) 0.0134\n",
      "35m 15s (- 3m 58s) (67400 89%) 0.0161\n",
      "35m 17s (- 3m 56s) (67450 89%) 0.0146\n",
      "35m 18s (- 3m 55s) (67500 90%) 0.0078\n",
      "35m 20s (- 3m 53s) (67550 90%) 0.0171\n",
      "35m 22s (- 3m 52s) (67600 90%) 0.0135\n",
      "35m 23s (- 3m 50s) (67650 90%) 0.0096\n",
      "35m 25s (- 3m 49s) (67700 90%) 0.0121\n",
      "35m 26s (- 3m 47s) (67750 90%) 0.0107\n",
      "35m 28s (- 3m 46s) (67800 90%) 0.0072\n",
      "35m 30s (- 3m 44s) (67850 90%) 0.0089\n",
      "35m 31s (- 3m 42s) (67900 90%) 0.0065\n",
      "35m 33s (- 3m 41s) (67950 90%) 0.0076\n",
      "35m 34s (- 3m 39s) (68000 90%) 0.0193\n",
      "35m 36s (- 3m 38s) (68050 90%) 0.0076\n",
      "35m 38s (- 3m 36s) (68100 90%) 0.0146\n",
      "35m 39s (- 3m 35s) (68150 90%) 0.0183\n",
      "35m 41s (- 3m 33s) (68200 90%) 0.0087\n",
      "35m 42s (- 3m 31s) (68250 91%) 0.0154\n",
      "35m 44s (- 3m 30s) (68300 91%) 0.0093\n",
      "35m 45s (- 3m 28s) (68350 91%) 0.0121\n",
      "35m 47s (- 3m 27s) (68400 91%) 0.0127\n",
      "35m 49s (- 3m 25s) (68450 91%) 0.0152\n",
      "35m 50s (- 3m 24s) (68500 91%) 0.0131\n",
      "35m 52s (- 3m 22s) (68550 91%) 0.0118\n",
      "35m 54s (- 3m 20s) (68600 91%) 0.0077\n",
      "35m 56s (- 3m 19s) (68650 91%) 0.0122\n",
      "35m 57s (- 3m 17s) (68700 91%) 0.0204\n",
      "35m 59s (- 3m 16s) (68750 91%) 0.0070\n",
      "36m 0s (- 3m 14s) (68800 91%) 0.0051\n",
      "36m 2s (- 3m 13s) (68850 91%) 0.0078\n",
      "36m 4s (- 3m 11s) (68900 91%) 0.0163\n",
      "36m 5s (- 3m 10s) (68950 91%) 0.0263\n",
      "36m 7s (- 3m 8s) (69000 92%) 0.0172\n",
      "36m 8s (- 3m 6s) (69050 92%) 0.0248\n",
      "36m 10s (- 3m 5s) (69100 92%) 0.0142\n",
      "36m 12s (- 3m 3s) (69150 92%) 0.0078\n",
      "36m 13s (- 3m 2s) (69200 92%) 0.0106\n",
      "36m 15s (- 3m 0s) (69250 92%) 0.0107\n",
      "36m 17s (- 2m 59s) (69300 92%) 0.0179\n",
      "36m 18s (- 2m 57s) (69350 92%) 0.0236\n",
      "36m 20s (- 2m 55s) (69400 92%) 0.0120\n",
      "36m 21s (- 2m 54s) (69450 92%) 0.0123\n",
      "36m 23s (- 2m 52s) (69500 92%) 0.0165\n",
      "36m 25s (- 2m 51s) (69550 92%) 0.0095\n",
      "36m 26s (- 2m 49s) (69600 92%) 0.0090\n",
      "36m 28s (- 2m 48s) (69650 92%) 0.0073\n",
      "36m 29s (- 2m 46s) (69700 92%) 0.0189\n",
      "36m 31s (- 2m 44s) (69750 93%) 0.0175\n",
      "36m 33s (- 2m 43s) (69800 93%) 0.0080\n",
      "36m 34s (- 2m 41s) (69850 93%) 0.0107\n",
      "36m 36s (- 2m 40s) (69900 93%) 0.0082\n",
      "36m 37s (- 2m 38s) (69950 93%) 0.0153\n",
      "36m 39s (- 2m 37s) (70000 93%) 0.0112\n",
      "36m 40s (- 2m 35s) (70050 93%) 0.0089\n",
      "36m 42s (- 2m 33s) (70100 93%) 0.0153\n",
      "36m 44s (- 2m 32s) (70150 93%) 0.0160\n",
      "36m 45s (- 2m 30s) (70200 93%) 0.0077\n",
      "36m 47s (- 2m 29s) (70250 93%) 0.0119\n",
      "36m 48s (- 2m 27s) (70300 93%) 0.0139\n",
      "36m 50s (- 2m 26s) (70350 93%) 0.0125\n",
      "36m 52s (- 2m 24s) (70400 93%) 0.0132\n",
      "36m 53s (- 2m 22s) (70450 93%) 0.0114\n",
      "36m 55s (- 2m 21s) (70500 94%) 0.0113\n",
      "36m 56s (- 2m 19s) (70550 94%) 0.0102\n",
      "36m 58s (- 2m 18s) (70600 94%) 0.0083\n",
      "37m 0s (- 2m 16s) (70650 94%) 0.0103\n",
      "37m 1s (- 2m 15s) (70700 94%) 0.0064\n",
      "37m 3s (- 2m 13s) (70750 94%) 0.0085\n",
      "37m 5s (- 2m 12s) (70800 94%) 0.0232\n",
      "37m 6s (- 2m 10s) (70850 94%) 0.0160\n",
      "37m 8s (- 2m 8s) (70900 94%) 0.0102\n",
      "37m 10s (- 2m 7s) (70950 94%) 0.0116\n",
      "37m 11s (- 2m 5s) (71000 94%) 0.0084\n",
      "37m 13s (- 2m 4s) (71050 94%) 0.0102\n",
      "37m 14s (- 2m 2s) (71100 94%) 0.0135\n",
      "37m 16s (- 2m 1s) (71150 94%) 0.0079\n",
      "37m 17s (- 1m 59s) (71200 94%) 0.0127\n",
      "37m 19s (- 1m 57s) (71250 95%) 0.0128\n",
      "37m 21s (- 1m 56s) (71300 95%) 0.0141\n",
      "37m 22s (- 1m 54s) (71350 95%) 0.0152\n",
      "37m 24s (- 1m 53s) (71400 95%) 0.0104\n",
      "37m 26s (- 1m 51s) (71450 95%) 0.0123\n",
      "37m 27s (- 1m 50s) (71500 95%) 0.0097\n",
      "37m 29s (- 1m 48s) (71550 95%) 0.0093\n",
      "37m 31s (- 1m 46s) (71600 95%) 0.0101\n",
      "37m 32s (- 1m 45s) (71650 95%) 0.0122\n",
      "37m 34s (- 1m 43s) (71700 95%) 0.0114\n",
      "37m 35s (- 1m 42s) (71750 95%) 0.0067\n",
      "37m 37s (- 1m 40s) (71800 95%) 0.0155\n",
      "37m 39s (- 1m 39s) (71850 95%) 0.0072\n",
      "37m 40s (- 1m 37s) (71900 95%) 0.0115\n",
      "37m 42s (- 1m 35s) (71950 95%) 0.0074\n",
      "37m 43s (- 1m 34s) (72000 96%) 0.0084\n",
      "37m 45s (- 1m 32s) (72050 96%) 0.0079\n",
      "37m 46s (- 1m 31s) (72100 96%) 0.0124\n",
      "37m 48s (- 1m 29s) (72150 96%) 0.0124\n",
      "37m 50s (- 1m 28s) (72200 96%) 0.0067\n",
      "37m 51s (- 1m 26s) (72250 96%) 0.0053\n",
      "37m 53s (- 1m 24s) (72300 96%) 0.0153\n",
      "37m 54s (- 1m 23s) (72350 96%) 0.0043\n",
      "37m 56s (- 1m 21s) (72400 96%) 0.0116\n",
      "37m 58s (- 1m 20s) (72450 96%) 0.0166\n",
      "37m 59s (- 1m 18s) (72500 96%) 0.0079\n",
      "38m 1s (- 1m 17s) (72550 96%) 0.0075\n",
      "38m 2s (- 1m 15s) (72600 96%) 0.0070\n",
      "38m 4s (- 1m 13s) (72650 96%) 0.0084\n",
      "38m 5s (- 1m 12s) (72700 96%) 0.0104\n",
      "38m 7s (- 1m 10s) (72750 97%) 0.0104\n",
      "38m 9s (- 1m 9s) (72800 97%) 0.0060\n",
      "38m 10s (- 1m 7s) (72850 97%) 0.0097\n",
      "38m 12s (- 1m 6s) (72900 97%) 0.0099\n",
      "38m 14s (- 1m 4s) (72950 97%) 0.0085\n",
      "38m 15s (- 1m 2s) (73000 97%) 0.0212\n",
      "38m 17s (- 1m 1s) (73050 97%) 0.0085\n",
      "38m 18s (- 0m 59s) (73100 97%) 0.0064\n",
      "38m 20s (- 0m 58s) (73150 97%) 0.0167\n",
      "38m 22s (- 0m 56s) (73200 97%) 0.0119\n",
      "38m 23s (- 0m 55s) (73250 97%) 0.0105\n",
      "38m 25s (- 0m 53s) (73300 97%) 0.0046\n",
      "38m 27s (- 0m 51s) (73350 97%) 0.0110\n",
      "38m 28s (- 0m 50s) (73400 97%) 0.0105\n",
      "38m 30s (- 0m 48s) (73450 97%) 0.0123\n",
      "38m 31s (- 0m 47s) (73500 98%) 0.0120\n",
      "38m 33s (- 0m 45s) (73550 98%) 0.0128\n",
      "38m 35s (- 0m 44s) (73600 98%) 0.0086\n",
      "38m 36s (- 0m 42s) (73650 98%) 0.0060\n",
      "38m 38s (- 0m 40s) (73700 98%) 0.0132\n",
      "38m 39s (- 0m 39s) (73750 98%) 0.0181\n",
      "38m 41s (- 0m 37s) (73800 98%) 0.0092\n",
      "38m 43s (- 0m 36s) (73850 98%) 0.0097\n",
      "38m 44s (- 0m 34s) (73900 98%) 0.0103\n",
      "38m 46s (- 0m 33s) (73950 98%) 0.0068\n",
      "38m 48s (- 0m 31s) (74000 98%) 0.0169\n",
      "38m 49s (- 0m 29s) (74050 98%) 0.0062\n",
      "38m 51s (- 0m 28s) (74100 98%) 0.0060\n",
      "38m 52s (- 0m 26s) (74150 98%) 0.0122\n",
      "38m 54s (- 0m 25s) (74200 98%) 0.0101\n",
      "38m 56s (- 0m 23s) (74250 99%) 0.0060\n",
      "38m 57s (- 0m 22s) (74300 99%) 0.0071\n",
      "38m 59s (- 0m 20s) (74350 99%) 0.0048\n",
      "39m 0s (- 0m 18s) (74400 99%) 0.0051\n",
      "39m 2s (- 0m 17s) (74450 99%) 0.0077\n",
      "39m 4s (- 0m 15s) (74500 99%) 0.0079\n",
      "39m 5s (- 0m 14s) (74550 99%) 0.0093\n",
      "39m 7s (- 0m 12s) (74600 99%) 0.0084\n",
      "39m 8s (- 0m 11s) (74650 99%) 0.0096\n",
      "39m 10s (- 0m 9s) (74700 99%) 0.0060\n",
      "39m 11s (- 0m 7s) (74750 99%) 0.0067\n",
      "39m 13s (- 0m 6s) (74800 99%) 0.0061\n",
      "39m 14s (- 0m 4s) (74850 99%) 0.0147\n",
      "39m 16s (- 0m 3s) (74900 99%) 0.0064\n",
      "39m 18s (- 0m 1s) (74950 99%) 0.0117\n",
      "39m 19s (- 0m 0s) (75000 100%) 0.0181\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, 75000, print_every=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> some rabbit behind some bird could live ident\n",
      "= some rabbit behind some bird could live\n",
      "< some rabbit behind some bird could live <EOS>\n",
      "\n",
      "> the dog that the seals can live would live quest\n",
      "= would the dog that the seals can live live\n",
      "< would the dog that the seals can live live <EOS>\n",
      "\n",
      "> my monkeys that my monkeys will giggle will giggle quest\n",
      "= will my monkeys that my monkeys will giggle giggle\n",
      "< will my monkeys that my monkeys will giggle giggle <EOS>\n",
      "\n",
      "> some unicorns who some birds could live could live quest\n",
      "= could some unicorns who some birds could live live\n",
      "< could some unicorns who some birds could live live <EOS>\n",
      "\n",
      "> my monkeys will impress the seals that will impress the elephants quest\n",
      "= will my monkeys impress the seals that will impress the elephants\n",
      "< will my monkeys impress the seals that will impress the elephants <EOS>\n",
      "\n",
      "> the cat around our monkey would irritate the cat quest\n",
      "= would the cat around our monkey irritate the cat\n",
      "< would the cat around our monkey irritate the cat <EOS>\n",
      "\n",
      "> your dogs would confuse our dog that your dogs would live ident\n",
      "= your dogs would confuse our dog that your dogs would live\n",
      "< your dogs would confuse our dog that your dogs would live <EOS>\n",
      "\n",
      "> her elephant would confuse her elephant who can confuse our elephant ident\n",
      "= her elephant would confuse her elephant who can confuse our elephant\n",
      "< her elephant would confuse her elephant who would confuse our elephant <EOS>\n",
      "\n",
      "> our seals that our dog will smile could giggle quest\n",
      "= could our seals that our dog will smile giggle\n",
      "< could our seals that our dog will smile giggle <EOS>\n",
      "\n",
      "> our rabbit that our rabbit can giggle can read ident\n",
      "= our rabbit that our rabbit can giggle can read\n",
      "< our rabbit that our rabbit can giggle can read <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder1, attn_decoder1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing Attention\n",
    "---------------------\n",
    "\n",
    "A useful property of the attention mechanism is its highly interpretable\n",
    "outputs. Because it is used to weight specific encoder outputs of the\n",
    "input sequence, we can imagine looking where the network is focused most\n",
    "at each time step.\n",
    "\n",
    "You could simply run ``plt.matshow(attentions)`` to see attention output\n",
    "displayed as a matrix, with the columns being input steps and rows being\n",
    "output steps:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f2cb05112b0>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_words, attentions = evaluate(\n",
    "    encoder1, attn_decoder1, \"her rabbit who would irritate our rabbit could smile ident\")\n",
    "plt.matshow(attentions.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "conv1.weight \t torch.Size([6, 3, 5, 5])\n",
      "conv1.bias \t torch.Size([6])\n",
      "conv2.weight \t torch.Size([16, 6, 5, 5])\n",
      "conv2.bias \t torch.Size([16])\n",
      "fc1.weight \t torch.Size([120, 400])\n",
      "fc1.bias \t torch.Size([120])\n",
      "fc2.weight \t torch.Size([84, 120])\n",
      "fc2.bias \t torch.Size([84])\n",
      "fc3.weight \t torch.Size([10, 84])\n",
      "fc3.bias \t torch.Size([10])\n",
      "Optimizer's state_dict:\n",
      "param_groups \t [{'weight_decay': 0, 'params': [139830040301072, 139830040299920, 139830040299776, 139830040297832, 139830040298840, 139830040300064, 139830042299488, 139830039709160, 139830039709232, 139830039709304], 'lr': 0.001, 'dampening': 0, 'momentum': 0.9, 'nesterov': False}]\n",
      "state \t {}\n"
     ]
    }
   ],
   "source": [
    "class TheModelClass(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TheModelClass, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "model = TheModelClass()\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "\n",
    "# Print optimizer's state_dict\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])\n",
    "\n",
    "torch.save(model,\"no_agreement.pt\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a better viewing experience we will do the extra work of adding axes\n",
    "and labels:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = your cat who will admire your cat could smile quest\n",
      "output = could your cat who will admire your cat smile <EOS>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/badwolf/.virtualenvs/dl/lib/python3.5/site-packages/matplotlib/figure.py:445: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  % get_backend())\n"
     ]
    }
   ],
   "source": [
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
    "                       ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    fig.savefig(input_sentence+'.png')\n",
    "\n",
    "\n",
    "def evaluateAndShowAttention(input_sentence):\n",
    "    output_words, attentions = evaluate(\n",
    "        encoder1, attn_decoder1, input_sentence)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', ' '.join(output_words))\n",
    "    showAttention(input_sentence, output_words, attentions)\n",
    "\n",
    "\n",
    "evaluateAndShowAttention(\"your cat who will admire your cat could smile quest\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercises\n",
    "=========\n",
    "\n",
    "-  Try with a different dataset\n",
    "\n",
    "   -  Another language pair\n",
    "   -  Human → Machine (e.g. IOT commands)\n",
    "   -  Chat → Response\n",
    "   -  Question → Answer\n",
    "\n",
    "-  Replace the embeddings with pre-trained word embeddings such as word2vec or\n",
    "   GloVe\n",
    "-  Try with more layers, more hidden units, and more sentences. Compare\n",
    "   the training time and results.\n",
    "-  If you use a translation file where pairs have two of the same phrase\n",
    "   (``I am test \\t I am test``), you can use this as an autoencoder. Try\n",
    "   this:\n",
    "\n",
    "   -  Train as an autoencoder\n",
    "   -  Save only the Encoder network\n",
    "   -  Train a new Decoder for translation from there\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seq2seq",
   "language": "python",
   "name": "seq2seq"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
