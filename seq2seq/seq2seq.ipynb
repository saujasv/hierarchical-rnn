{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generation of Questions with a seq2seq network \n",
    "*************************************************************\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "::\n",
    "\n",
    "    [KEY: > input, = target, < output]\n",
    "\n",
    "   \n",
    "\n",
    "This is made possible by the simple but powerful idea of the `sequence\n",
    "to sequence network <http://arxiv.org/abs/1409.3215>`__, in which two\n",
    "recurrent neural networks work together to transform one sequence to\n",
    "another. An encoder network condenses an input sequence into a vector,\n",
    "and a decoder network unfolds that vector into a new sequence.\n",
    "\n",
    ".. figure:: /_static/img/seq-seq-images/seq2seq.png\n",
    "   :alt:\n",
    "\n",
    "To improve upon this model we'll use an `attention\n",
    "mechanism <https://arxiv.org/abs/1409.0473>`__, which lets the decoder\n",
    "learn to focus over a specific range of the input sequence.\n",
    "\n",
    "**Requirements**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data files\n",
    "===================\n",
    "\n",
    "The data for this project is a set of many thousands of sentence pairs, broken down into two main groups i.e agreement and no agreement and are present in the dataset folder of the project\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the character encoding used in the character-level RNN\n",
    "tutorials, we will be representing each word in a language as a one-hot\n",
    "vector, or giant vector of zeros except for a single one (at the index\n",
    "of the word). Compared to the dozens of characters that might exist in a\n",
    "language, there are many many more words, so the encoding vector is much\n",
    "larger. We will however cheat a bit and trim the data to only use a few\n",
    "thousand words per language.\n",
    "\n",
    ".. figure:: /_static/img/seq-seq-images/word-encoding.png\n",
    "   :alt:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need a unique index per word to use as the inputs and targets of\n",
    "the networks later. To keep track of all this we will use a helper class\n",
    "called ``Lang`` which has word → index (``word2index``) and index → word\n",
    "(``index2word``) dictionaries, as well as a count of each word\n",
    "``word2count`` to use to later replace rare words.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "IDENT_TOKEN = -1\n",
    "QUEST_TOKEN = -1\n",
    "EOS_TOKEN =1\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will make evertything lower case adn trim most punctuation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = (s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read the data file we will split the file into lines, and then split\n",
    "lines into pairs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('./../data/agreement_data/train_data.txt' ,).\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum length of each sentence has been set to a 100 words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 100\n",
    "\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full process for preparing the data is:\n",
    "\n",
    "-  Read text file and split into lines, split lines into pairs\n",
    "-  Normalize text, filter by length and content\n",
    "-  Make word lists from sentences in pairs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 435500 sentence pairs\n",
      "Trimmed to 435500 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "inp 54\n",
      "out 52\n",
      "['her rabbit by your elephants does sleep ident', 'her rabbit by your elephants does sleep']\n"
     ]
    }
   ],
   "source": [
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('out', 'inp', True)\n",
    "IDENT_TOKEN = input_lang.word2index[\"ident\"]\n",
    "QUEST_TOKEN = input_lang.word2index[\"quest\"]\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Seq2Seq Model\n",
    "=================\n",
    "\n",
    "A Recurrent Neural Network, or RNN, is a network that operates on a\n",
    "sequence and uses its own output as input for subsequent steps.\n",
    "\n",
    "A `Sequence to Sequence network <http://arxiv.org/abs/1409.3215>`__, or\n",
    "seq2seq network, or `Encoder Decoder\n",
    "network <https://arxiv.org/pdf/1406.1078v3.pdf>`__, is a model\n",
    "consisting of two RNNs called the encoder and decoder. The encoder reads\n",
    "an input sequence and outputs a single vector, and the decoder reads\n",
    "that vector to produce an output sequence.\n",
    "\n",
    ".. figure:: /_static/img/seq-seq-images/seq2seq.png\n",
    "   :alt:\n",
    "\n",
    "Unlike sequence prediction with a single RNN, where every input\n",
    "corresponds to an output, the seq2seq model frees us from sequence\n",
    "length and order, which makes it ideal for translation between two\n",
    "languages.\n",
    "\n",
    "With a seq2seq model the encoder creates a single vector which, in the\n",
    "ideal case, encodes the \"meaning\" of the input sequence into a single\n",
    "vector — a single point in some N dimensional space of sentences.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Encoder\n",
    "-----------\n",
    "\n",
    "The encoder of a seq2seq network is a RNN that outputs some value for\n",
    "every word from the input sentence. For every input word the encoder\n",
    "outputs a vector and a hidden state, and uses the hidden state for the\n",
    "next input word.\n",
    "\n",
    ".. figure:: /_static/img/seq-seq-images/encoder-network.png\n",
    "   :alt:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.randn(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Decoder\n",
    "-----------\n",
    "\n",
    "The decoder is another RNN that takes the encoder output vector(s) and\n",
    "outputs a sequence of words to create the translation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Decoder\n",
    "^^^^^^^^^^^^^^\n",
    "\n",
    "In the simplest seq2seq decoder we use only last output of the encoder.\n",
    "This last output is sometimes called the *context vector* as it encodes\n",
    "context from the entire sequence. This context vector is used as the\n",
    "initial hidden state of the decoder.\n",
    "\n",
    "At every step of decoding, the decoder is given an input token and\n",
    "hidden state. The initial input token is the start-of-string ``<SOS>``\n",
    "token, and the first hidden state is the context vector (the encoder's\n",
    "last hidden state).\n",
    "\n",
    ".. figure:: /_static/img/seq-seq-images/decoder-network.png\n",
    "   :alt:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.randn(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention Decoder\n",
    "^^^^^^^^^^^^^^^^^\n",
    "\n",
    "If only the context vector is passed betweeen the encoder and decoder,\n",
    "that single vector carries the burden of encoding the entire sentence.\n",
    "\n",
    "Attention allows the decoder network to \"focus\" on a different part of\n",
    "the encoder's outputs for every step of the decoder's own outputs. First\n",
    "we calculate a set of *attention weights*. These will be multiplied by\n",
    "the encoder output vectors to create a weighted combination. The result\n",
    "(called ``attn_applied`` in the code) should contain information about\n",
    "that specific part of the input sequence, and thus help the decoder\n",
    "choose the right output words.\n",
    "\n",
    ".. figure:: https://i.imgur.com/1152PYf.png\n",
    "   :alt:\n",
    "\n",
    "Calculating the attention weights is done with another feed-forward\n",
    "layer ``attn``, using the decoder's input and hidden state as inputs.\n",
    "Because there are sentences of all sizes in the training data, to\n",
    "actually create and train this layer we have to choose a maximum\n",
    "sentence length (input length, for encoder outputs) that it can apply\n",
    "to. Sentences of the maximum length will use all the attention weights,\n",
    "while shorter sentences will only use the first few.\n",
    "\n",
    ".. figure:: /_static/img/seq-seq-images/attention-decoder-network.png\n",
    "   :alt:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.randn(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Training\n",
    "========\n",
    "\n",
    "Preparing Training Data\n",
    "-----------------------\n",
    "\n",
    "To train, for each pair we will need an input tensor (indexes of the\n",
    "words in the input sentence) and target tensor (indexes of the words in\n",
    "the target sentence). While creating these vectors we will append the\n",
    "EOS token to both sequences.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_TOKEN)\n",
    "        \n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Model\n",
    "------------------\n",
    "\n",
    "To train we run the input sentence through the encoder, and keep track\n",
    "of every output and the latest hidden state. Then the decoder is given\n",
    "the ``<SOS>`` token as its first input, and the last hidden state of the\n",
    "encoder as its first hidden state.\n",
    "\n",
    "\"Teacher forcing\" is the concept of using the real target outputs as\n",
    "each next input, instead of using the decoder's guess as the next input.\n",
    "Using teacher forcing causes it to converge faster but `when the trained\n",
    "network is exploited, it may exhibit\n",
    "instability <http://minds.jacobs-university.de/sites/default/files/uploads/papers/ESNTutorialRev.pdf>`__.\n",
    "\n",
    "You can observe outputs of teacher-forced networks that read with\n",
    "coherent grammar but wander far from the correct translation -\n",
    "intuitively it has learned to represent the output grammar and can \"pick\n",
    "up\" the meaning once the teacher tells it the first few words, but it\n",
    "has not properly learned how to create the sentence from the translation\n",
    "in the first place.\n",
    "\n",
    "Because of the freedom PyTorch's autograd gives us, we can randomly\n",
    "choose to use teacher forcing or not with a simple if statement. Turn\n",
    "``teacher_forcing_ratio`` up to use more of it.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[2],\n",
       "         [3],\n",
       "         [4],\n",
       "         [5],\n",
       "         [6],\n",
       "         [1]]), tensor([[2],\n",
       "         [3],\n",
       "         [4],\n",
       "         [5],\n",
       "         [1]]))"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorsFromPair(pairs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == IDENT_TOKEN or decoder_input.item() == QUEST_TOKEN or decoder_input.item()== EOS_TOKEN:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a helper function to print time elapsed and estimated time\n",
    "remaining given the current time and progress %.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole training process looks like this:\n",
    "\n",
    "-  Start a timer\n",
    "-  Initialize optimizers and criterion\n",
    "-  Create set of training pairs\n",
    "-  Start empty losses array for plotting\n",
    "\n",
    "Then we call ``train`` many times and occasionally print the progress (%\n",
    "of examples, time so far, estimated time) and average loss.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting results\n",
    "----------------\n",
    "\n",
    "Plotting is done with matplotlib, using the array of loss values\n",
    "``plot_losses`` saved while training.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation\n",
    "==========\n",
    "\n",
    "Evaluation is mostly the same as training, but there are no targets so\n",
    "we simply feed the decoder's predictions back to itself for each step.\n",
    "Every time it predicts a word we add it to the output string, and if it\n",
    "predicts the EOS token we stop there. We also store the decoder's\n",
    "attention outputs for display later.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == IDENT_TOKEN :\n",
    "                decoded_words.append('<IDENT>')\n",
    "                break\n",
    "            elif topi.item() == QUEST_TOKEN :\n",
    "                decoded_words.append('<QUEST>')\n",
    "                break\n",
    "            elif topi.item() == EOS_TOKEN :\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate random sentences from the training set and print out the\n",
    "input, target, and output to make some subjective quality judgements:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and Evaluating\n",
    "=======================\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "conv1.weight \t torch.Size([6, 3, 5, 5])\n",
      "conv1.bias \t torch.Size([6])\n",
      "conv2.weight \t torch.Size([16, 6, 5, 5])\n",
      "conv2.bias \t torch.Size([16])\n",
      "fc1.weight \t torch.Size([120, 400])\n",
      "fc1.bias \t torch.Size([120])\n",
      "fc2.weight \t torch.Size([84, 120])\n",
      "fc2.bias \t torch.Size([84])\n",
      "fc3.weight \t torch.Size([10, 84])\n",
      "fc3.bias \t torch.Size([10])\n",
      "Optimizer's state_dict:\n",
      "state \t {}\n",
      "param_groups \t [{'momentum': 0.9, 'params': [140310933905768, 140310933909152, 140310793436904, 140310793433160, 140310793435752, 140310793436256, 140310839989160, 140310793436400, 140310793437120, 140310793437048], 'lr': 0.001, 'nesterov': False, 'weight_decay': 0, 'dampening': 0}]\n"
     ]
    }
   ],
   "source": [
    "class TheModelClass(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TheModelClass, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "model = TheModelClass()\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "\n",
    "# Print optimizer's state_dict\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_exists = False\n",
    "if model_exists :\n",
    "    model = torch.load(\"no_agreement.pt\")\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aagreement_1.pt\n",
      "0m 2s (- 61m 49s) (50 0%) 2.9914\n",
      "0m 3s (- 42m 31s) (100 0%) 2.5123\n",
      "0m 4s (- 36m 42s) (150 0%) 2.7625\n",
      "0m 5s (- 34m 7s) (200 0%) 2.9020\n",
      "0m 6s (- 32m 48s) (250 0%) 2.7935\n",
      "0m 7s (- 32m 9s) (300 0%) 2.6161\n",
      "0m 8s (- 31m 25s) (350 0%) 2.4848\n",
      "0m 9s (- 30m 39s) (400 0%) 2.3509\n",
      "0m 10s (- 30m 0s) (450 0%) 2.4903\n",
      "0m 12s (- 29m 57s) (500 0%) 2.4747\n",
      "0m 13s (- 29m 27s) (550 0%) 1.9994\n",
      "0m 14s (- 29m 14s) (600 0%) 2.4514\n",
      "0m 15s (- 29m 1s) (650 0%) 2.5777\n",
      "0m 16s (- 28m 34s) (700 0%) 2.2137\n",
      "0m 17s (- 28m 22s) (750 1%) 2.3310\n",
      "0m 18s (- 28m 14s) (800 1%) 2.1025\n",
      "0m 19s (- 28m 1s) (850 1%) 2.1806\n",
      "0m 20s (- 27m 48s) (900 1%) 2.1513\n",
      "0m 21s (- 27m 43s) (950 1%) 2.2729\n",
      "0m 22s (- 27m 43s) (1000 1%) 2.4233\n",
      "0m 23s (- 27m 38s) (1050 1%) 2.2265\n",
      "0m 24s (- 27m 29s) (1100 1%) 2.4179\n",
      "0m 25s (- 27m 24s) (1150 1%) 2.2518\n",
      "0m 26s (- 27m 16s) (1200 1%) 1.9391\n",
      "0m 27s (- 27m 6s) (1250 1%) 2.0662\n",
      "0m 28s (- 26m 58s) (1300 1%) 2.1858\n",
      "0m 29s (- 26m 52s) (1350 1%) 2.3340\n",
      "0m 30s (- 26m 41s) (1400 1%) 1.9544\n",
      "0m 31s (- 26m 38s) (1450 1%) 2.3225\n",
      "0m 32s (- 26m 32s) (1500 2%) 1.9620\n",
      "0m 33s (- 26m 26s) (1550 2%) 1.9628\n",
      "0m 34s (- 26m 22s) (1600 2%) 2.1735\n",
      "0m 35s (- 26m 17s) (1650 2%) 2.0605\n",
      "0m 36s (- 26m 21s) (1700 2%) 2.1307\n",
      "0m 37s (- 26m 23s) (1750 2%) 1.9721\n",
      "0m 38s (- 26m 20s) (1800 2%) 2.1397\n",
      "0m 39s (- 26m 16s) (1850 2%) 1.8053\n",
      "0m 40s (- 26m 13s) (1900 2%) 1.8604\n",
      "0m 42s (- 26m 13s) (1950 2%) 2.0183\n",
      "0m 43s (- 26m 14s) (2000 2%) 1.9191\n",
      "0m 44s (- 26m 10s) (2050 2%) 1.8942\n",
      "0m 45s (- 26m 12s) (2100 2%) 2.0803\n",
      "0m 46s (- 26m 11s) (2150 2%) 1.8812\n",
      "0m 47s (- 26m 12s) (2200 2%) 1.8547\n",
      "0m 48s (- 26m 13s) (2250 3%) 1.9115\n",
      "0m 49s (- 26m 12s) (2300 3%) 1.6442\n",
      "0m 50s (- 26m 12s) (2350 3%) 1.7923\n",
      "0m 51s (- 26m 12s) (2400 3%) 1.7234\n",
      "0m 53s (- 26m 18s) (2450 3%) 1.7422\n",
      "0m 54s (- 26m 22s) (2500 3%) 1.6896\n",
      "0m 55s (- 26m 23s) (2550 3%) 1.6586\n",
      "0m 56s (- 26m 21s) (2600 3%) 1.7350\n",
      "0m 58s (- 26m 24s) (2650 3%) 1.7503\n",
      "0m 59s (- 26m 25s) (2700 3%) 1.7631\n",
      "1m 0s (- 26m 23s) (2750 3%) 1.5058\n",
      "1m 1s (- 26m 21s) (2800 3%) 1.5161\n",
      "1m 2s (- 26m 21s) (2850 3%) 1.4649\n",
      "1m 3s (- 26m 21s) (2900 3%) 1.7816\n",
      "1m 4s (- 26m 22s) (2950 3%) 1.5282\n",
      "1m 5s (- 26m 19s) (3000 4%) 1.7614\n",
      "1m 6s (- 26m 20s) (3050 4%) 1.6489\n",
      "1m 8s (- 26m 18s) (3100 4%) 1.6631\n",
      "1m 9s (- 26m 18s) (3150 4%) 1.6802\n",
      "1m 10s (- 26m 19s) (3200 4%) 1.5753\n",
      "1m 11s (- 26m 18s) (3250 4%) 1.5513\n",
      "1m 12s (- 26m 17s) (3300 4%) 1.5658\n",
      "1m 13s (- 26m 16s) (3350 4%) 1.4553\n",
      "1m 14s (- 26m 18s) (3400 4%) 1.4931\n",
      "1m 16s (- 26m 17s) (3450 4%) 1.4640\n",
      "1m 17s (- 26m 17s) (3500 4%) 1.2784\n",
      "1m 18s (- 26m 16s) (3550 4%) 1.3481\n",
      "1m 19s (- 26m 16s) (3600 4%) 1.5234\n",
      "1m 20s (- 26m 15s) (3650 4%) 1.3088\n",
      "1m 21s (- 26m 16s) (3700 4%) 1.5760\n",
      "1m 22s (- 26m 16s) (3750 5%) 1.4979\n",
      "1m 24s (- 26m 15s) (3800 5%) 1.5876\n",
      "1m 25s (- 26m 15s) (3850 5%) 1.5331\n",
      "1m 26s (- 26m 14s) (3900 5%) 1.4405\n",
      "1m 27s (- 26m 13s) (3950 5%) 1.3831\n",
      "1m 28s (- 26m 12s) (4000 5%) 1.2577\n",
      "1m 29s (- 26m 11s) (4050 5%) 1.3567\n",
      "1m 30s (- 26m 11s) (4100 5%) 1.3891\n",
      "1m 32s (- 26m 11s) (4150 5%) 1.3549\n",
      "1m 33s (- 26m 10s) (4200 5%) 1.1455\n",
      "1m 34s (- 26m 9s) (4250 5%) 1.1292\n",
      "1m 35s (- 26m 8s) (4300 5%) 1.1916\n",
      "1m 36s (- 26m 7s) (4350 5%) 1.2145\n",
      "1m 37s (- 26m 7s) (4400 5%) 1.2060\n",
      "1m 38s (- 26m 5s) (4450 5%) 1.1962\n",
      "1m 39s (- 26m 4s) (4500 6%) 1.2518\n",
      "1m 40s (- 26m 3s) (4550 6%) 1.2880\n",
      "1m 42s (- 26m 3s) (4600 6%) 1.3221\n",
      "1m 43s (- 26m 3s) (4650 6%) 1.1874\n",
      "1m 44s (- 26m 3s) (4700 6%) 1.0951\n",
      "1m 45s (- 26m 3s) (4750 6%) 1.2030\n",
      "1m 46s (- 26m 3s) (4800 6%) 1.1852\n",
      "1m 48s (- 26m 3s) (4850 6%) 1.2750\n",
      "1m 49s (- 26m 3s) (4900 6%) 1.1502\n",
      "1m 50s (- 26m 2s) (4950 6%) 1.1801\n",
      "1m 51s (- 26m 3s) (5000 6%) 1.1671\n",
      "1m 52s (- 26m 1s) (5050 6%) 0.9956\n",
      "1m 53s (- 26m 1s) (5100 6%) 0.9611\n",
      "1m 55s (- 26m 0s) (5150 6%) 0.8989\n",
      "1m 56s (- 26m 0s) (5200 6%) 1.0271\n",
      "1m 57s (- 25m 58s) (5250 7%) 1.0358\n",
      "1m 58s (- 25m 56s) (5300 7%) 0.7786\n",
      "1m 59s (- 25m 55s) (5350 7%) 1.1198\n",
      "2m 0s (- 25m 54s) (5400 7%) 0.9081\n",
      "2m 1s (- 25m 54s) (5450 7%) 1.0398\n",
      "2m 3s (- 25m 55s) (5500 7%) 1.2216\n",
      "2m 4s (- 25m 55s) (5550 7%) 1.0559\n",
      "2m 5s (- 25m 55s) (5600 7%) 1.0953\n",
      "2m 6s (- 25m 55s) (5650 7%) 1.0213\n",
      "2m 8s (- 25m 56s) (5700 7%) 0.9313\n",
      "2m 9s (- 25m 55s) (5750 7%) 0.7223\n",
      "2m 10s (- 25m 55s) (5800 7%) 0.7588\n",
      "2m 11s (- 25m 55s) (5850 7%) 0.8641\n",
      "2m 12s (- 25m 53s) (5900 7%) 0.7115\n",
      "2m 13s (- 25m 52s) (5950 7%) 0.7824\n",
      "2m 14s (- 25m 50s) (6000 8%) 0.9145\n",
      "2m 16s (- 25m 50s) (6050 8%) 0.7921\n",
      "2m 17s (- 25m 50s) (6100 8%) 0.7616\n",
      "2m 18s (- 25m 50s) (6150 8%) 0.8196\n",
      "2m 19s (- 25m 50s) (6200 8%) 0.6973\n",
      "2m 20s (- 25m 49s) (6250 8%) 0.7989\n",
      "2m 21s (- 25m 48s) (6300 8%) 0.7848\n",
      "2m 23s (- 25m 46s) (6350 8%) 0.6816\n",
      "2m 24s (- 25m 47s) (6400 8%) 0.6596\n",
      "2m 25s (- 25m 46s) (6450 8%) 0.8771\n",
      "2m 26s (- 25m 46s) (6500 8%) 0.7114\n",
      "2m 27s (- 25m 45s) (6550 8%) 0.5234\n",
      "2m 29s (- 25m 44s) (6600 8%) 0.7495\n",
      "2m 30s (- 25m 48s) (6650 8%) 0.8290\n",
      "2m 32s (- 25m 51s) (6700 8%) 0.6732\n",
      "2m 33s (- 25m 51s) (6750 9%) 0.7641\n",
      "2m 34s (- 25m 49s) (6800 9%) 0.6199\n",
      "2m 35s (- 25m 49s) (6850 9%) 0.5885\n",
      "2m 36s (- 25m 49s) (6900 9%) 0.5810\n",
      "2m 38s (- 25m 48s) (6950 9%) 0.6826\n",
      "2m 39s (- 25m 48s) (7000 9%) 0.7826\n",
      "2m 40s (- 25m 48s) (7050 9%) 0.5835\n",
      "2m 41s (- 25m 48s) (7100 9%) 0.5907\n",
      "2m 43s (- 25m 48s) (7150 9%) 0.5991\n",
      "2m 44s (- 25m 48s) (7200 9%) 0.6899\n",
      "2m 45s (- 25m 47s) (7250 9%) 0.5639\n",
      "2m 46s (- 25m 48s) (7300 9%) 0.6407\n",
      "2m 48s (- 25m 47s) (7350 9%) 0.5231\n",
      "2m 49s (- 25m 46s) (7400 9%) 0.5312\n",
      "2m 50s (- 25m 46s) (7450 9%) 0.5828\n",
      "2m 51s (- 25m 45s) (7500 10%) 0.6251\n",
      "2m 53s (- 25m 46s) (7550 10%) 0.5548\n",
      "2m 54s (- 25m 45s) (7600 10%) 0.5274\n",
      "2m 55s (- 25m 45s) (7650 10%) 0.4615\n",
      "2m 56s (- 25m 45s) (7700 10%) 0.5070\n",
      "2m 58s (- 25m 44s) (7750 10%) 0.4497\n",
      "2m 59s (- 25m 44s) (7800 10%) 0.5706\n",
      "3m 0s (- 25m 44s) (7850 10%) 0.4897\n",
      "3m 1s (- 25m 42s) (7900 10%) 0.6408\n",
      "3m 2s (- 25m 42s) (7950 10%) 0.5670\n",
      "3m 4s (- 25m 41s) (8000 10%) 0.6514\n",
      "3m 5s (- 25m 40s) (8050 10%) 0.5248\n",
      "3m 6s (- 25m 39s) (8100 10%) 0.4787\n",
      "3m 7s (- 25m 39s) (8150 10%) 0.4362\n",
      "3m 8s (- 25m 38s) (8200 10%) 0.4647\n",
      "3m 10s (- 25m 37s) (8250 11%) 0.5253\n",
      "3m 11s (- 25m 36s) (8300 11%) 0.3756\n",
      "3m 12s (- 25m 35s) (8350 11%) 0.4139\n",
      "3m 13s (- 25m 34s) (8400 11%) 0.5059\n",
      "3m 14s (- 25m 34s) (8450 11%) 0.4383\n",
      "3m 16s (- 25m 33s) (8500 11%) 0.4822\n",
      "3m 17s (- 25m 33s) (8550 11%) 0.6083\n",
      "3m 18s (- 25m 33s) (8600 11%) 0.4208\n",
      "3m 19s (- 25m 32s) (8650 11%) 0.4573\n",
      "3m 21s (- 25m 32s) (8700 11%) 0.4539\n",
      "3m 22s (- 25m 31s) (8750 11%) 0.5037\n",
      "3m 23s (- 25m 30s) (8800 11%) 0.4945\n",
      "3m 24s (- 25m 30s) (8850 11%) 0.4606\n",
      "3m 26s (- 25m 30s) (8900 11%) 0.4601\n",
      "3m 27s (- 25m 30s) (8950 11%) 0.4658\n",
      "3m 28s (- 25m 29s) (9000 12%) 0.4323\n",
      "3m 29s (- 25m 29s) (9050 12%) 0.4124\n",
      "3m 31s (- 25m 28s) (9100 12%) 0.5209\n",
      "3m 32s (- 25m 28s) (9150 12%) 0.4210\n",
      "3m 33s (- 25m 27s) (9200 12%) 0.3303\n",
      "3m 34s (- 25m 26s) (9250 12%) 0.3169\n",
      "3m 36s (- 25m 25s) (9300 12%) 0.3619\n",
      "3m 37s (- 25m 25s) (9350 12%) 0.4334\n",
      "3m 38s (- 25m 24s) (9400 12%) 0.3845\n",
      "3m 39s (- 25m 24s) (9450 12%) 0.5262\n",
      "3m 40s (- 25m 22s) (9500 12%) 0.3209\n",
      "3m 42s (- 25m 22s) (9550 12%) 0.3812\n",
      "3m 43s (- 25m 21s) (9600 12%) 0.3713\n",
      "3m 44s (- 25m 21s) (9650 12%) 0.3921\n",
      "3m 45s (- 25m 20s) (9700 12%) 0.3786\n",
      "3m 47s (- 25m 20s) (9750 13%) 0.4318\n",
      "3m 48s (- 25m 19s) (9800 13%) 0.4438\n",
      "3m 49s (- 25m 18s) (9850 13%) 0.3406\n",
      "3m 50s (- 25m 17s) (9900 13%) 0.2730\n",
      "3m 51s (- 25m 16s) (9950 13%) 0.2952\n",
      "3m 53s (- 25m 15s) (10000 13%) 0.3672\n",
      "3m 54s (- 25m 15s) (10050 13%) 0.4038\n",
      "3m 55s (- 25m 14s) (10100 13%) 0.3310\n",
      "3m 57s (- 25m 14s) (10150 13%) 0.3543\n",
      "3m 58s (- 25m 13s) (10200 13%) 0.3244\n",
      "3m 59s (- 25m 13s) (10250 13%) 0.3224\n",
      "4m 0s (- 25m 11s) (10300 13%) 0.3255\n",
      "4m 2s (- 25m 11s) (10350 13%) 0.3677\n",
      "4m 3s (- 25m 10s) (10400 13%) 0.3333\n",
      "4m 4s (- 25m 10s) (10450 13%) 0.2453\n",
      "4m 5s (- 25m 8s) (10500 14%) 0.2795\n",
      "4m 6s (- 25m 7s) (10550 14%) 0.3339\n",
      "4m 7s (- 25m 6s) (10600 14%) 0.3335\n",
      "4m 9s (- 25m 5s) (10650 14%) 0.3959\n",
      "4m 10s (- 25m 4s) (10700 14%) 0.3089\n",
      "4m 11s (- 25m 4s) (10750 14%) 0.2924\n",
      "4m 13s (- 25m 3s) (10800 14%) 0.3961\n",
      "4m 14s (- 25m 3s) (10850 14%) 0.3229\n",
      "4m 15s (- 25m 2s) (10900 14%) 0.3149\n",
      "4m 16s (- 25m 0s) (10950 14%) 0.2776\n",
      "4m 17s (- 24m 59s) (11000 14%) 0.2425\n",
      "4m 18s (- 24m 58s) (11050 14%) 0.2862\n",
      "4m 20s (- 24m 58s) (11100 14%) 0.3528\n",
      "4m 21s (- 24m 58s) (11150 14%) 0.3315\n",
      "4m 22s (- 24m 56s) (11200 14%) 0.3373\n",
      "4m 23s (- 24m 55s) (11250 15%) 0.3091\n",
      "4m 25s (- 24m 55s) (11300 15%) 0.2861\n",
      "4m 26s (- 24m 55s) (11350 15%) 0.3368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4m 27s (- 24m 54s) (11400 15%) 0.2721\n",
      "4m 29s (- 24m 53s) (11450 15%) 0.2830\n",
      "4m 30s (- 24m 53s) (11500 15%) 0.2737\n",
      "4m 31s (- 24m 52s) (11550 15%) 0.2694\n",
      "4m 32s (- 24m 52s) (11600 15%) 0.3132\n",
      "4m 34s (- 24m 51s) (11650 15%) 0.2988\n",
      "4m 35s (- 24m 51s) (11700 15%) 0.3669\n",
      "4m 36s (- 24m 50s) (11750 15%) 0.3025\n",
      "4m 38s (- 24m 49s) (11800 15%) 0.2495\n",
      "4m 39s (- 24m 48s) (11850 15%) 0.2319\n",
      "4m 40s (- 24m 47s) (11900 15%) 0.2504\n",
      "4m 41s (- 24m 47s) (11950 15%) 0.3067\n",
      "4m 43s (- 24m 46s) (12000 16%) 0.2810\n",
      "4m 44s (- 24m 45s) (12050 16%) 0.2630\n",
      "4m 45s (- 24m 45s) (12100 16%) 0.2997\n",
      "4m 46s (- 24m 44s) (12150 16%) 0.2603\n",
      "4m 48s (- 24m 43s) (12200 16%) 0.1975\n",
      "4m 49s (- 24m 42s) (12250 16%) 0.1793\n",
      "4m 50s (- 24m 41s) (12300 16%) 0.2568\n",
      "4m 51s (- 24m 39s) (12350 16%) 0.3640\n",
      "4m 52s (- 24m 38s) (12400 16%) 0.2397\n",
      "4m 54s (- 24m 37s) (12450 16%) 0.2760\n",
      "4m 55s (- 24m 37s) (12500 16%) 0.2510\n",
      "4m 56s (- 24m 36s) (12550 16%) 0.3128\n",
      "4m 57s (- 24m 35s) (12600 16%) 0.3185\n",
      "4m 59s (- 24m 34s) (12650 16%) 0.2232\n",
      "5m 0s (- 24m 33s) (12700 16%) 0.2068\n",
      "5m 1s (- 24m 33s) (12750 17%) 0.2833\n",
      "5m 3s (- 24m 32s) (12800 17%) 0.2481\n",
      "5m 4s (- 24m 32s) (12850 17%) 0.2434\n",
      "5m 5s (- 24m 31s) (12900 17%) 0.2705\n",
      "5m 7s (- 24m 31s) (12950 17%) 0.2847\n",
      "5m 8s (- 24m 30s) (13000 17%) 0.2493\n",
      "5m 9s (- 24m 30s) (13050 17%) 0.1938\n",
      "5m 10s (- 24m 28s) (13100 17%) 0.2446\n",
      "5m 12s (- 24m 27s) (13150 17%) 0.2364\n",
      "5m 13s (- 24m 27s) (13200 17%) 0.2294\n",
      "5m 14s (- 24m 26s) (13250 17%) 0.2397\n",
      "5m 15s (- 24m 25s) (13300 17%) 0.1589\n",
      "5m 17s (- 24m 24s) (13350 17%) 0.2176\n",
      "5m 18s (- 24m 24s) (13400 17%) 0.2620\n",
      "5m 19s (- 24m 23s) (13450 17%) 0.2067\n",
      "5m 21s (- 24m 22s) (13500 18%) 0.1988\n",
      "5m 22s (- 24m 21s) (13550 18%) 0.2084\n",
      "5m 23s (- 24m 19s) (13600 18%) 0.2270\n",
      "5m 24s (- 24m 18s) (13650 18%) 0.2806\n",
      "5m 25s (- 24m 18s) (13700 18%) 0.2352\n",
      "5m 27s (- 24m 17s) (13750 18%) 0.1855\n",
      "5m 28s (- 24m 16s) (13800 18%) 0.2342\n",
      "5m 29s (- 24m 15s) (13850 18%) 0.2189\n",
      "5m 31s (- 24m 15s) (13900 18%) 0.2468\n",
      "5m 32s (- 24m 14s) (13950 18%) 0.2570\n",
      "5m 33s (- 24m 14s) (14000 18%) 0.2146\n",
      "5m 35s (- 24m 13s) (14050 18%) 0.2370\n",
      "5m 36s (- 24m 13s) (14100 18%) 0.2593\n",
      "5m 37s (- 24m 12s) (14150 18%) 0.2117\n",
      "5m 39s (- 24m 11s) (14200 18%) 0.3065\n",
      "5m 40s (- 24m 10s) (14250 19%) 0.2521\n",
      "5m 41s (- 24m 10s) (14300 19%) 0.2062\n",
      "5m 43s (- 24m 9s) (14350 19%) 0.2549\n",
      "5m 44s (- 24m 8s) (14400 19%) 0.1692\n",
      "5m 45s (- 24m 7s) (14450 19%) 0.2528\n",
      "5m 46s (- 24m 6s) (14500 19%) 0.2252\n",
      "5m 47s (- 24m 5s) (14550 19%) 0.2074\n",
      "5m 49s (- 24m 4s) (14600 19%) 0.2164\n",
      "5m 50s (- 24m 3s) (14650 19%) 0.1621\n",
      "5m 51s (- 24m 3s) (14700 19%) 0.1692\n",
      "5m 53s (- 24m 2s) (14750 19%) 0.2578\n",
      "5m 54s (- 24m 2s) (14800 19%) 0.1805\n",
      "5m 55s (- 24m 1s) (14850 19%) 0.2214\n",
      "5m 57s (- 24m 1s) (14900 19%) 0.2461\n",
      "5m 58s (- 24m 0s) (14950 19%) 0.2448\n",
      "5m 59s (- 23m 59s) (15000 20%) 0.1891\n",
      "6m 1s (- 23m 58s) (15050 20%) 0.2061\n",
      "6m 2s (- 23m 57s) (15100 20%) 0.2021\n",
      "6m 3s (- 23m 56s) (15150 20%) 0.1691\n",
      "6m 5s (- 23m 56s) (15200 20%) 0.1919\n",
      "6m 6s (- 23m 55s) (15250 20%) 0.2004\n",
      "6m 7s (- 23m 55s) (15300 20%) 0.1896\n",
      "6m 9s (- 23m 54s) (15350 20%) 0.1898\n",
      "6m 10s (- 23m 53s) (15400 20%) 0.2973\n",
      "6m 11s (- 23m 52s) (15450 20%) 0.2227\n",
      "6m 13s (- 23m 52s) (15500 20%) 0.2240\n",
      "6m 14s (- 23m 51s) (15550 20%) 0.2148\n",
      "6m 15s (- 23m 51s) (15600 20%) 0.2130\n",
      "6m 17s (- 23m 50s) (15650 20%) 0.1824\n",
      "6m 18s (- 23m 49s) (15700 20%) 0.2127\n",
      "6m 19s (- 23m 48s) (15750 21%) 0.2012\n",
      "6m 21s (- 23m 48s) (15800 21%) 0.2936\n",
      "6m 22s (- 23m 47s) (15850 21%) 0.2150\n",
      "6m 23s (- 23m 46s) (15900 21%) 0.1649\n",
      "6m 25s (- 23m 45s) (15950 21%) 0.1907\n",
      "6m 26s (- 23m 44s) (16000 21%) 0.1881\n",
      "6m 27s (- 23m 44s) (16050 21%) 0.1846\n",
      "6m 29s (- 23m 43s) (16100 21%) 0.1574\n",
      "6m 30s (- 23m 42s) (16150 21%) 0.1685\n",
      "6m 31s (- 23m 42s) (16200 21%) 0.1953\n",
      "6m 32s (- 23m 40s) (16250 21%) 0.1893\n",
      "6m 34s (- 23m 40s) (16300 21%) 0.2959\n",
      "6m 35s (- 23m 39s) (16350 21%) 0.1928\n",
      "6m 36s (- 23m 38s) (16400 21%) 0.1576\n",
      "6m 38s (- 23m 37s) (16450 21%) 0.1299\n",
      "6m 39s (- 23m 36s) (16500 22%) 0.1758\n",
      "6m 40s (- 23m 35s) (16550 22%) 0.1428\n",
      "6m 42s (- 23m 34s) (16600 22%) 0.1550\n",
      "6m 43s (- 23m 33s) (16650 22%) 0.1211\n",
      "6m 44s (- 23m 32s) (16700 22%) 0.1459\n",
      "6m 45s (- 23m 31s) (16750 22%) 0.1263\n",
      "6m 47s (- 23m 30s) (16800 22%) 0.1548\n",
      "6m 48s (- 23m 29s) (16850 22%) 0.1900\n",
      "6m 49s (- 23m 28s) (16900 22%) 0.2046\n",
      "6m 51s (- 23m 28s) (16950 22%) 0.2207\n",
      "6m 52s (- 23m 27s) (17000 22%) 0.1612\n",
      "6m 53s (- 23m 26s) (17050 22%) 0.1513\n",
      "6m 55s (- 23m 25s) (17100 22%) 0.1615\n",
      "6m 56s (- 23m 24s) (17150 22%) 0.1753\n",
      "6m 57s (- 23m 23s) (17200 22%) 0.1568\n",
      "6m 58s (- 23m 22s) (17250 23%) 0.1121\n",
      "7m 0s (- 23m 21s) (17300 23%) 0.1120\n",
      "7m 1s (- 23m 20s) (17350 23%) 0.1494\n",
      "7m 2s (- 23m 19s) (17400 23%) 0.1950\n",
      "7m 4s (- 23m 18s) (17450 23%) 0.1984\n",
      "7m 5s (- 23m 17s) (17500 23%) 0.1661\n",
      "7m 6s (- 23m 16s) (17550 23%) 0.1656\n",
      "7m 8s (- 23m 16s) (17600 23%) 0.1830\n",
      "7m 9s (- 23m 15s) (17650 23%) 0.1186\n",
      "7m 10s (- 23m 14s) (17700 23%) 0.1732\n",
      "7m 11s (- 23m 13s) (17750 23%) 0.2037\n",
      "7m 13s (- 23m 12s) (17800 23%) 0.1711\n",
      "7m 14s (- 23m 11s) (17850 23%) 0.1390\n",
      "7m 15s (- 23m 10s) (17900 23%) 0.1579\n",
      "7m 17s (- 23m 9s) (17950 23%) 0.1518\n",
      "7m 18s (- 23m 8s) (18000 24%) 0.1224\n",
      "7m 19s (- 23m 8s) (18050 24%) 0.1762\n",
      "7m 21s (- 23m 7s) (18100 24%) 0.1620\n",
      "7m 22s (- 23m 6s) (18150 24%) 0.1627\n",
      "7m 23s (- 23m 5s) (18200 24%) 0.1119\n",
      "7m 25s (- 23m 4s) (18250 24%) 0.1851\n",
      "7m 26s (- 23m 3s) (18300 24%) 0.1454\n",
      "7m 27s (- 23m 2s) (18350 24%) 0.1822\n",
      "7m 29s (- 23m 2s) (18400 24%) 0.2061\n",
      "7m 30s (- 23m 1s) (18450 24%) 0.1039\n",
      "7m 32s (- 23m 0s) (18500 24%) 0.1302\n",
      "7m 33s (- 22m 59s) (18550 24%) 0.1212\n",
      "7m 34s (- 22m 58s) (18600 24%) 0.1555\n",
      "7m 36s (- 22m 58s) (18650 24%) 0.2241\n",
      "7m 37s (- 22m 57s) (18700 24%) 0.1456\n",
      "7m 38s (- 22m 56s) (18750 25%) 0.1128\n",
      "7m 40s (- 22m 55s) (18800 25%) 0.1313\n",
      "7m 41s (- 22m 54s) (18850 25%) 0.1079\n",
      "7m 42s (- 22m 52s) (18900 25%) 0.1139\n",
      "7m 43s (- 22m 52s) (18950 25%) 0.1350\n",
      "7m 45s (- 22m 51s) (19000 25%) 0.1401\n",
      "7m 46s (- 22m 50s) (19050 25%) 0.1313\n",
      "7m 47s (- 22m 49s) (19100 25%) 0.1320\n",
      "7m 49s (- 22m 48s) (19150 25%) 0.1661\n",
      "7m 50s (- 22m 47s) (19200 25%) 0.0928\n",
      "7m 51s (- 22m 45s) (19250 25%) 0.1233\n",
      "7m 52s (- 22m 45s) (19300 25%) 0.1346\n",
      "7m 54s (- 22m 44s) (19350 25%) 0.1327\n",
      "7m 55s (- 22m 43s) (19400 25%) 0.1201\n",
      "7m 56s (- 22m 42s) (19450 25%) 0.1207\n",
      "7m 58s (- 22m 41s) (19500 26%) 0.1303\n",
      "7m 59s (- 22m 40s) (19550 26%) 0.1440\n",
      "8m 0s (- 22m 38s) (19600 26%) 0.1083\n",
      "8m 1s (- 22m 37s) (19650 26%) 0.1357\n",
      "8m 3s (- 22m 36s) (19700 26%) 0.1243\n",
      "8m 4s (- 22m 35s) (19750 26%) 0.1536\n",
      "8m 5s (- 22m 34s) (19800 26%) 0.1109\n",
      "8m 7s (- 22m 33s) (19850 26%) 0.1207\n",
      "8m 8s (- 22m 32s) (19900 26%) 0.0887\n",
      "8m 9s (- 22m 31s) (19950 26%) 0.1329\n",
      "8m 11s (- 22m 30s) (20000 26%) 0.1128\n",
      "8m 12s (- 22m 28s) (20050 26%) 0.1402\n",
      "8m 13s (- 22m 27s) (20100 26%) 0.1655\n",
      "8m 14s (- 22m 26s) (20150 26%) 0.1073\n",
      "8m 16s (- 22m 25s) (20200 26%) 0.1167\n",
      "8m 17s (- 22m 24s) (20250 27%) 0.1265\n",
      "8m 18s (- 22m 23s) (20300 27%) 0.1031\n",
      "8m 20s (- 22m 23s) (20350 27%) 0.1322\n",
      "8m 21s (- 22m 22s) (20400 27%) 0.0886\n",
      "8m 22s (- 22m 21s) (20450 27%) 0.1091\n",
      "8m 24s (- 22m 20s) (20500 27%) 0.2649\n",
      "8m 25s (- 22m 19s) (20550 27%) 0.1429\n",
      "8m 26s (- 22m 18s) (20600 27%) 0.1458\n",
      "8m 28s (- 22m 17s) (20650 27%) 0.1151\n",
      "8m 29s (- 22m 16s) (20700 27%) 0.1501\n",
      "8m 30s (- 22m 15s) (20750 27%) 0.1177\n",
      "8m 32s (- 22m 14s) (20800 27%) 0.1223\n",
      "8m 33s (- 22m 13s) (20850 27%) 0.0996\n",
      "8m 34s (- 22m 12s) (20900 27%) 0.1035\n",
      "8m 36s (- 22m 11s) (20950 27%) 0.0908\n",
      "8m 37s (- 22m 10s) (21000 28%) 0.1189\n",
      "8m 38s (- 22m 10s) (21050 28%) 0.1889\n",
      "8m 40s (- 22m 8s) (21100 28%) 0.0825\n",
      "8m 41s (- 22m 8s) (21150 28%) 0.1474\n",
      "8m 42s (- 22m 7s) (21200 28%) 0.1162\n",
      "8m 44s (- 22m 6s) (21250 28%) 0.1456\n",
      "8m 45s (- 22m 5s) (21300 28%) 0.1575\n",
      "8m 47s (- 22m 4s) (21350 28%) 0.3524\n",
      "8m 48s (- 22m 3s) (21400 28%) 0.1302\n",
      "8m 49s (- 22m 2s) (21450 28%) 0.1140\n",
      "8m 51s (- 22m 1s) (21500 28%) 0.1214\n",
      "8m 52s (- 22m 0s) (21550 28%) 0.1400\n",
      "8m 53s (- 21m 59s) (21600 28%) 0.1050\n",
      "8m 55s (- 21m 58s) (21650 28%) 0.1337\n",
      "8m 56s (- 21m 58s) (21700 28%) 0.2642\n",
      "8m 57s (- 21m 56s) (21750 28%) 0.2284\n",
      "8m 59s (- 21m 55s) (21800 29%) 0.1365\n",
      "9m 0s (- 21m 54s) (21850 29%) 0.1154\n",
      "9m 1s (- 21m 53s) (21900 29%) 0.1583\n",
      "9m 3s (- 21m 52s) (21950 29%) 0.1237\n",
      "9m 4s (- 21m 51s) (22000 29%) 0.0743\n",
      "9m 5s (- 21m 50s) (22050 29%) 0.0949\n",
      "9m 7s (- 21m 49s) (22100 29%) 0.1230\n",
      "9m 8s (- 21m 48s) (22150 29%) 0.1109\n",
      "9m 9s (- 21m 47s) (22200 29%) 0.1261\n",
      "9m 10s (- 21m 46s) (22250 29%) 0.1035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9m 12s (- 21m 44s) (22300 29%) 0.2010\n",
      "9m 13s (- 21m 43s) (22350 29%) 0.1044\n",
      "9m 14s (- 21m 42s) (22400 29%) 0.1761\n",
      "9m 16s (- 21m 42s) (22450 29%) 0.1492\n",
      "9m 17s (- 21m 41s) (22500 30%) 0.0724\n",
      "9m 18s (- 21m 40s) (22550 30%) 0.0916\n",
      "9m 20s (- 21m 38s) (22600 30%) 0.1154\n",
      "9m 21s (- 21m 37s) (22650 30%) 0.1377\n",
      "9m 22s (- 21m 37s) (22700 30%) 0.0883\n",
      "9m 24s (- 21m 36s) (22750 30%) 0.1011\n",
      "9m 25s (- 21m 35s) (22800 30%) 0.1195\n",
      "9m 26s (- 21m 33s) (22850 30%) 0.1464\n",
      "9m 28s (- 21m 33s) (22900 30%) 0.0851\n",
      "9m 29s (- 21m 31s) (22950 30%) 0.1140\n",
      "9m 31s (- 21m 30s) (23000 30%) 0.1130\n",
      "9m 32s (- 21m 29s) (23050 30%) 0.0839\n",
      "9m 33s (- 21m 28s) (23100 30%) 0.1030\n",
      "9m 34s (- 21m 27s) (23150 30%) 0.0903\n",
      "9m 36s (- 21m 26s) (23200 30%) 0.1006\n",
      "9m 37s (- 21m 25s) (23250 31%) 0.0984\n",
      "9m 38s (- 21m 24s) (23300 31%) 0.0812\n",
      "9m 40s (- 21m 23s) (23350 31%) 0.0700\n",
      "9m 41s (- 21m 22s) (23400 31%) 0.0992\n",
      "9m 42s (- 21m 20s) (23450 31%) 0.0814\n",
      "9m 44s (- 21m 19s) (23500 31%) 0.0801\n",
      "9m 45s (- 21m 18s) (23550 31%) 0.0833\n",
      "9m 46s (- 21m 17s) (23600 31%) 0.1181\n",
      "9m 48s (- 21m 16s) (23650 31%) 0.1026\n",
      "9m 49s (- 21m 15s) (23700 31%) 0.0831\n",
      "9m 50s (- 21m 14s) (23750 31%) 0.0958\n",
      "9m 51s (- 21m 12s) (23800 31%) 0.0957\n",
      "9m 53s (- 21m 11s) (23850 31%) 0.1152\n",
      "9m 54s (- 21m 10s) (23900 31%) 0.0800\n",
      "9m 55s (- 21m 9s) (23950 31%) 0.0674\n",
      "9m 56s (- 21m 8s) (24000 32%) 0.1113\n",
      "9m 58s (- 21m 7s) (24050 32%) 0.1027\n",
      "9m 59s (- 21m 6s) (24100 32%) 0.0638\n",
      "10m 0s (- 21m 5s) (24150 32%) 0.0765\n",
      "10m 2s (- 21m 3s) (24200 32%) 0.0681\n",
      "10m 3s (- 21m 2s) (24250 32%) 0.1067\n",
      "10m 4s (- 21m 1s) (24300 32%) 0.0791\n",
      "10m 5s (- 21m 0s) (24350 32%) 0.0857\n",
      "10m 7s (- 20m 59s) (24400 32%) 0.0716\n",
      "10m 8s (- 20m 58s) (24450 32%) 0.1182\n",
      "10m 9s (- 20m 57s) (24500 32%) 0.0736\n",
      "10m 11s (- 20m 56s) (24550 32%) 0.0861\n",
      "10m 12s (- 20m 54s) (24600 32%) 0.0709\n",
      "10m 13s (- 20m 53s) (24650 32%) 0.1040\n",
      "10m 15s (- 20m 52s) (24700 32%) 0.0974\n",
      "10m 16s (- 20m 51s) (24750 33%) 0.0958\n",
      "10m 17s (- 20m 50s) (24800 33%) 0.0983\n",
      "10m 19s (- 20m 49s) (24850 33%) 0.1150\n",
      "10m 20s (- 20m 48s) (24900 33%) 0.1023\n",
      "10m 22s (- 20m 47s) (24950 33%) 0.0810\n",
      "10m 23s (- 20m 46s) (25000 33%) 0.0756\n",
      "10m 24s (- 20m 45s) (25050 33%) 0.1164\n",
      "10m 25s (- 20m 44s) (25100 33%) 0.0854\n",
      "10m 27s (- 20m 42s) (25150 33%) 0.0621\n",
      "10m 28s (- 20m 41s) (25200 33%) 0.1140\n",
      "10m 29s (- 20m 40s) (25250 33%) 0.0819\n",
      "10m 30s (- 20m 39s) (25300 33%) 0.1028\n",
      "10m 32s (- 20m 38s) (25350 33%) 0.1071\n",
      "10m 33s (- 20m 37s) (25400 33%) 0.0735\n",
      "10m 34s (- 20m 36s) (25450 33%) 0.0618\n",
      "10m 36s (- 20m 35s) (25500 34%) 0.0867\n",
      "10m 37s (- 20m 33s) (25550 34%) 0.0562\n",
      "10m 38s (- 20m 32s) (25600 34%) 0.0948\n",
      "10m 40s (- 20m 31s) (25650 34%) 0.0639\n",
      "10m 41s (- 20m 30s) (25700 34%) 0.0737\n",
      "10m 42s (- 20m 29s) (25750 34%) 0.0888\n",
      "10m 44s (- 20m 28s) (25800 34%) 0.0897\n",
      "10m 45s (- 20m 27s) (25850 34%) 0.0657\n",
      "10m 46s (- 20m 26s) (25900 34%) 0.0538\n",
      "10m 48s (- 20m 25s) (25950 34%) 0.0766\n",
      "10m 49s (- 20m 24s) (26000 34%) 0.0724\n",
      "10m 50s (- 20m 22s) (26050 34%) 0.0785\n",
      "10m 52s (- 20m 21s) (26100 34%) 0.0715\n",
      "10m 53s (- 20m 20s) (26150 34%) 0.0718\n",
      "10m 54s (- 20m 19s) (26200 34%) 0.0565\n",
      "10m 56s (- 20m 18s) (26250 35%) 0.0697\n",
      "10m 57s (- 20m 17s) (26300 35%) 0.0744\n",
      "10m 58s (- 20m 16s) (26350 35%) 0.0470\n",
      "11m 0s (- 20m 15s) (26400 35%) 0.0907\n",
      "11m 1s (- 20m 13s) (26450 35%) 0.0579\n",
      "11m 2s (- 20m 12s) (26500 35%) 0.0756\n",
      "11m 3s (- 20m 11s) (26550 35%) 0.0586\n",
      "11m 5s (- 20m 10s) (26600 35%) 0.0949\n",
      "11m 6s (- 20m 9s) (26650 35%) 0.0866\n",
      "11m 8s (- 20m 8s) (26700 35%) 0.0773\n",
      "11m 9s (- 20m 7s) (26750 35%) 0.0739\n",
      "11m 10s (- 20m 6s) (26800 35%) 0.0522\n",
      "11m 11s (- 20m 4s) (26850 35%) 0.0677\n",
      "11m 13s (- 20m 3s) (26900 35%) 0.0467\n",
      "11m 14s (- 20m 2s) (26950 35%) 0.1253\n",
      "11m 15s (- 20m 0s) (27000 36%) 0.0979\n",
      "11m 16s (- 19m 59s) (27050 36%) 0.0494\n",
      "11m 18s (- 19m 58s) (27100 36%) 0.0423\n",
      "11m 19s (- 19m 57s) (27150 36%) 0.0623\n",
      "11m 20s (- 19m 56s) (27200 36%) 0.0615\n",
      "11m 22s (- 19m 55s) (27250 36%) 0.0582\n",
      "11m 23s (- 19m 53s) (27300 36%) 0.0739\n",
      "11m 24s (- 19m 52s) (27350 36%) 0.0642\n",
      "11m 25s (- 19m 51s) (27400 36%) 0.0636\n",
      "11m 27s (- 19m 50s) (27450 36%) 0.0560\n",
      "11m 28s (- 19m 49s) (27500 36%) 0.0730\n",
      "11m 29s (- 19m 48s) (27550 36%) 0.0635\n",
      "11m 31s (- 19m 47s) (27600 36%) 0.0672\n",
      "11m 32s (- 19m 45s) (27650 36%) 0.0678\n",
      "11m 33s (- 19m 44s) (27700 36%) 0.0522\n",
      "11m 35s (- 19m 43s) (27750 37%) 0.0527\n",
      "11m 36s (- 19m 42s) (27800 37%) 0.0466\n",
      "11m 37s (- 19m 41s) (27850 37%) 0.0711\n",
      "11m 39s (- 19m 40s) (27900 37%) 0.0751\n",
      "11m 40s (- 19m 39s) (27950 37%) 0.0779\n",
      "11m 41s (- 19m 38s) (28000 37%) 0.0848\n",
      "11m 43s (- 19m 36s) (28050 37%) 0.0766\n",
      "11m 44s (- 19m 35s) (28100 37%) 0.0850\n",
      "11m 45s (- 19m 34s) (28150 37%) 0.0746\n",
      "11m 47s (- 19m 33s) (28200 37%) 0.0730\n",
      "11m 48s (- 19m 32s) (28250 37%) 0.0553\n",
      "11m 49s (- 19m 31s) (28300 37%) 0.0754\n",
      "11m 51s (- 19m 30s) (28350 37%) 0.0620\n",
      "11m 52s (- 19m 29s) (28400 37%) 0.0603\n",
      "11m 53s (- 19m 27s) (28450 37%) 0.0528\n",
      "11m 55s (- 19m 26s) (28500 38%) 0.0648\n",
      "11m 56s (- 19m 26s) (28550 38%) 0.0912\n",
      "11m 58s (- 19m 24s) (28600 38%) 0.0530\n",
      "11m 59s (- 19m 23s) (28650 38%) 0.0512\n",
      "12m 0s (- 19m 22s) (28700 38%) 0.0480\n",
      "12m 2s (- 19m 21s) (28750 38%) 0.0593\n",
      "12m 3s (- 19m 20s) (28800 38%) 0.0413\n",
      "12m 4s (- 19m 19s) (28850 38%) 0.0617\n",
      "12m 6s (- 19m 18s) (28900 38%) 0.0468\n",
      "12m 7s (- 19m 17s) (28950 38%) 0.0610\n",
      "12m 8s (- 19m 15s) (29000 38%) 0.0397\n",
      "12m 9s (- 19m 14s) (29050 38%) 0.0345\n",
      "12m 11s (- 19m 13s) (29100 38%) 0.0620\n",
      "12m 12s (- 19m 12s) (29150 38%) 0.0576\n",
      "12m 13s (- 19m 11s) (29200 38%) 0.0465\n",
      "12m 15s (- 19m 9s) (29250 39%) 0.0467\n",
      "12m 16s (- 19m 8s) (29300 39%) 0.0453\n",
      "12m 17s (- 19m 7s) (29350 39%) 0.1281\n",
      "12m 19s (- 19m 6s) (29400 39%) 0.0787\n",
      "12m 20s (- 19m 5s) (29450 39%) 0.0717\n",
      "12m 21s (- 19m 4s) (29500 39%) 0.0699\n",
      "12m 23s (- 19m 3s) (29550 39%) 0.0794\n",
      "12m 24s (- 19m 2s) (29600 39%) 0.1232\n",
      "12m 26s (- 19m 1s) (29650 39%) 0.0419\n",
      "12m 27s (- 19m 0s) (29700 39%) 0.0656\n",
      "12m 28s (- 18m 59s) (29750 39%) 0.0751\n",
      "12m 30s (- 18m 58s) (29800 39%) 0.0958\n",
      "12m 31s (- 18m 56s) (29850 39%) 0.0456\n",
      "12m 32s (- 18m 55s) (29900 39%) 0.0559\n",
      "12m 34s (- 18m 54s) (29950 39%) 0.0458\n",
      "12m 35s (- 18m 53s) (30000 40%) 0.0367\n",
      "12m 36s (- 18m 52s) (30050 40%) 0.0439\n",
      "12m 38s (- 18m 50s) (30100 40%) 0.0783\n",
      "12m 39s (- 18m 49s) (30150 40%) 0.0726\n",
      "12m 40s (- 18m 48s) (30200 40%) 0.0544\n",
      "12m 41s (- 18m 47s) (30250 40%) 0.0495\n",
      "12m 43s (- 18m 45s) (30300 40%) 0.0481\n",
      "12m 44s (- 18m 44s) (30350 40%) 0.0596\n",
      "12m 45s (- 18m 43s) (30400 40%) 0.0446\n",
      "12m 47s (- 18m 42s) (30450 40%) 0.0514\n",
      "12m 48s (- 18m 41s) (30500 40%) 0.0565\n",
      "12m 49s (- 18m 40s) (30550 40%) 0.0555\n",
      "12m 51s (- 18m 38s) (30600 40%) 0.0647\n",
      "12m 52s (- 18m 37s) (30650 40%) 0.0409\n",
      "12m 53s (- 18m 36s) (30700 40%) 0.0585\n",
      "12m 55s (- 18m 35s) (30750 41%) 0.0391\n",
      "12m 56s (- 18m 34s) (30800 41%) 0.0360\n",
      "12m 57s (- 18m 32s) (30850 41%) 0.0685\n",
      "12m 58s (- 18m 31s) (30900 41%) 0.0662\n",
      "13m 0s (- 18m 30s) (30950 41%) 0.1049\n",
      "13m 1s (- 18m 29s) (31000 41%) 0.0721\n",
      "13m 2s (- 18m 28s) (31050 41%) 0.0657\n",
      "13m 4s (- 18m 26s) (31100 41%) 0.0710\n",
      "13m 5s (- 18m 25s) (31150 41%) 0.0379\n",
      "13m 6s (- 18m 24s) (31200 41%) 0.0763\n",
      "13m 8s (- 18m 23s) (31250 41%) 0.0412\n",
      "13m 9s (- 18m 22s) (31300 41%) 0.0370\n",
      "13m 10s (- 18m 21s) (31350 41%) 0.1076\n",
      "13m 12s (- 18m 19s) (31400 41%) 0.1231\n",
      "13m 13s (- 18m 18s) (31450 41%) 0.0438\n",
      "13m 14s (- 18m 17s) (31500 42%) 0.0369\n",
      "13m 15s (- 18m 16s) (31550 42%) 0.0826\n",
      "13m 17s (- 18m 15s) (31600 42%) 0.0982\n",
      "13m 18s (- 18m 13s) (31650 42%) 0.0537\n",
      "13m 19s (- 18m 12s) (31700 42%) 0.0340\n",
      "13m 21s (- 18m 11s) (31750 42%) 0.0629\n",
      "13m 22s (- 18m 10s) (31800 42%) 0.0800\n",
      "13m 23s (- 18m 9s) (31850 42%) 0.0473\n",
      "13m 25s (- 18m 7s) (31900 42%) 0.0441\n",
      "13m 26s (- 18m 6s) (31950 42%) 0.0657\n",
      "13m 27s (- 18m 5s) (32000 42%) 0.0451\n",
      "13m 29s (- 18m 4s) (32050 42%) 0.0704\n",
      "13m 30s (- 18m 3s) (32100 42%) 0.0371\n",
      "13m 31s (- 18m 2s) (32150 42%) 0.0673\n",
      "13m 33s (- 18m 0s) (32200 42%) 0.0399\n",
      "13m 34s (- 17m 59s) (32250 43%) 0.0623\n",
      "13m 35s (- 17m 58s) (32300 43%) 0.0731\n",
      "13m 37s (- 17m 57s) (32350 43%) 0.0667\n",
      "13m 38s (- 17m 56s) (32400 43%) 0.0898\n",
      "13m 39s (- 17m 55s) (32450 43%) 0.0260\n",
      "13m 42s (- 17m 55s) (32500 43%) 0.0434\n",
      "13m 43s (- 17m 54s) (32550 43%) 0.0561\n",
      "13m 45s (- 17m 53s) (32600 43%) 0.0399\n",
      "13m 46s (- 17m 51s) (32650 43%) 0.0388\n",
      "13m 47s (- 17m 50s) (32700 43%) 0.0437\n",
      "13m 48s (- 17m 49s) (32750 43%) 0.0475\n",
      "13m 50s (- 17m 48s) (32800 43%) 0.0548\n",
      "13m 51s (- 17m 47s) (32850 43%) 0.0434\n",
      "13m 52s (- 17m 45s) (32900 43%) 0.0320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13m 54s (- 17m 44s) (32950 43%) 0.0593\n",
      "13m 55s (- 17m 43s) (33000 44%) 0.0531\n",
      "13m 56s (- 17m 42s) (33050 44%) 0.0389\n",
      "13m 58s (- 17m 41s) (33100 44%) 0.0423\n",
      "13m 59s (- 17m 39s) (33150 44%) 0.0466\n",
      "14m 0s (- 17m 38s) (33200 44%) 0.0438\n",
      "14m 2s (- 17m 37s) (33250 44%) 0.0388\n",
      "14m 3s (- 17m 36s) (33300 44%) 0.0350\n",
      "14m 4s (- 17m 35s) (33350 44%) 0.0681\n",
      "14m 6s (- 17m 33s) (33400 44%) 0.0407\n",
      "14m 7s (- 17m 32s) (33450 44%) 0.0419\n",
      "14m 8s (- 17m 31s) (33500 44%) 0.0696\n",
      "14m 10s (- 17m 30s) (33550 44%) 0.0533\n",
      "14m 11s (- 17m 29s) (33600 44%) 0.0534\n",
      "14m 12s (- 17m 27s) (33650 44%) 0.0586\n",
      "14m 14s (- 17m 26s) (33700 44%) 0.0509\n",
      "14m 15s (- 17m 25s) (33750 45%) 0.0621\n",
      "14m 16s (- 17m 24s) (33800 45%) 0.0643\n",
      "14m 18s (- 17m 23s) (33850 45%) 0.0460\n",
      "14m 19s (- 17m 21s) (33900 45%) 0.0549\n",
      "14m 20s (- 17m 20s) (33950 45%) 0.1108\n",
      "14m 22s (- 17m 19s) (34000 45%) 0.0462\n",
      "14m 23s (- 17m 18s) (34050 45%) 0.0568\n",
      "14m 24s (- 17m 17s) (34100 45%) 0.0288\n",
      "14m 26s (- 17m 15s) (34150 45%) 0.0376\n",
      "14m 27s (- 17m 14s) (34200 45%) 0.0292\n",
      "14m 28s (- 17m 13s) (34250 45%) 0.0815\n",
      "14m 30s (- 17m 12s) (34300 45%) 0.0476\n",
      "14m 31s (- 17m 11s) (34350 45%) 0.0277\n",
      "14m 32s (- 17m 9s) (34400 45%) 0.0681\n",
      "14m 34s (- 17m 8s) (34450 45%) 0.0454\n",
      "14m 35s (- 17m 7s) (34500 46%) 0.0654\n",
      "14m 36s (- 17m 6s) (34550 46%) 0.0425\n",
      "14m 38s (- 17m 5s) (34600 46%) 0.0381\n",
      "14m 39s (- 17m 4s) (34650 46%) 0.0315\n",
      "14m 40s (- 17m 2s) (34700 46%) 0.0263\n",
      "14m 42s (- 17m 1s) (34750 46%) 0.0399\n",
      "14m 43s (- 17m 0s) (34800 46%) 0.0432\n",
      "14m 44s (- 16m 59s) (34850 46%) 0.0406\n",
      "14m 46s (- 16m 58s) (34900 46%) 0.0450\n",
      "14m 47s (- 16m 57s) (34950 46%) 0.0506\n",
      "14m 48s (- 16m 55s) (35000 46%) 0.0316\n",
      "14m 50s (- 16m 54s) (35050 46%) 0.0364\n",
      "14m 51s (- 16m 53s) (35100 46%) 0.0347\n",
      "14m 52s (- 16m 52s) (35150 46%) 0.0395\n",
      "14m 54s (- 16m 50s) (35200 46%) 0.0336\n",
      "14m 55s (- 16m 49s) (35250 47%) 0.0483\n",
      "14m 56s (- 16m 48s) (35300 47%) 0.0396\n",
      "14m 58s (- 16m 47s) (35350 47%) 0.0394\n",
      "14m 59s (- 16m 46s) (35400 47%) 0.0673\n",
      "15m 0s (- 16m 45s) (35450 47%) 0.0342\n",
      "15m 2s (- 16m 43s) (35500 47%) 0.0409\n",
      "15m 3s (- 16m 42s) (35550 47%) 0.0501\n",
      "15m 4s (- 16m 41s) (35600 47%) 0.0267\n",
      "15m 6s (- 16m 40s) (35650 47%) 0.0419\n",
      "15m 7s (- 16m 39s) (35700 47%) 0.0361\n",
      "15m 9s (- 16m 38s) (35750 47%) 0.0469\n",
      "15m 10s (- 16m 36s) (35800 47%) 0.0335\n",
      "15m 11s (- 16m 35s) (35850 47%) 0.0432\n",
      "15m 12s (- 16m 34s) (35900 47%) 0.0401\n",
      "15m 14s (- 16m 33s) (35950 47%) 0.0446\n",
      "15m 15s (- 16m 32s) (36000 48%) 0.0772\n",
      "15m 17s (- 16m 30s) (36050 48%) 0.0357\n",
      "15m 18s (- 16m 29s) (36100 48%) 0.0246\n",
      "15m 19s (- 16m 28s) (36150 48%) 0.0290\n",
      "15m 21s (- 16m 27s) (36200 48%) 0.0369\n",
      "15m 22s (- 16m 26s) (36250 48%) 0.0241\n",
      "15m 23s (- 16m 24s) (36300 48%) 0.0319\n",
      "15m 25s (- 16m 23s) (36350 48%) 0.0253\n",
      "15m 26s (- 16m 22s) (36400 48%) 0.0513\n",
      "15m 27s (- 16m 20s) (36450 48%) 0.0401\n",
      "15m 28s (- 16m 19s) (36500 48%) 0.0487\n",
      "15m 30s (- 16m 18s) (36550 48%) 0.0388\n",
      "15m 31s (- 16m 17s) (36600 48%) 0.0478\n",
      "15m 32s (- 16m 16s) (36650 48%) 0.0477\n",
      "15m 34s (- 16m 15s) (36700 48%) 0.0461\n",
      "15m 35s (- 16m 13s) (36750 49%) 0.1763\n",
      "15m 37s (- 16m 12s) (36800 49%) 0.1114\n",
      "15m 38s (- 16m 11s) (36850 49%) 0.0262\n",
      "15m 39s (- 16m 9s) (36900 49%) 0.0299\n",
      "15m 40s (- 16m 8s) (36950 49%) 0.0638\n",
      "15m 41s (- 16m 7s) (37000 49%) 0.0497\n",
      "15m 43s (- 16m 6s) (37050 49%) 0.0417\n",
      "15m 44s (- 16m 5s) (37100 49%) 0.0337\n",
      "15m 46s (- 16m 3s) (37150 49%) 0.0671\n",
      "15m 47s (- 16m 2s) (37200 49%) 0.0688\n",
      "15m 48s (- 16m 1s) (37250 49%) 0.0388\n",
      "15m 50s (- 16m 0s) (37300 49%) 0.0433\n",
      "15m 51s (- 15m 59s) (37350 49%) 0.0311\n",
      "15m 52s (- 15m 58s) (37400 49%) 0.0410\n",
      "15m 54s (- 15m 56s) (37450 49%) 0.0415\n",
      "15m 55s (- 15m 55s) (37500 50%) 0.0394\n",
      "15m 56s (- 15m 54s) (37550 50%) 0.0571\n",
      "15m 58s (- 15m 53s) (37600 50%) 0.0251\n",
      "15m 59s (- 15m 52s) (37650 50%) 0.0507\n",
      "16m 1s (- 15m 50s) (37700 50%) 0.0223\n",
      "16m 2s (- 15m 49s) (37750 50%) 0.0245\n",
      "16m 3s (- 15m 48s) (37800 50%) 0.0500\n",
      "16m 4s (- 15m 47s) (37850 50%) 0.0343\n",
      "16m 6s (- 15m 45s) (37900 50%) 0.0415\n",
      "16m 7s (- 15m 44s) (37950 50%) 0.0333\n",
      "16m 8s (- 15m 43s) (38000 50%) 0.0237\n",
      "16m 10s (- 15m 42s) (38050 50%) 0.0438\n",
      "16m 11s (- 15m 40s) (38100 50%) 0.0351\n",
      "16m 12s (- 15m 39s) (38150 50%) 0.0616\n",
      "16m 14s (- 15m 38s) (38200 50%) 0.0210\n",
      "16m 15s (- 15m 37s) (38250 51%) 0.0487\n",
      "16m 16s (- 15m 36s) (38300 51%) 0.0316\n",
      "16m 18s (- 15m 34s) (38350 51%) 0.0219\n",
      "16m 19s (- 15m 33s) (38400 51%) 0.0242\n",
      "16m 20s (- 15m 32s) (38450 51%) 0.0459\n",
      "16m 22s (- 15m 30s) (38500 51%) 0.0283\n",
      "16m 23s (- 15m 29s) (38550 51%) 0.0231\n",
      "16m 24s (- 15m 28s) (38600 51%) 0.0586\n",
      "16m 25s (- 15m 27s) (38650 51%) 0.0196\n",
      "16m 27s (- 15m 25s) (38700 51%) 0.0345\n",
      "16m 28s (- 15m 24s) (38750 51%) 0.0272\n",
      "16m 29s (- 15m 23s) (38800 51%) 0.0439\n",
      "16m 30s (- 15m 22s) (38850 51%) 0.0190\n",
      "16m 32s (- 15m 20s) (38900 51%) 0.0322\n",
      "16m 33s (- 15m 19s) (38950 51%) 0.0405\n",
      "16m 35s (- 15m 18s) (39000 52%) 0.0295\n",
      "16m 36s (- 15m 17s) (39050 52%) 0.0269\n",
      "16m 37s (- 15m 15s) (39100 52%) 0.0267\n",
      "16m 38s (- 15m 14s) (39150 52%) 0.0279\n",
      "16m 40s (- 15m 13s) (39200 52%) 0.0462\n",
      "16m 41s (- 15m 12s) (39250 52%) 0.0388\n",
      "16m 42s (- 15m 11s) (39300 52%) 0.0219\n",
      "16m 44s (- 15m 9s) (39350 52%) 0.0443\n",
      "16m 45s (- 15m 8s) (39400 52%) 0.0336\n",
      "16m 46s (- 15m 7s) (39450 52%) 0.0362\n",
      "16m 48s (- 15m 6s) (39500 52%) 0.0377\n",
      "16m 49s (- 15m 4s) (39550 52%) 0.0486\n",
      "16m 50s (- 15m 3s) (39600 52%) 0.0520\n",
      "16m 52s (- 15m 2s) (39650 52%) 0.1078\n",
      "16m 53s (- 15m 1s) (39700 52%) 0.0291\n",
      "16m 55s (- 15m 0s) (39750 53%) 0.0342\n",
      "16m 56s (- 14m 58s) (39800 53%) 0.0518\n",
      "16m 57s (- 14m 57s) (39850 53%) 0.0293\n",
      "16m 59s (- 14m 56s) (39900 53%) 0.0286\n",
      "17m 0s (- 14m 55s) (39950 53%) 0.0381\n",
      "17m 1s (- 14m 54s) (40000 53%) 0.0334\n",
      "17m 3s (- 14m 52s) (40050 53%) 0.0240\n",
      "17m 4s (- 14m 51s) (40100 53%) 0.0297\n",
      "17m 5s (- 14m 50s) (40150 53%) 0.0306\n",
      "17m 7s (- 14m 49s) (40200 53%) 0.0592\n",
      "17m 8s (- 14m 48s) (40250 53%) 0.0272\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-197-8aef51866bcf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mencoder1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoderRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_lang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mattn_decoder1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAttnDecoderRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_lang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtrainIters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_decoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m75000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-191-29eaa6fdc28a>\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(encoder, decoder, n_iters, print_every, plot_every, learning_rate)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         loss = train(input_tensor, target_tensor, encoder,\n\u001b[0;32m---> 19\u001b[0;31m                      decoder, encoder_optimizer, decoder_optimizer, criterion)\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mprint_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mplot_loss_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-189-bbfa52ed37f8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length)\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/dl/lib/python3.5/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/dl/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "counter=1\n",
    "while(counter == 1 ):\n",
    "    name = \"aagreement_\"+str(counter)+\".pt\"\n",
    "    print(name)\n",
    "    \n",
    "    encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "    attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "    trainIters(encoder1, attn_decoder1, 75000, print_every=50)\n",
    "    \n",
    "    torch.save(model.state_dict,name)   \n",
    "    counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> the unicorn who the bird does sleep does sleep quest\n",
      "= does the unicorn who the bird does sleep sleep\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'EOS_TOKEM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-200-752866089bea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluateRandomly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_decoder1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-194-9d7626baa304>\u001b[0m in \u001b[0;36mevaluateRandomly\u001b[0;34m(encoder, decoder, n)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'>'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'='\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0moutput_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattentions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0moutput_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-193-61e68c329f9d>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(encoder, decoder, sentence, max_length)\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mdecoded_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<QUEST>'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0;32melif\u001b[0m \u001b[0mtopi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mEOS_TOKEM\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m                 \u001b[0mdecoded_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<EOS>'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'EOS_TOKEM' is not defined"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder1, attn_decoder1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing Attention\n",
    "---------------------\n",
    "\n",
    "A useful property of the attention mechanism is its highly interpretable\n",
    "outputs. Because it is used to weight specific encoder outputs of the\n",
    "input sequence, we can imagine looking where the network is focused most\n",
    "at each time step.\n",
    "\n",
    "You could simply run ``plt.matshow(attentions)`` to see attention output\n",
    "displayed as a matrix, with the columns being input steps and rows being\n",
    "output steps:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f3010f49630>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_words, attentions = evaluate(\n",
    "    encoder1, attn_decoder1, \"our cats do read quest\")\n",
    "plt.matshow(attentions.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a better viewing experience we will do the extra work of adding axes\n",
    "and labels:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = my cats by my yaks dont smile quest\n",
      "output = dont my cats by my yaks smile dont smile read smile smile sleep smile my cats smile dont smile read smile confuse my yaks that dont smile do irritate my cats that dont smile do smile read smile irritate my seals that dont smile do irritate my cats that dont smile do irritate my yaks that dont smile do confuse my cats that dont smile do irritate my seals that dont smile do confuse my yaks that dont smile do irritate my cats that dont smile do confuse my seals that dont smile do irritate my monkey that dont smile\n"
     ]
    }
   ],
   "source": [
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
    "                       ['<end>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    fig.savefig(input_sentence+'.png')\n",
    "\n",
    "\n",
    "def evaluateAndShowAttention(input_sentence):\n",
    "    output_words, attentions = evaluate(\n",
    "        encoder1, attn_decoder1, input_sentence)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', ' '.join(output_words))\n",
    "    showAttention(input_sentence, output_words, attentions)\n",
    "\n",
    "\n",
    "evaluateAndShowAttention(\"my cats by my yaks dont smile quest\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seq2seq",
   "language": "python",
   "name": "seq2seq"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
